title,content,date,url,author,domain,categories
Lecture Notes: Optimization for Machine Learning,"Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley.",2019-09-08,http://arxiv.org/abs/1909.03550v1,Elad Hazan,arxiv.org,"cs.LG, stat.ML"
An Optimal Control View of Adversarial Machine Learning,"I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning.",2018-11-11,http://arxiv.org/abs/1811.04422v1,Xiaojin Zhu,arxiv.org,"cs.LG, stat.ML"
Minimax deviation strategies for machine learning and recognition with   short learning samples,The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws.,2017-07-16,http://arxiv.org/abs/1707.04849v1,"Michail Schlesinger, Evgeniy Vodolazskiy",arxiv.org,cs.LG
Machine Learning for Clinical Predictive Analytics,"In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the reasons behind why they are useful for solving clinical prediction problems, (2) understand the intuition behind some machine learning models, including regression, decision trees, and support vector machines, and (3) understand how to apply these models to clinical prediction problems using publicly available datasets via case studies.",2019-09-19,http://arxiv.org/abs/1909.09246v1,Wei-Hung Weng,arxiv.org,"cs.LG, stat.ML"
Towards Modular Machine Learning Solution Development: Benefits and   Trade-offs,"Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands in the way of efficient and cost effective customized machine learning solution development. In this work we explore the benefits of modular machine learning solutions and discuss how modular machine learning solutions can overcome some of the major solution engineering limitations of monolithic machine learning solutions. We analyze the trade-offs between modular and monolithic machine learning solutions through three deep learning problems; one text based and the two image based. Our experimental results show that modular machine learning solutions have a promising potential to reap the solution engineering advantages of modularity while gaining performance and data advantages in a way the monolithic machine learning solutions do not permit.",2023-01-23,http://arxiv.org/abs/2301.09753v1,"Samiyuru Menik, Lakshmish Ramaswamy",arxiv.org,"cs.LG, cs.SE"
Introduction to Machine Learning: Class Notes 67577,"Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem).",2009-04-23,http://arxiv.org/abs/0904.3664v1,Amnon Shashua,arxiv.org,cs.LG
The Tribes of Machine Learning and the Realm of Computer Architecture,"Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecture.",2020-12-07,http://arxiv.org/abs/2012.04105v1,"Ayaz Akram, Jason Lowe-Power",arxiv.org,"cs.LG, cs.AR"
"A Machine Learning Tutorial for Operational Meteorology, Part I:   Traditional Machine Learning","Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of machine learning methods and lower hesitancy towards machine learning in meteorology, this paper provides a survey of some of the most common machine learning methods. A familiar meteorological example is used to contextualize the machine learning methods while also discussing machine learning topics using plain language. The following machine learning methods are demonstrated: linear regression; logistic regression; decision trees; random forest; gradient boosted decision trees; naive Bayes; and support vector machines. Beyond discussing the different methods, the paper also contains discussions on the general machine learning process as well as best practices to enable readers to apply machine learning to their own datasets. Furthermore, all code (in the form of Jupyter notebooks and Google Colaboratory notebooks) used to make the examples in the paper is provided in an effort to catalyse the use of machine learning in meteorology.",2022-04-15,http://arxiv.org/abs/2204.07492v2,"Randy J. Chase, David R. Harrison, Amanda Burke, Gary M. Lackmann, Amy McGovern",arxiv.org,"physics.ao-ph, cs.LG"
Position Paper: Towards Transparent Machine Learning,"Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward.",2019-11-12,http://arxiv.org/abs/1911.06612v1,Dustin Juliano,arxiv.org,cs.LG
Understanding Bias in Machine Learning,"Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate the impact that biased data can have on a machine learning model. To reach such a goal, we develop interactive plots to visualizing the bias learned from synthetic data.",2019-09-02,http://arxiv.org/abs/1909.01866v1,"Jindong Gu, Daniela Oelke",arxiv.org,"cs.LG, stat.ML"
A Unified Analytical Framework for Trustable Machine Learning and   Automation Running with Blockchain,"Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between machine learning technology and blockchain technology. Previously, machine learning and blockchain have been considered two independent technologies without an obvious link. Second, it proposes a unified analytical framework for trustable machine learning by using blockchain technology. This unified framework solves both the trustability and automation issues in machine learning. Third, it enables a computer to translate core machine learning implementation from a single thread on a single machine to multiple threads on multiple machines running with blockchain by using a unified approach. The paper uses association rule mining as an example to demonstrate how trustable machine learning can be implemented with blockchain, and it shows how this approach can be used to analyze opioid prescriptions to help combat the opioid crisis.",2019-03-21,http://arxiv.org/abs/1903.08801v1,Tao Wang,arxiv.org,"cs.LG, cs.CR"
MLBench: How Good Are Machine Learning Clouds for Binary Classification   Tasks on Structured Data?,"We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is possible. How good, then, are current machine learning clouds on real-world machine learning workloads?   We study this question with a focus on binary classication problems. We present mlbench, a novel benchmark constructed by harvesting datasets from Kaggle competitions. We then compare the performance of the top winning code available from Kaggle with that of running machine learning clouds from both Azure and Amazon on mlbench. Our comparative study reveals the strength and weakness of existing machine learning clouds and points out potential future directions for improvement.",2017-07-29,http://arxiv.org/abs/1707.09562v3,"Yu Liu, Hantian Zhang, Luyuan Zeng, Wentao Wu, Ce Zhang",arxiv.org,"cs.DC, cs.LG, stat.ML"
Data Pricing in Machine Learning Pipelines,"Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be successful, have to form a constructive and dynamic eco-system, marketplaces and data pricing are fundamental in connecting and facilitating those many parties. In this article, we survey the principles and the latest research development of data pricing in machine learning pipelines. We start with a brief review of data marketplaces and pricing desiderata. Then, we focus on pricing in three important steps in machine learning pipelines. To understand pricing in the step of training data collection, we review pricing raw data sets and data labels. We also investigate pricing in the step of collaborative training of machine learning models, and overview pricing machine learning models for end users in the step of machine learning deployment. We also discuss a series of possible future directions.",2021-08-18,http://arxiv.org/abs/2108.07915v1,"Zicun Cong, Xuan Luo, Pei Jian, Feida Zhu, Yong Zhang",arxiv.org,cs.LG
Techniques for Automated Machine Learning,"Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter learning (AutoMHL), and automated deep learning (AutoDL). State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches. We summarize popular AutoML frameworks and conclude with current open challenges of AutoML.",2019-07-21,http://arxiv.org/abs/1907.08908v1,"Yi-Wei Chen, Qingquan Song, Xia Hu",arxiv.org,"cs.LG, cs.AI, stat.ML"
"The Landscape of Modern Machine Learning: A Review of Machine,   Distributed and Federated Learning","With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning, deep learning as well as federated learning. As a result, our work serves as an introductory text to the vast field of modern machine learning.",2023-12-05,http://arxiv.org/abs/2312.03120v1,"Omer Subasi, Oceane Bel, Joseph Manzano, Kevin Barker",arxiv.org,"cs.LG, cs.AI, cs.DC"
Parallelization of Machine Learning Algorithms Respectively on Single   Machine and Spark,"With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and efficiency of traditional machine learning algorithms with parallelized machine learning algorithms respectively on the single machine and Spark platform. The research results have shown significant improvement in runtime and efficiency of parallelized machine learning algorithms.",2022-05-08,http://arxiv.org/abs/2206.07090v2,Jiajun Shen,arxiv.org,cs.DC
AutoCompete: A Framework for Machine Learning Competition,"In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data types, choosing a machine learn- ing model, tuning hyper-parameters, avoiding over-fitting and optimization for a provided evaluation metric. We also observe that the proposed system produces better (or comparable) results with less runtime as compared to other approaches.",2015-07-08,http://arxiv.org/abs/1507.02188v1,"Abhishek Thakur, Artus Krohn-Grimberghe",arxiv.org,"stat.ML, cs.LG"
Joint Training of Deep Boltzmann Machines,"We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks.",2012-12-12,http://arxiv.org/abs/1212.2686v1,"Ian Goodfellow, Aaron Courville, Yoshua Bengio",arxiv.org,"stat.ML, cs.LG"
Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in   Social Good Applications,"This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York.",2016-07-08,http://arxiv.org/abs/1607.02450v2,Kush R. Varshney,arxiv.org,"stat.ML, cs.CY, cs.LG"
Mathematical Perspective of Machine Learning,"We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective.",2020-07-03,http://arxiv.org/abs/2007.01503v1,Yarema Boryshchak,arxiv.org,"cs.LG, stat.ML, 68T07"
Private Machine Learning via Randomised Response,We introduce a general learning framework for private machine learning based on randomised response. Our assumption is that all actors are potentially adversarial and as such we trust only to release a single noisy version of an individual's datapoint. We discuss a general approach that forms a consistent way to estimate the true underlying machine learning model and demonstrate this in the case of logistic regression.,2020-01-14,http://arxiv.org/abs/2001.04942v2,David Barber,arxiv.org,"cs.LG, stat.ML"
A Survey of Optimization Methods from a Machine Learning Perspective,"Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this paper, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Next, we summarize the applications and developments of optimization methods in some popular machine learning fields. Finally, we explore and give some challenges and open problems for the optimization in machine learning.",2019-06-17,http://arxiv.org/abs/1906.06821v2,"Shiliang Sun, Zehui Cao, Han Zhu, Jing Zhao",arxiv.org,"cs.LG, math.OC, stat.ML"
Ten-year Survival Prediction for Breast Cancer Patients,This report assesses different machine learning approaches to 10-year survival prediction of breast cancer patients.,2019-11-02,http://arxiv.org/abs/1911.00776v1,"Changmao Li, Han He, Yunze Hao, Caleb Ziems",arxiv.org,"cs.LG, stat.ML"
Tuning Learning Rates with the Cumulative-Learning Constant,"This paper introduces a novel method for optimizing learning rates in machine learning. A previously unrecognized proportionality between learning rates and dataset sizes is discovered, providing valuable insights into how dataset scale influences training dynamics. Additionally, a cumulative learning constant is identified, offering a framework for designing and optimizing advanced learning rate schedules. These findings have the potential to enhance training efficiency and performance across a wide range of machine learning applications.",2025-04-30,http://arxiv.org/abs/2505.13457v1,Nathan Faraj,arxiv.org,cs.LG
When Machine Learning Meets Privacy: A Survey and Outlook,"The newly emerged machine learning (e.g. deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning (ML) is still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This paper surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.",2020-11-24,http://arxiv.org/abs/2011.11819v1,"Bo Liu, Ming Ding, Sina Shaham, Wenny Rahayu, Farhad Farokhi, Zihuai Lin",arxiv.org,"cs.LG, cs.AI, cs.CR"
Probabilistic Machine Learning for Healthcare,"Machine learning can be used to make sense of healthcare data. Probabilistic machine learning models help provide a complete picture of observed data in healthcare. In this review, we examine how probabilistic machine learning can advance healthcare. We consider challenges in the predictive model building pipeline where probabilistic models can be beneficial including calibration and missing data. Beyond predictive models, we also investigate the utility of probabilistic machine learning models in phenotyping, in generative models for clinical use cases, and in reinforcement learning.",2020-09-23,http://arxiv.org/abs/2009.11087v1,"Irene Y. Chen, Shalmali Joshi, Marzyeh Ghassemi, Rajesh Ranganath",arxiv.org,"stat.ML, cs.CY, cs.LG"
Evaluation Challenges for Geospatial ML,"As geospatial machine learning models and maps derived from their predictions are increasingly used for downstream analyses in science and policy, it is imperative to evaluate their accuracy and applicability. Geospatial machine learning has key distinctions from other learning paradigms, and as such, the correct way to measure performance of spatial machine learning outputs has been a topic of debate. In this paper, I delineate unique challenges of model evaluation for geospatial machine learning with global or remotely sensed datasets, culminating in concrete takeaways to improve evaluations of geospatial model performance.",2023-03-31,http://arxiv.org/abs/2303.18087v1,Esther Rolf,arxiv.org,"cs.LG, stat.ML"
A comprehensive review of Quantum Machine Learning: from NISQ to Fault   Tolerance,"Quantum machine learning, which involves running machine learning algorithms on quantum devices, has garnered significant attention in both academic and business circles. In this paper, we offer a comprehensive and unbiased review of the various concepts that have emerged in the field of quantum machine learning. This includes techniques used in Noisy Intermediate-Scale Quantum (NISQ) technologies and approaches for algorithms compatible with fault-tolerant quantum computing hardware. Our review covers fundamental concepts, algorithms, and the statistical learning theory pertinent to quantum machine learning.",2024-01-21,http://arxiv.org/abs/2401.11351v2,"Yunfei Wang, Junyu Liu",arxiv.org,"quant-ph, cs.AI, cs.LG, stat.ML"
Augmented Q Imitation Learning (AQIL),"The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learning.",2020-03-31,http://arxiv.org/abs/2004.00993v2,"Xiao Lei Zhang, Anish Agarwal",arxiv.org,"cs.LG, cs.AI"
Towards CRISP-ML(Q): A Machine Learning Process Model with Quality   Assurance Methodology,"Machine learning is an established and frequently used technique in industry and academia but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners have a need for guidance throughout the life cycle of a machine learning application to meet business expectations. We therefore propose a process model for the development of machine learning applications, that covers six phases from defining the scope to maintaining the deployed machine learning application. The first phase combines business and data understanding as data availability oftentimes affects the feasibility of the project. The sixth phase covers state-of-the-art approaches for monitoring and maintenance of a machine learning applications, as the risk of model degradation in a changing environment is eminent. With each task of the process, we propose quality assurance methodology that is suitable to adress challenges in machine learning development that we identify in form of risks. The methodology is drawn from practical experience and scientific literature and has proven to be general and stable. The process model expands on CRISP-DM, a data mining process model that enjoys strong industry support but lacks to address machine learning specific tasks. Our work proposes an industry and application neutral process model tailored for machine learning applications with focus on technical tasks for quality assurance.",2020-03-11,http://arxiv.org/abs/2003.05155v2,"Stefan Studer, Thanh Binh Bui, Christian Drescher, Alexander Hanuschkin, Ludwig Winkler, Steven Peters, Klaus-Robert Mueller",arxiv.org,"cs.LG, cs.SE, stat.ML"
Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of   learning relational order via reinforcement learning procedure?,"In this article, we extend the conventional framework of convolutional-Restricted-Boltzmann-Machine to learn highly abstract features among abitrary number of time related input maps by constructing a layer of multiplicative units, which capture the relations among inputs. In many cases, more than two maps are strongly related, so it is wise to make multiplicative unit learn relations among more input maps, in other words, to find the optimal relational-order of each unit. In order to enable our machine to learn relational order, we developed a reinforcement-learning method whose optimality is proven to train the network.",2017-06-24,http://arxiv.org/abs/1706.08001v1,Zizhuang Wang,arxiv.org,"cs.AI, cs.LG, stat.ML"
Spatial Transfer Learning with Simple MLP,First step to investigate the potential of transfer learning applied to the field of spatial statistics,2024-05-05,http://arxiv.org/abs/2405.03720v1,Hongjian Yang,arxiv.org,"cs.LG, stat.ME, stat.ML"
Proceedings of the 29th International Conference on Machine Learning   (ICML-12),"This is an index to the papers that appear in the Proceedings of the 29th International Conference on Machine Learning (ICML-12). The conference was held in Edinburgh, Scotland, June 27th - July 3rd, 2012.",2012-07-19,http://arxiv.org/abs/1207.4676v2,"John Langford, Joelle Pineau",arxiv.org,"cs.LG, stat.ML"
Distributed Multi-Task Learning with Shared Representation,"We study the problem of distributed multi-task learning with shared representation, where each machine aims to learn a separate, but related, task in an unknown shared low-dimensional subspaces, i.e. when the predictor matrix has low rank. We consider a setting where each task is handled by a different machine, with samples for the task available locally on the machine, and study communication-efficient methods for exploiting the shared structure.",2016-03-07,http://arxiv.org/abs/1603.02185v1,"Jialei Wang, Mladen Kolar, Nathan Srebro",arxiv.org,"cs.LG, stat.ML"
Components of Machine Learning: Binding Bits and FLOPS,"Many machine learning problems and methods are combinations of three components: data, hypothesis space and loss function. Different machine learning methods are obtained as combinations of different choices for the representation of data, hypothesis space and loss function. After reviewing the mathematical structure of these three components, we discuss intrinsic trade-offs between statistical and computational properties of machine learning methods.",2019-10-25,http://arxiv.org/abs/1910.12387v2,Alexander Jung,arxiv.org,cs.LG
Impact of Legal Requirements on Explainability in Machine Learning,"The requirements on explainability imposed by European laws and their implications for machine learning (ML) models are not always clear. In that perspective, our research analyzes explanation obligations imposed for private and public decision-making, and how they can be implemented by machine learning techniques.",2020-07-10,http://arxiv.org/abs/2007.05479v1,"Adrien Bibal, Michael Lognoul, Alexandre de Streel, Benoît Frénay",arxiv.org,"cs.AI, cs.CY, cs.LG"
Machine Learning Potential Repository,"This paper introduces a machine learning potential repository that includes Pareto optimal machine learning potentials. It also shows the systematic development of accurate and fast machine learning potentials for a wide range of elemental systems. As a result, many Pareto optimal machine learning potentials are available in the repository from a website. Therefore, the repository will help many scientists to perform accurate and fast atomistic simulations.",2020-07-27,http://arxiv.org/abs/2007.14206v1,Atsuto Seko,arxiv.org,"physics.comp-ph, cond-mat.mtrl-sci, physics.chem-ph, physics.data-an"
Quantum memristors for neuromorphic quantum machine learning,"Quantum machine learning may permit to realize more efficient machine learning calculations with near-term quantum devices. Among the diverse quantum machine learning paradigms which are currently being considered, quantum memristors are promising as a way of combining, in the same quantum hardware, a unitary evolution with the nonlinearity provided by the measurement and feedforward. Thus, an efficient way of deploying neuromorphic quantum computing for quantum machine learning may be enabled.",2024-12-25,http://arxiv.org/abs/2412.18979v1,Lucas Lamata,arxiv.org,"quant-ph, cs.NE"
metric-learn: Metric Learning Algorithms in Python,"metric-learn is an open source Python package implementing supervised and weakly-supervised distance metric learning algorithms. As part of scikit-learn-contrib, it provides a unified interface compatible with scikit-learn which allows to easily perform cross-validation, model selection, and pipelining with other machine learning estimators. metric-learn is thoroughly tested and available on PyPi under the MIT licence.",2019-08-13,http://arxiv.org/abs/1908.04710v3,"William de Vazelhes, CJ Carey, Yuan Tang, Nathalie Vauquier, Aurélien Bellet",arxiv.org,"cs.LG, stat.ML"
Theoretical Models of Learning to Learn,"A Machine can only learn if it is biased in some way. Typically the bias is supplied by hand, for example through the choice of an appropriate set of features. However, if the learning machine is embedded within an {\em environment} of related tasks, then it can {\em learn} its own bias by learning sufficiently many tasks from the environment. In this paper two models of bias learning (or equivalently, learning to learn) are introduced and the main theoretical results presented. The first model is a PAC-type model based on empirical process theory, while the second is a hierarchical Bayes model.",2020-02-27,http://arxiv.org/abs/2002.12364v1,Jonathan Baxter,arxiv.org,"cs.LG, stat.ML"
An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality   in Machine Learning,"We propose a clustering-based iterative algorithm to solve certain optimization problems in machine learning, where we start the algorithm by aggregating the original data, solving the problem on aggregated data, and then in subsequent steps gradually disaggregate the aggregated data. We apply the algorithm to common machine learning problems such as the least absolute deviation regression problem, support vector machines, and semi-supervised support vector machines. We derive model-specific data aggregation and disaggregation procedures. We also show optimality, convergence, and the optimality gap of the approximated solution in each iteration. A computational study is provided.",2016-07-05,http://arxiv.org/abs/1607.01400v1,"Young Woong Park, Diego Klabjan",arxiv.org,"stat.ML, cs.LG"
Human-in-the-loop Machine Learning: A Macro-Micro Perspective,"Though technical advance of artificial intelligence and machine learning has enabled many promising intelligent systems, many computing tasks are still not able to be fully accomplished by machine intelligence. Motivated by the complementary nature of human and machine intelligence, an emerging trend is to involve humans in the loop of machine learning and decision-making. In this paper, we provide a macro-micro review of human-in-the-loop machine learning. We first describe major machine learning challenges which can be addressed by human intervention in the loop. Then we examine closely the latest research and findings of introducing humans into each step of the lifecycle of machine learning. Finally, we analyze current research gaps and point out future research directions.",2022-02-21,http://arxiv.org/abs/2202.10564v1,"Jiangtao Wang, Bin Guo, Liming Chen",arxiv.org,cs.HC
Can Machines Learn the True Probabilities?,"When there exists uncertainty, AI machines are designed to make decisions so as to reach the best expected outcomes. Expectations are based on true facts about the objective environment the machines interact with, and those facts can be encoded into AI models in the form of true objective probability functions. Accordingly, AI models involve probabilistic machine learning in which the probabilities should be objectively interpreted. We prove under some basic assumptions when machines can learn the true objective probabilities, if any, and when machines cannot learn them.",2024-07-08,http://arxiv.org/abs/2407.05526v1,Jinsook Kim,arxiv.org,"cs.LG, cs.AI, stat.ML"
On-the-Fly Learning in a Perpetual Learning Machine,"Despite the promise of brain-inspired machine learning, deep neural networks (DNN) have frustratingly failed to bridge the deceptively large gap between learning and memory. Here, we introduce a Perpetual Learning Machine; a new type of DNN that is capable of brain-like dynamic 'on the fly' learning because it exists in a self-supervised state of Perpetual Stochastic Gradient Descent. Thus, we provide the means to unify learning and memory within a machine learning framework. We also explore the elegant duality of abstraction and synthesis: the Yin and Yang of deep learning.",2015-09-03,http://arxiv.org/abs/1509.00913v3,Andrew J. R. Simpson,arxiv.org,"cs.LG, 68Txx"
Scientific Machine Learning Benchmarks,"The breakthrough in Deep Learning neural networks has transformed the use of AI and machine learning technologies for the analysis of very large experimental datasets. These datasets are typically generated by large-scale experimental facilities at national laboratories. In the context of science, scientific machine learning focuses on training machines to identify patterns, trends, and anomalies to extract meaningful scientific insights from such datasets. With a new generation of experimental facilities, the rate of data generation and the scale of data volumes will increasingly require the use of more automated data analysis. At present, identifying the most appropriate machine learning algorithm for the analysis of any given scientific dataset is still a challenge for scientists. This is due to many different machine learning frameworks, computer architectures, and machine learning models. Historically, for modelling and simulation on HPC systems such problems have been addressed through benchmarking computer applications, algorithms, and architectures. Extending such a benchmarking approach and identifying metrics for the application of machine learning methods to scientific datasets is a new challenge for both scientists and computer scientists. In this paper, we describe our approach to the development of scientific machine learning benchmarks and review other approaches to benchmarking scientific machine learning.",2021-10-25,http://arxiv.org/abs/2110.12773v1,"Jeyan Thiyagalingam, Mallikarjun Shankar, Geoffrey Fox, Tony Hey",arxiv.org,"cs.LG, physics.comp-ph, I.2"
Some Insights into Lifelong Reinforcement Learning Systems,"A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system.",2020-01-27,http://arxiv.org/abs/2001.09608v1,Changjian Li,arxiv.org,"cs.LG, stat.ML"
Bayesian Optimization for Machine Learning : A Practical Guidebook,"The engineering of machine learning systems is still a nascent field; relying on a seemingly daunting collection of quickly evolving tools and best practices. It is our hope that this guidebook will serve as a useful resource for machine learning practitioners looking to take advantage of Bayesian optimization techniques. We outline four example machine learning problems that can be solved using open source machine learning libraries, and highlight the benefits of using Bayesian optimization in the context of these common machine learning applications.",2016-12-14,http://arxiv.org/abs/1612.04858v1,"Ian Dewancker, Michael McCourt, Scott Clark",arxiv.org,cs.LG
Infrastructure for Usable Machine Learning: The Stanford DAWN Project,"Despite incredible recent advances in machine learning, building machine learning applications remains prohibitively time-consuming and expensive for all but the best-trained, best-funded engineering organizations. This expense comes not from a need for new and improved statistical models but instead from a lack of systems and tools for supporting end-to-end machine learning application development, from data preparation and labeling to productionization and monitoring. In this document, we outline opportunities for infrastructure supporting usable, end-to-end machine learning applications in the context of the nascent DAWN (Data Analytics for What's Next) project at Stanford.",2017-05-22,http://arxiv.org/abs/1705.07538v2,"Peter Bailis, Kunle Olukotun, Christopher Re, Matei Zaharia",arxiv.org,"cs.LG, cs.DB, stat.ML"
Towards A Rigorous Science of Interpretable Machine Learning,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",2017-02-28,http://arxiv.org/abs/1702.08608v2,"Finale Doshi-Velez, Been Kim",arxiv.org,"stat.ML, cs.AI, cs.LG"
Solving machine learning optimization problems using quantum computers,Classical optimization algorithms in machine learning often take a long time to compute when applied to a multi-dimensional problem and require a huge amount of CPU and GPU resource. Quantum parallelism has a potential to speed up machine learning algorithms. We describe a generic mathematical model to leverage quantum parallelism to speed-up machine learning algorithms. We also apply quantum machine learning and quantum parallelism applied to a $3$-dimensional image that vary with time.,2019-11-17,http://arxiv.org/abs/1911.08587v1,"Venkat R. Dasari, Mee Seong Im, Lubjana Beshaj",arxiv.org,"quant-ph, cs.LG, stat.ML"
Teaching Uncertainty Quantification in Machine Learning through Use   Cases,"Uncertainty in machine learning is not generally taught as general knowledge in Machine Learning course curricula. In this paper we propose a short curriculum for a course about uncertainty in machine learning, and complement the course with a selection of use cases, aimed to trigger discussion and let students play with the concepts of uncertainty in a programming setting. Our use cases cover the concept of output uncertainty, Bayesian neural networks and weight distributions, sources of uncertainty, and out of distribution detection. We expect that this curriculum and set of use cases motivates the community to adopt these important concepts into courses for safety in AI.",2021-08-19,http://arxiv.org/abs/2108.08712v1,Matias Valdenegro-Toro,arxiv.org,"cs.LG, stat.ML"
Techniques for Interpretable Machine Learning,"Interpretable machine learning tackles the important problem that humans cannot understand the behaviors of complex machine learning models and how these models arrive at a particular decision. Although many approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. We provide a survey covering existing techniques to increase the interpretability of machine learning models. We also discuss crucial issues that the community should consider in future work such as designing user-friendly explanations and developing comprehensive evaluation metrics to further push forward the area of interpretable machine learning.",2018-07-31,http://arxiv.org/abs/1808.00033v3,"Mengnan Du, Ninghao Liu, Xia Hu",arxiv.org,"cs.LG, cs.AI, stat.ML"
Lale: Consistent Automated Machine Learning,"Automated machine learning makes it easier for data scientists to develop pipelines by searching over possible choices for hyperparameters, algorithms, and even pipeline topologies. Unfortunately, the syntax for automated machine learning tools is inconsistent with manual machine learning, with each other, and with error checks. Furthermore, few tools support advanced features such as topology search or higher-order operators. This paper introduces Lale, a library of high-level Python interfaces that simplifies and unifies automated machine learning in a consistent way.",2020-07-04,http://arxiv.org/abs/2007.01977v1,"Guillaume Baudart, Martin Hirzel, Kiran Kate, Parikshit Ram, Avraham Shinnar",arxiv.org,"cs.LG, cs.AI"
Differential Replication in Machine Learning,"When deployed in the wild, machine learning models are usually confronted with data and requirements that constantly vary, either because of changes in the generating distribution or because external constraints change the environment where the model operates. To survive in such an ecosystem, machine learning models need to adapt to new conditions by evolving over time. The idea of model adaptability has been studied from different perspectives. In this paper, we propose a solution based on reusing the knowledge acquired by the already deployed machine learning models and leveraging it to train future generations. This is the idea behind differential replication of machine learning models.",2020-07-15,http://arxiv.org/abs/2007.07981v1,"Irene Unceta, Jordi Nin, Oriol Pujol",arxiv.org,"cs.LG, stat.ML, cs.LG, stat.ML"
mlr3proba: An R Package for Machine Learning in Survival Analysis,"As machine learning has become increasingly popular over the last few decades, so too has the number of machine learning interfaces for implementing these models. Whilst many R libraries exist for machine learning, very few offer extended support for survival analysis. This is problematic considering its importance in fields like medicine, bioinformatics, economics, engineering, and more. mlr3proba provides a comprehensive machine learning interface for survival analysis and connects with mlr3's general model tuning and benchmarking facilities to provide a systematic infrastructure for survival modeling and evaluation.",2020-08-18,http://arxiv.org/abs/2008.08080v2,"Raphael Sonabend, Franz J. Király, Andreas Bender, Bernd Bischl, Michel Lang",arxiv.org,"stat.CO, cs.LG, stat.ML"
Introduction to Machine Learning for Physicians: A Survival Guide for   Data Deluge,"Many modern research fields increasingly rely on collecting and analysing massive, often unstructured, and unwieldy datasets. Consequently, there is growing interest in machine learning and artificial intelligence applications that can harness this `data deluge'. This broad nontechnical overview provides a gentle introduction to machine learning with a specific focus on medical and biological applications. We explain the common types of machine learning algorithms and typical tasks that can be solved, illustrating the basics with concrete examples from healthcare. Lastly, we provide an outlook on open challenges, limitations, and potential impacts of machine-learning-powered medicine.",2022-12-23,http://arxiv.org/abs/2212.12303v1,"Ričards Marcinkevičs, Ece Ozkan, Julia E. Vogt",arxiv.org,"cs.LG, stat.ML"
Machine learning-assisted close-set X-ray diffraction phase   identification of transition metals,"Machine learning has been applied to the problem of X-ray diffraction phase prediction with promising results. In this paper, we describe a method for using machine learning to predict crystal structure phases from X-ray diffraction data of transition metals and their oxides. We evaluate the performance of our method and compare the variety of its settings. Our results demonstrate that the proposed machine learning framework achieves competitive performance. This demonstrates the potential for machine learning to significantly impact the field of X-ray diffraction and crystal structure determination. Open-source implementation: https://github.com/maxnygma/NeuralXRD.",2023-04-28,http://arxiv.org/abs/2305.15410v1,"Maksim Zhdanov, Andrey Zhdanov",arxiv.org,"cond-mat.mtrl-sci, cs.AI, cs.LG"
Insights From Insurance for Fair Machine Learning,"We argue that insurance can act as an analogon for the social situatedness of machine learning systems, hence allowing machine learning scholars to take insights from the rich and interdisciplinary insurance literature. Tracing the interaction of uncertainty, fairness and responsibility in insurance provides a fresh perspective on fairness in machine learning. We link insurance fairness conceptions to their machine learning relatives, and use this bridge to problematize fairness as calibration. In this process, we bring to the forefront two themes that have been largely overlooked in the machine learning literature: responsibility and aggregate-individual tensions.",2023-06-26,http://arxiv.org/abs/2306.14624v2,"Christian Fröhlich, Robert C. Williamson",arxiv.org,"cs.LG, cs.CY"
Quantum Dynamics of Machine Learning,"The quantum dynamic equation (QDE) of machine learning is obtained based on Schr\""odinger equation and potential energy equivalence relationship. Through Wick rotation, the relationship between quantum dynamics and thermodynamics is also established in this paper. This equation reformulates the iterative process of machine learning into a time-dependent partial differential equation with a clear mathematical structure, offering a theoretical framework for investigating machine learning iterations through quantum and mathematical theories. Within this framework, the fundamental iterative process, the diffusion model, and the Softmax and Sigmoid functions are examined, validating the proposed quantum dynamics equations. This approach not only presents a rigorous theoretical foundation for machine learning but also holds promise for supporting the implementation of machine learning algorithms on quantum computers.",2024-07-07,http://arxiv.org/abs/2407.19890v1,"Peng Wang, Maimaitiniyazi Maimaitiabudula",arxiv.org,"quant-ph, cs.LG"
On the Conditions for Domain Stability for Machine Learning: a   Mathematical Approach,"This work proposes a mathematical approach that (re)defines a property of Machine Learning models named stability and determines sufficient conditions to validate it. Machine Learning models are represented as functions, and the characteristics in scope depend upon the domain of the function, what allows us to adopt topological and metric spaces theory as a basis. Finally, this work provides some equivalences useful to prove and test stability in Machine Learning models. The results suggest that whenever stability is aligned with the notion of function smoothness, then the stability of Machine Learning models primarily depends upon certain topological, measurable properties of the classification sets within the ML model domain.",2024-11-30,http://arxiv.org/abs/2412.00464v1,Gabriel Pedroza,arxiv.org,"cs.LG, cs.AI, stat.ML, F.4.1; I.2.0"
Distributed Multitask Learning,"We consider the problem of distributed multi-task learning, where each machine learns a separate, but related, task. Specifically, each machine learns a linear predictor in high-dimensional space,where all tasks share the same small support. We present a communication-efficient estimator based on the debiased lasso and show that it is comparable with the optimal centralized method.",2015-10-02,http://arxiv.org/abs/1510.00633v1,"Jialei Wang, Mladen Kolar, Nathan Srebro",arxiv.org,"stat.ML, cs.LG"
Distributed Stochastic Multi-Task Learning with Graph Regularization,"We propose methods for distributed graph-based multi-task learning that are based on weighted averaging of messages from other machines. Uniform averaging or diminishing stepsize in these methods would yield consensus (single task) learning. We show how simply skewing the averaging weights or controlling the stepsize allows learning different, but related, tasks on the different machines.",2018-02-11,http://arxiv.org/abs/1802.03830v1,"Weiran Wang, Jialei Wang, Mladen Kolar, Nathan Srebro",arxiv.org,"stat.ML, cs.LG"
Category Theory in Machine Learning,"Over the past two decades machine learning has permeated almost every realm of technology. At the same time, many researchers have begun using category theory as a unifying language, facilitating communication between different scientific disciplines. It is therefore unsurprising that there is a burgeoning interest in applying category theory to machine learning. We aim to document the motivations, goals and common themes across these applications. We touch on gradient-based learning, probability, and equivariant learning.",2021-06-13,http://arxiv.org/abs/2106.07032v1,"Dan Shiebler, Bruno Gavranović, Paul Wilson",arxiv.org,cs.LG
Energy-Harvesting Distributed Machine Learning,"This paper provides a first study of utilizing energy harvesting for sustainable machine learning in distributed networks. We consider a distributed learning setup in which a machine learning model is trained over a large number of devices that can harvest energy from the ambient environment, and develop a practical learning framework with theoretical convergence guarantees. We demonstrate through numerical experiments that the proposed framework can significantly outperform energy-agnostic benchmarks. Our framework is scalable, requires only local estimation of the energy statistics, and can be applied to a wide range of distributed training settings, including machine learning in wireless networks, edge computing, and mobile internet of things.",2021-02-10,http://arxiv.org/abs/2102.05639v1,"Basak Guler, Aylin Yener",arxiv.org,"cs.LG, cs.IT, math.IT, stat.ML"
Representation Learning for Electronic Health Records,"Information in electronic health records (EHR), such as clinical narratives, examination reports, lab measurements, demographics, and other patient encounter entries, can be transformed into appropriate data representations that can be used for downstream clinical machine learning tasks using representation learning. Learning better representations is critical to improve the performance of downstream tasks. Due to the advances in machine learning, we now can learn better and meaningful representations from EHR through disentangling the underlying factors inside data and distilling large amounts of information and knowledge from heterogeneous EHR sources. In this chapter, we first introduce the background of learning representations and reasons why we need good EHR representations in machine learning for medicine and healthcare in Section 1. Next, we explain the commonly-used machine learning and evaluation methods for representation learning using a deep learning approach in Section 2. Following that, we review recent related studies of learning patient state representation from EHR for clinical machine learning tasks in Section 3. Finally, in Section 4 we discuss more techniques, studies, and challenges for learning natural language representations when free texts, such as clinical notes, examination reports, or biomedical literature are used. We also discuss challenges and opportunities in these rapidly growing research fields.",2019-09-19,http://arxiv.org/abs/1909.09248v1,"Wei-Hung Weng, Peter Szolovits",arxiv.org,"cs.LG, stat.ML"
Meta-Learning: A Survey,"Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.",2018-10-08,http://arxiv.org/abs/1810.03548v1,Joaquin Vanschoren,arxiv.org,"cs.LG, stat.ML"
Introduction to intelligent computing unit 1,This brief note highlights some basic concepts required toward understanding the evolution of machine learning and deep learning models. The note starts with an overview of artificial intelligence and its relationship to biological neuron that ultimately led to the evolution of todays intelligent models.,2017-11-15,http://arxiv.org/abs/1711.06552v1,Isa Inuwa-Dutse,arxiv.org,"cs.LG, stat.ML"
In-Machine-Learning Database: Reimagining Deep Learning with Old-School   SQL,"In-database machine learning has been very popular, almost being a cliche. However, can we do it the other way around? In this work, we say ""yes"" by applying plain old SQL to deep learning, in a sense implementing deep learning algorithms with SQL. Most deep learning frameworks, as well as generic machine learning ones, share a de facto standard of multidimensional array operations, underneath fancier infrastructure such as automatic differentiation. As SQL tables can be regarded as generalisations of (multi-dimensional) arrays, we have found a way to express common deep learning operations in SQL, encouraging a different way of thinking and thus potentially novel models. In particular, one of the latest trend in deep learning was the introduction of sparsity in the name of graph convolutional networks, whereas we take sparsity almost for granted in the database world. As both databases and machine learning involve transformation of datasets, we hope this work can inspire further works utilizing the large body of existing wisdom, algorithms and technologies in the database field to advance the state of the art in machine learning, rather than merely integerating machine learning into databases.",2020-04-11,http://arxiv.org/abs/2004.05366v2,Len Du,arxiv.org,"cs.LG, cs.DB, stat.ML"
Machine Learning Interpretability: A Science rather than a tool,"The term ""interpretability"" is oftenly used by machine learning researchers each with their own intuitive understanding of it. There is no universal well agreed upon definition of interpretability in machine learning. As any type of science discipline is mainly driven by the set of formulated questions rather than by different tools in that discipline, e.g. astrophysics is the discipline that learns the composition of stars, not as the discipline that use the spectroscopes. Similarly, we propose that machine learning interpretability should be a discipline that answers specific questions related to interpretability. These questions can be of statistical, causal and counterfactual nature. Therefore, there is a need to look into the interpretability problem of machine learning in the context of questions that need to be addressed rather than different tools. We discuss about a hypothetical interpretability framework driven by a question based scientific approach rather than some specific machine learning model. Using a question based notion of interpretability, we can step towards understanding the science of machine learning rather than its engineering. This notion will also help us understanding any specific problem more in depth rather than relying solely on machine learning methods.",2018-07-18,http://arxiv.org/abs/1807.06722v2,"Abdul Karim, Avinash Mishra, MA Hakim Newton, Abdul Sattar",arxiv.org,"cs.LG, stat.ML"
Automated Machine Learning on Graphs: A Survey,"Machine learning on graphs has been extensively studied in both academic and industry. However, as the literature on graph learning booms with a vast number of emerging methods and techniques, it becomes increasingly difficult to manually design the optimal machine learning algorithm for different graph-related tasks. To solve this critical challenge, automated machine learning (AutoML) on graphs which combines the strength of graph machine learning and AutoML together, is gaining attention from the research community. Therefore, we comprehensively survey AutoML on graphs in this paper, primarily focusing on hyper-parameter optimization (HPO) and neural architecture search (NAS) for graph machine learning. We further overview libraries related to automated graph machine learning and in-depth discuss AutoGL, the first dedicated open-source library for AutoML on graphs. In the end, we share our insights on future research directions for automated graph machine learning. This paper is the first systematic and comprehensive review of automated machine learning on graphs to the best of our knowledge.",2021-03-01,http://arxiv.org/abs/2103.00742v4,"Ziwei Zhang, Xin Wang, Wenwu Zhu",arxiv.org,cs.LG
Can Machine Learning be Moral?,"The ethics of Machine Learning has become an unavoidable topic in the AI Community. The deployment of machine learning systems in multiple social contexts has resulted in a closer ethical scrutiny of the design, development, and application of these systems. The AI/ML community has come to terms with the imperative to think about the ethical implications of machine learning, not only as a product but also as a practice (Birhane, 2021; Shen et al. 2021). The critical question that is troubling many debates is what can constitute an ethically accountable machine learning system. In this paper we explore possibilities for ethical evaluation of machine learning methodologies. We scrutinize techniques, methods and technical practices in machine learning from a relational ethics perspective, taking into consideration how machine learning systems are part of the world and how they relate to different forms of agency. Taking a page from Phil Agre (1997) we use the notion of a critical technical practice as a means of analysis of machine learning approaches. Our radical proposal is that supervised learning appears to be the only machine learning method that is ethically defensible.",2021-12-13,http://arxiv.org/abs/2201.06921v1,"Miguel Sicart, Irina Shklovski, Mirabelle Jones",arxiv.org,"cs.CY, cs.HC"
Compressive Classification (Machine Learning without learning),"Compressive learning is a framework where (so far unsupervised) learning tasks use not the entire dataset but a compressed summary (sketch) of it. We propose a compressive learning classification method, and a novel sketch function for images.",2018-12-04,http://arxiv.org/abs/1812.01410v1,"Vincent Schellekens, Laurent Jacques",arxiv.org,"cs.LG, cs.CV, stat.ML"
A Survey on Resilient Machine Learning,"Machine learning based system are increasingly being used for sensitive tasks such as security surveillance, guiding autonomous vehicle, taking investment decisions, detecting and blocking network intrusion and malware etc. However, recent research has shown that machine learning models are venerable to attacks by adversaries at all phases of machine learning (eg, training data collection, training, operation). All model classes of machine learning systems can be misled by providing carefully crafted inputs making them wrongly classify inputs. Maliciously created input samples can affect the learning process of a ML system by either slowing down the learning process, or affecting the performance of the learned mode, or causing the system make error(s) only in attacker's planned scenario. Because of these developments, understanding security of machine learning algorithms and systems is emerging as an important research area among computer security and machine learning researchers and practitioners. We present a survey of this emerging area in machine learning.",2017-07-11,http://arxiv.org/abs/1707.03184v1,"Atul Kumar, Sameep Mehta",arxiv.org,"cs.AI, cs.CR, cs.LG"
An Introduction to MM Algorithms for Machine Learning and Statistical,"MM (majorization--minimization) algorithms are an increasingly popular tool for solving optimization problems in machine learning and statistical estimation. This article introduces the MM algorithm framework in general and via three popular example applications: Gaussian mixture regressions, multinomial logistic regressions, and support vector machines. Specific algorithms for the three examples are derived and numerical demonstrations are presented. Theoretical and practical aspects of MM algorithm design are discussed.",2016-11-12,http://arxiv.org/abs/1611.03969v1,Hien D. Nguyen,arxiv.org,"stat.CO, cs.LG, stat.ML"
Some Requests for Machine Learning Research from the East African Tech   Scene,"Based on 46 in-depth interviews with scientists, engineers, and CEOs, this document presents a list of concrete machine research problems, progress on which would directly benefit tech ventures in East Africa.",2018-10-25,http://arxiv.org/abs/1810.11383v2,Milan Cvitkovic,arxiv.org,"cs.LG, cs.CY, stat.ML"
Machine learning and deep learning,"Today, intelligent systems that offer artificial intelligence capabilities often rely on machine learning. Machine learning describes the capacity of systems to learn from problem-specific training data to automate the process of analytical model building and solve associated tasks. Deep learning is a machine learning concept based on artificial neural networks. For many applications, deep learning models outperform shallow machine learning models and traditional data analysis approaches. In this article, we summarize the fundamentals of machine learning and deep learning to generate a broader understanding of the methodical underpinning of current intelligent systems. In particular, we provide a conceptual distinction between relevant terms and concepts, explain the process of automated analytical model building through machine learning and deep learning, and discuss the challenges that arise when implementing such intelligent systems in the field of electronic markets and networked business. These naturally go beyond technological aspects and highlight issues in human-machine interaction and artificial intelligence servitization.",2021-04-12,http://arxiv.org/abs/2104.05314v2,"Christian Janiesch, Patrick Zschech, Kai Heinrich",arxiv.org,cs.AI
Application of Machine Learning Techniques in Aquaculture,"In this paper we present applications of different machine learning algorithms in aquaculture. Machine learning algorithms learn models from historical data. In aquaculture historical data are obtained from farm practices, yields, and environmental data sources. Associations between these different variables can be obtained by applying machine learning algorithms to historical data. In this paper we present applications of different machine learning algorithms in aquaculture applications.",2014-05-03,http://arxiv.org/abs/1405.1304v1,"Akhlaqur Rahman, Sumaira Tasnim",arxiv.org,"cs.CE, cs.LG"
TF.Learn: TensorFlow's High-level Module for Distributed Machine   Learning,"TF.Learn is a high-level Python module for distributed machine learning inside TensorFlow. It provides an easy-to-use Scikit-learn style interface to simplify the process of creating, configuring, training, evaluating, and experimenting a machine learning model. TF.Learn integrates a wide range of state-of-art machine learning algorithms built on top of TensorFlow's low level APIs for small to large-scale supervised and unsupervised problems. This module focuses on bringing machine learning to non-specialists using a general-purpose high-level language as well as researchers who want to implement, benchmark, and compare their new methods in a structured environment. Emphasis is put on ease of use, performance, documentation, and API consistency.",2016-12-13,http://arxiv.org/abs/1612.04251v1,Yuan Tang,arxiv.org,"cs.DC, cs.LG"
Machine Learning as Ecology,"Machine learning methods have had spectacular success on numerous problems. Here we show that a prominent class of learning algorithms - including Support Vector Machines (SVMs) -- have a natural interpretation in terms of ecological dynamics. We use these ideas to design new online SVM algorithms that exploit ecological invasions, and benchmark performance using the MNIST dataset. Our work provides a new ecological lens through which we can view statistical learning and opens the possibility of designing ecosystems for machine learning.   Supplemental code is found at https://github.com/owenhowell20/EcoSVM.",2019-08-02,http://arxiv.org/abs/1908.00868v2,"Owen Howell, Cui Wenping, Robert Marsland III, Pankaj Mehta",arxiv.org,"cs.LG, cond-mat.stat-mech, stat.ML"
Machine Learning in Network Security Using KNIME Analytics,"Machine learning has more and more effect on our every day's life. This field keeps growing and expanding into new areas. Machine learning is based on the implementation of artificial intelligence that gives systems the capability to automatically learn and enhance from experiments without being explicitly programmed. Machine Learning algorithms apply mathematical equations to analyze datasets and predict values based on the dataset. In the field of cybersecurity, machine learning algorithms can be utilized to train and analyze the Intrusion Detection Systems (IDSs) on security-related datasets. In this paper, we tested different machine learning algorithms to analyze NSL-KDD dataset using KNIME analytics.",2019-11-18,http://arxiv.org/abs/2001.11489v1,Munther Abualkibash,arxiv.org,cs.CR
SELM: Software Engineering of Machine Learning Models,"One of the pillars of any machine learning model is its concepts. Using software engineering, we can engineer these concepts and then develop and expand them. In this article, we present a SELM framework for Software Engineering of machine Learning Models. We then evaluate this framework through a case study. Using the SELM framework, we can improve a machine learning process efficiency and provide more accuracy in learning with less processing hardware resources and a smaller training dataset. This issue highlights the importance of an interdisciplinary approach to machine learning. Therefore, in this article, we have provided interdisciplinary teams' proposals for machine learning.",2021-03-20,http://arxiv.org/abs/2103.11249v1,"Nafiseh Jafari, Mohammad Reza Besharati, Mohammad Izadi, Maryam Hourali",arxiv.org,"cs.SE, cs.AI"
Using Deep Learning and Machine Learning to Detect Epileptic Seizure   with Electroencephalography (EEG) Data,"The prediction of epileptic seizure has always been extremely challenging in medical domain. However, as the development of computer technology, the application of machine learning introduced new ideas for seizure forecasting. Applying machine learning model onto the predication of epileptic seizure could help us obtain a better result and there have been plenty of scientists who have been doing such works so that there are sufficient medical data provided for researchers to do training of machine learning models.",2019-10-06,http://arxiv.org/abs/1910.02544v1,"Haotian Liu, Lin Xi, Ying Zhao, Zhixiang Li",arxiv.org,"cs.LG, eess.SP, stat.ML"
Challenges and Opportunities in Quantum Machine Learning,"At the intersection of machine learning and quantum computing, Quantum Machine Learning (QML) has the potential of accelerating data analysis, especially for quantum data, with applications for quantum materials, biochemistry, and high-energy physics. Nevertheless, challenges remain regarding the trainability of QML models. Here we review current methods and applications for QML. We highlight differences between quantum and classical machine learning, with a focus on quantum neural networks and quantum deep learning. Finally, we discuss opportunities for quantum advantage with QML.",2023-03-16,http://arxiv.org/abs/2303.09491v1,"M. Cerezo, Guillaume Verdon, Hsin-Yuan Huang, Lukasz Cincio, Patrick J. Coles",arxiv.org,"quant-ph, cs.LG, stat.ML"
A Theory of Machine Learning,"We critically review three major theories of machine learning and provide a new theory according to which machines learn a function when the machines successfully compute it. We show that this theory challenges common assumptions in the statistical and the computational learning theories, for it implies that learning true probabilities is equivalent neither to obtaining a correct calculation of the true probabilities nor to obtaining an almost-sure convergence to them. We also briefly discuss some case studies from natural language processing and macroeconomics from the perspective of the new theory.",2024-07-07,http://arxiv.org/abs/2407.05520v1,"Jinsook Kim, Jinho Kang",arxiv.org,"cs.LG, stat.ML"
How Developers Iterate on Machine Learning Workflows -- A Survey of the   Applied Machine Learning Literature,"Machine learning workflow development is anecdotally regarded to be an iterative process of trial-and-error with humans-in-the-loop. However, we are not aware of quantitative evidence corroborating this popular belief. A quantitative characterization of iteration can serve as a benchmark for machine learning workflow development in practice, and can aid the development of human-in-the-loop machine learning systems. To this end, we conduct a small-scale survey of the applied machine learning literature from five distinct application domains. We collect and distill statistics on the role of iteration within machine learning workflow development, and report preliminary trends and insights from our investigation, as a starting point towards this benchmark. Based on our findings, we finally describe desiderata for effective and versatile human-in-the-loop machine learning systems that can cater to users in diverse domains.",2018-03-27,http://arxiv.org/abs/1803.10311v2,"Doris Xin, Litian Ma, Shuchen Song, Aditya Parameswaran",arxiv.org,"cs.LG, cs.DB, cs.HC, stat.ML"
Practical Solutions for Machine Learning Safety in Autonomous Vehicles,"Autonomous vehicles rely on machine learning to solve challenging tasks in perception and motion planning. However, automotive software safety standards have not fully evolved to address the challenges of machine learning safety such as interpretability, verification, and performance limitations. In this paper, we review and organize practical machine learning safety techniques that can complement engineering safety for machine learning based software in autonomous vehicles. Our organization maps safety strategies to state-of-the-art machine learning techniques in order to enhance dependability and safety of machine learning algorithms. We also discuss security limitations and user experience aspects of machine learning components in autonomous vehicles.",2019-12-20,http://arxiv.org/abs/1912.09630v1,"Sina Mohseni, Mandar Pitale, Vasu Singh, Zhangyang Wang",arxiv.org,"cs.LG, stat.ML"
Learning Moore Machines from Input-Output Traces,"The problem of learning automata from example traces (but no equivalence or membership queries) is fundamental in automata learning theory and practice. In this paper we study this problem for finite state machines with inputs and outputs, and in particular for Moore machines. We develop three algorithms for solving this problem: (1) the PTAP algorithm, which transforms a set of input-output traces into an incomplete Moore machine and then completes the machine with self-loops; (2) the PRPNI algorithm, which uses the well-known RPNI algorithm for automata learning to learn a product of automata encoding a Moore machine; and (3) the MooreMI algorithm, which directly learns a Moore machine using PTAP extended with state merging. We prove that MooreMI has the fundamental identification in the limit property. We also compare the algorithms experimentally in terms of the size of the learned machine and several notions of accuracy, introduced in this paper. Finally, we compare with OSTIA, an algorithm that learns a more general class of transducers, and find that OSTIA generally does not learn a Moore machine, even when fed with a characteristic sample.",2016-05-25,http://arxiv.org/abs/1605.07805v2,"Georgios Giantamidis, Stavros Tripakis",arxiv.org,"cs.FL, cs.LG"
Modeling Generalization in Machine Learning: A Methodological and   Computational Study,"As machine learning becomes more and more available to the general public, theoretical questions are turning into pressing practical issues. Possibly, one of the most relevant concerns is the assessment of our confidence in trusting machine learning predictions. In many real-world cases, it is of utmost importance to estimate the capabilities of a machine learning algorithm to generalize, i.e., to provide accurate predictions on unseen data, depending on the characteristics of the target problem. In this work, we perform a meta-analysis of 109 publicly-available classification data sets, modeling machine learning generalization as a function of a variety of data set characteristics, ranging from number of samples to intrinsic dimensionality, from class-wise feature skewness to $F1$ evaluated on test samples falling outside the convex hull of the training set. Experimental results demonstrate the relevance of using the concept of the convex hull of the training data in assessing machine learning generalization, by emphasizing the difference between interpolated and extrapolated predictions. Besides several predictable correlations, we observe unexpectedly weak associations between the generalization ability of machine learning models and all metrics related to dimensionality, thus challenging the common assumption that the \textit{curse of dimensionality} might impair generalization in machine learning.",2020-06-28,http://arxiv.org/abs/2006.15680v1,"Pietro Barbiero, Giovanni Squillero, Alberto Tonda",arxiv.org,"cs.LG, stat.ML"
"Julia Language in Machine Learning: Algorithms, Applications, and Open   Issues","Machine learning is driving development across many fields in science and engineering. A simple and efficient programming language could accelerate applications of machine learning in various fields. Currently, the programming languages most commonly used to develop machine learning algorithms include Python, MATLAB, and C/C ++. However, none of these languages well balance both efficiency and simplicity. The Julia language is a fast, easy-to-use, and open-source programming language that was originally designed for high-performance computing, which can well balance the efficiency and simplicity. This paper summarizes the related research work and developments in the application of the Julia language in machine learning. It first surveys the popular machine learning algorithms that are developed in the Julia language. Then, it investigates applications of the machine learning algorithms implemented with the Julia language. Finally, it discusses the open issues and the potential future directions that arise in the use of the Julia language in machine learning.",2020-03-23,http://arxiv.org/abs/2003.10146v2,"Kaifeng Gao, Gang Mei, Francesco Piccialli, Salvatore Cuomo, Jingzhi Tu, Zenan Huo",arxiv.org,"cs.LG, stat.ML"
Mental Models of Adversarial Machine Learning,"Although machine learning is widely used in practice, little is known about practitioners' understanding of potential security challenges. In this work, we close this substantial gap and contribute a qualitative study focusing on developers' mental models of the machine learning pipeline and potentially vulnerable components. Similar studies have helped in other security fields to discover root causes or improve risk communication. Our study reveals two \facets of practitioners' mental models of machine learning security. Firstly, practitioners often confuse machine learning security with threats and defences that are not directly related to machine learning. Secondly, in contrast to most academic research, our participants perceive security of machine learning as not solely related to individual models, but rather in the context of entire workflows that consist of multiple components. Jointly with our additional findings, these two facets provide a foundation to substantiate mental models for machine learning security and have implications for the integration of adversarial machine learning into corporate workflows, \new{decreasing practitioners' reported uncertainty}, and appropriate regulatory frameworks for machine learning security.",2021-05-08,http://arxiv.org/abs/2105.03726v4,"Lukas Bieringer, Kathrin Grosse, Michael Backes, Battista Biggio, Katharina Krombholz",arxiv.org,"cs.CR, cs.AI"
Applying Machine Learning to Life Insurance: some knowledge sharing to   master it,"Machine Learning permeates many industries, which brings new source of benefits for companies. However within the life insurance industry, Machine Learning is not widely used in practice as over the past years statistical models have shown their efficiency for risk assessment. Thus insurers may face difficulties to assess the value of the artificial intelligence. Focusing on the modification of the life insurance industry over time highlights the stake of using Machine Learning for insurers and benefits that it can bring by unleashing data value. This paper reviews traditional actuarial methodologies for survival modeling and extends them with Machine Learning techniques. It points out differences with regular machine learning models and emphasizes importance of specific implementations to face censored data with machine learning models family. In complement to this article, a Python library has been developed. Different open-source Machine Learning algorithms have been adjusted to adapt the specificities of life insurance data, namely censoring and truncation. Such models can be easily applied from this SCOR library to accurately model life insurance risks.",2022-09-05,http://arxiv.org/abs/2209.02057v3,"Antoine Chancel, Laura Bradier, Antoine Ly, Razvan Ionescu, Laurene Martin, Marguerite Sauce",arxiv.org,"stat.ML, cs.CY, cs.LG, stat.AP"
Machine learning and domain decomposition methods -- a survey,"Hybrid algorithms, which combine black-box machine learning methods with experience from traditional numerical methods and domain expertise from diverse application areas, are progressively gaining importance in scientific machine learning and various industrial domains, especially in computational science and engineering. In the present survey, several promising avenues of research will be examined which focus on the combination of machine learning (ML) and domain decomposition methods (DDMs). The aim of this survey is to provide an overview of existing work within this field and to structure it into domain decomposition for machine learning and machine learning-enhanced domain decomposition, including: domain decomposition for classical machine learning, domain decomposition to accelerate the training of physics-aware neural networks, machine learning to enhance the convergence properties or computational efficiency of DDMs, and machine learning as a discretization method in a DDM for the solution of PDEs. In each of these fields, we summarize existing work and key advances within a common framework and, finally, disuss ongoing challenges and opportunities for future research.",2023-12-21,http://arxiv.org/abs/2312.14050v1,"Axel Klawonn, Martin Lanser, Janine Weber",arxiv.org,"math.NA, cs.LG, cs.NA, 65F10, 65N22, 65N55, 68T05, 68T07"
Beyond Model Interpretability: Socio-Structural Explanations in Machine   Learning,"What is it to interpret the outputs of an opaque machine learning model. One approach is to develop interpretable machine learning techniques. These techniques aim to show how machine learning models function by providing either model centric local or global explanations, which can be based on mechanistic interpretations revealing the inner working mechanisms of models or nonmechanistic approximations showing input feature output data relationships. In this paper, we draw on social philosophy to argue that interpreting machine learning outputs in certain normatively salient domains could require appealing to a third type of explanation that we call sociostructural explanation. The relevance of this explanation type is motivated by the fact that machine learning models are not isolated entities but are embedded within and shaped by social structures. Sociostructural explanations aim to illustrate how social structures contribute to and partially explain the outputs of machine learning models. We demonstrate the importance of sociostructural explanations by examining a racially biased healthcare allocation algorithm. Our proposal highlights the need for transparency beyond model interpretability, understanding the outputs of machine learning systems could require a broader analysis that extends beyond the understanding of the machine learning model itself.",2024-09-05,http://arxiv.org/abs/2409.03632v1,"Andrew Smart, Atoosa Kasirzadeh",arxiv.org,cs.LG
Aspects of Artificial Intelligence: Transforming Machine Learning   Systems Naturally,"In this paper, we study the machine learning elements which we are interested in together as a machine learning system, consisting of a collection of machine learning elements and a collection of relations between the elements. The relations we concern are algebraic operations, binary relations, and binary relations with composition that can be reasoned categorically. A machine learning system transformation between two systems is a map between the systems, which preserves the relations we concern. The system transformations given by quotient or clustering, representable functor, and Yoneda embedding are highlighted and discussed by machine learning examples. An adjunction between machine learning systems, a special machine learning system transformation loop, provides the optimal way of solving problems. Machine learning system transformations are linked and compared by their maps at 2-cell, natural transformations. New insights and structures can be obtained from universal properties and algebraic structures given by monads, which are generated from adjunctions.",2025-02-03,http://arxiv.org/abs/2502.01708v1,Xiuzhan Guo,arxiv.org,"cs.LG, cs.AI, cs.DB, cs.DM"
Domain Knowledge in Artificial Intelligence: Using Conceptual Modeling   to Increase Machine Learning Accuracy and Explainability,"Machine learning enables the extraction of useful information from large, diverse datasets. However, despite many successful applications, machine learning continues to suffer from performance and transparency issues. These challenges can be partially attributed to the limited use of domain knowledge by machine learning models. This research proposes using the domain knowledge represented in conceptual models to improve the preparation of the data used to train machine learning models. We develop and demonstrate a method, called the Conceptual Modeling for Machine Learning (CMML), which is comprised of guidelines for data preparation in machine learning and based on conceptual modeling constructs and principles. To assess the impact of CMML on machine learning outcomes, we first applied it to two real-world problems to evaluate its impact on model performance. We then solicited an assessment by data scientists on the applicability of the method. These results demonstrate the value of CMML for improving machine learning outcomes.",2025-06-25,http://arxiv.org/abs/2507.02922v1,"V. C. Storey, J. Parsons, A. Castellanos, M. Tremblay, R. Lukyanenko, W. Maass, A. Castillo",arxiv.org,"cs.LG, cs.HC"
Financial Time Series Data Processing for Machine Learning,"This article studies the financial time series data processing for machine learning. It introduces the most frequent scaling methods, then compares the resulting stationarity and preservation of useful information for trend forecasting. It proposes an empirical test based on the capability to learn simple data relationship with simple models. It also speaks about the data split method specific to time series, avoiding unwanted overfitting and proposes various labelling for classification and regression.",2019-07-03,http://arxiv.org/abs/1907.03010v1,Fabrice Daniel,arxiv.org,"q-fin.ST, cs.LG, stat.ML"
Learning Theory and Support Vector Machines - a primer,"The main goal of statistical learning theory is to provide a fundamental framework for the problem of decision making and model construction based on sets of data. Here, we present a brief introduction to the fundamentals of statistical learning theory, in particular the difference between empirical and structural risk minimization, including one of its most prominent implementations, i.e. the Support Vector Machine.",2019-02-12,http://arxiv.org/abs/1902.04622v1,Michael Banf,arxiv.org,"cs.LG, stat.ML"
Machine Learning using Stata/Python,"We present two related Stata modules, r_ml_stata and c_ml_stata, for fitting popular Machine Learning (ML) methods both in regression and classification settings. Using the recent Stata/Python integration platform (sfi) of Stata 16, these commands provide hyper-parameters' optimal tuning via K-fold cross-validation using greed search. More specifically, they make use of the Python Scikit-learn API to carry out both cross-validation and outcome/label prediction.",2021-03-03,http://arxiv.org/abs/2103.03122v1,Giovanni Cerulli,arxiv.org,"stat.CO, cs.LG, cs.MS"
Pen and Paper Exercises in Machine Learning,"This is a collection of (mostly) pen-and-paper exercises in machine learning. The exercises are on the following topics: linear algebra, optimisation, directed graphical models, undirected graphical models, expressive power of graphical models, factor graphs and message passing, inference for hidden Markov models, model-based learning (including ICA and unnormalised models), sampling and Monte-Carlo integration, and variational inference.",2022-06-27,http://arxiv.org/abs/2206.13446v1,Michael U. Gutmann,arxiv.org,"cs.LG, stat.ML"
Classic machine learning methods,"In this chapter, we present the main classic machine learning methods. A large part of the chapter is devoted to supervised learning techniques for classification and regression, including nearest-neighbor methods, linear and logistic regressions, support vector machines and tree-based algorithms. We also describe the problem of overfitting as well as strategies to overcome it. We finally provide a brief overview of unsupervised learning methods, namely for clustering and dimensionality reduction.",2023-05-24,http://arxiv.org/abs/2310.11470v1,"Johann Faouzi, Olivier Colliot",arxiv.org,"cs.LG, cs.AI"
Information Theory and its Relation to Machine Learning,"In this position paper, I first describe a new perspective on machine learning (ML) by four basic problems (or levels), namely, ""What to learn?"", ""How to learn?"", ""What to evaluate?"", and ""What to adjust?"". The paper stresses more on the first level of ""What to learn?"", or ""Learning Target Selection"". Towards this primary problem within the four levels, I briefly review the existing studies about the connection between information theoretical learning (ITL [1]) and machine learning. A theorem is given on the relation between the empirically-defined similarity measure and information measures. Finally, a conjecture is proposed for pursuing a unified mathematical interpretation to learning target selection.",2015-01-18,http://arxiv.org/abs/1501.04309v1,Bao-Gang Hu,arxiv.org,"cs.IT, cs.LG, math.IT"
Discussion on Mechanical Learning and Learning Machine,"Mechanical learning is a computing system that is based on a set of simple and fixed rules, and can learn from incoming data. A learning machine is a system that realizes mechanical learning. Importantly, we emphasis that it is based on a set of simple and fixed rules, contrasting to often called machine learning that is sophisticated software based on very complicated mathematical theory, and often needs human intervene for software fine tune and manual adjustments. Here, we discuss some basic facts and principles of such system, and try to lay down a framework for further study. We propose 2 directions to approach mechanical learning, just like Church-Turing pair: one is trying to realize a learning machine, another is trying to well describe the mechanical learning.",2016-01-31,http://arxiv.org/abs/1602.00198v1,Chuyu Xiong,arxiv.org,cs.AI
A systematic review of fuzzing based on machine learning techniques,"Security vulnerabilities play a vital role in network security system. Fuzzing technology is widely used as a vulnerability discovery technology to reduce damage in advance. However, traditional fuzzing techniques have many challenges, such as how to mutate input seed files, how to increase code coverage, and how to effectively bypass verification. Machine learning technology has been introduced as a new method into fuzzing test to alleviate these challenges. This paper reviews the research progress of using machine learning technology for fuzzing test in recent years, analyzes how machine learning improve the fuzz process and results, and sheds light on future work in fuzzing. Firstly, this paper discusses the reasons why machine learning techniques can be used for fuzzing scenarios and identifies six different stages in which machine learning have been used. Then this paper systematically study the machine learning based fuzzing models from selection of machine learning algorithm, pre-processing methods, datasets, evaluation metrics, and hyperparameters setting. Next, this paper assesses the performance of the machine learning models based on the frequently used evaluation metrics. The results of the evaluation prove that machine learning technology has an acceptable capability of categorize predictive for fuzzing. Finally, the comparison on capability of discovering vulnerabilities between traditional fuzzing tools and machine learning based fuzzing tools is analyzed. The results depict that the introduction of machine learning technology can improve the performance of fuzzing. However, there are still some limitations, such as unbalanced training samples and difficult to extract the characteristics related to vulnerabilities.",2019-08-04,http://arxiv.org/abs/1908.01262v1,"Yan Wang, Peng Jia, Luping Liu, Jiayong Liu",arxiv.org,"cs.CR, cs.LG"
Human-Like Active Learning: Machines Simulating the Human Learning   Process,"Although the use of active learning to increase learners' engagement has recently been introduced in a variety of methods, empirical experiments are lacking. In this study, we attempted to align two experiments in order to (1) make a hypothesis for machine and (2) empirically confirm the effect of active learning on learning. In Experiment 1, we compared the effect of a passive form of learning to active form of learning. The results showed that active learning had a greater learning outcomes than passive learning. In the machine experiment based on the human result, we imitated the human active learning as a form of knowledge distillation. The active learning framework performed better than the passive learning framework. In the end, we showed not only that we can make build better machine training framework through the human experiment result, but also empirically confirm the result of human experiment through imitated machine experiments; human-like active learning have crucial effect on learning performance.",2020-11-07,http://arxiv.org/abs/2011.03733v1,"Jaeseo Lim, Hwiyeol Jo, Byoung-Tak Zhang, Jooyong Park",arxiv.org,cs.LG
Matched Machine Learning: A Generalized Framework for Treatment Effect   Inference With Learned Metrics,"We introduce Matched Machine Learning, a framework that combines the flexibility of machine learning black boxes with the interpretability of matching, a longstanding tool in observational causal inference. Interpretability is paramount in many high-stakes application of causal inference. Current tools for nonparametric estimation of both average and individualized treatment effects are black-boxes that do not allow for human auditing of estimates. Our framework uses machine learning to learn an optimal metric for matching units and estimating outcomes, thus achieving the performance of machine learning black-boxes, while being interpretable. Our general framework encompasses several published works as special cases. We provide asymptotic inference theory for our proposed framework, enabling users to construct approximate confidence intervals around estimates of both individualized and average treatment effects. We show empirically that instances of Matched Machine Learning perform on par with black-box machine learning methods and better than existing matching methods for similar problems. Finally, in our application we show how Matched Machine Learning can be used to perform causal inference even when covariate data are highly complex: we study an image dataset, and produce high quality matches and estimates of treatment effects.",2023-04-03,http://arxiv.org/abs/2304.01316v1,"Marco Morucci, Cynthia Rudin, Alexander Volfovsky",arxiv.org,"stat.ME, stat.ML"
Quantum-enhanced machine learning,"The emerging field of quantum machine learning has the potential to substantially aid in the problems and scope of artificial intelligence. This is only enhanced by recent successes in the field of classical machine learning. In this work we propose an approach for the systematic treatment of machine learning, from the perspective of quantum information. Our approach is general and covers all three main branches of machine learning: supervised, unsupervised and reinforcement learning. While quantum improvements in supervised and unsupervised learning have been reported, reinforcement learning has received much less attention. Within our approach, we tackle the problem of quantum enhancements in reinforcement learning as well, and propose a systematic scheme for providing improvements. As an example, we show that quadratic improvements in learning efficiency, and exponential improvements in performance over limited time periods, can be obtained for a broad class of learning problems.",2016-10-26,http://arxiv.org/abs/1610.08251v1,"Vedran Dunjko, Jacob M. Taylor, Hans J. Briegel",arxiv.org,"quant-ph, cs.AI, cs.LG"
Generalization Guarantees for a Binary Classification Framework for   Two-Stage Multiple Kernel Learning,We present generalization bounds for the TS-MKL framework for two stage multiple kernel learning. We also present bounds for sparse kernel learning formulations within the TS-MKL framework.,2013-02-02,http://arxiv.org/abs/1302.0406v1,Purushottam Kar,arxiv.org,"cs.LG, stat.ML"
Beneficial and Harmful Explanatory Machine Learning,"Given the recent successes of Deep Learning in AI there has been increased interest in the role and need for explanations in machine learned theories. A distinct notion in this context is that of Michie's definition of Ultra-Strong Machine Learning (USML). USML is demonstrated by a measurable increase in human performance of a task following provision to the human of a symbolic machine learned theory for task performance. A recent paper demonstrates the beneficial effect of a machine learned logic theory for a classification task, yet no existing work to our knowledge has examined the potential harmfulness of machine's involvement for human comprehension during learning. This paper investigates the explanatory effects of a machine learned theory in the context of simple two person games and proposes a framework for identifying the harmfulness of machine explanations based on the Cognitive Science literature. The approach involves a cognitive window consisting of two quantifiable bounds and it is supported by empirical evidence collected from human trials. Our quantitative and qualitative results indicate that human learning aided by a symbolic machine learned theory which satisfies a cognitive window has achieved significantly higher performance than human self learning. Results also demonstrate that human learning aided by a symbolic machine learned theory that fails to satisfy this window leads to significantly worse performance than unaided human learning.",2020-09-09,http://arxiv.org/abs/2009.06410v2,"Lun Ai, Stephen H. Muggleton, Céline Hocquette, Mark Gromowski, Ute Schmid",arxiv.org,"cs.AI, cs.LG"
Logistic Regression as Soft Perceptron Learning,"We comment on the fact that gradient ascent for logistic regression has a connection with the perceptron learning algorithm. Logistic learning is the ""soft"" variant of perceptron learning.",2017-08-24,http://arxiv.org/abs/1708.07826v1,Raul Rojas,arxiv.org,"stat.ML, 62M45, 68Q32, K.3.2; I.5.1"
Learning proofs for the classification of nilpotent semigroups,"Machine learning is applied to find proofs, with smaller or smallest numbers of nodes, for the classification of 4-nilpotent semigroups.",2021-06-06,http://arxiv.org/abs/2106.03015v1,Carlos Simpson,arxiv.org,"cs.LG, math.LO, math.RA, 68T15 (Primary) 20M10, 03F07, 03B35 (Secondary)"
PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python,"Machine learning is a general-purpose technology holding promises for many interdisciplinary research problems. However, significant barriers exist in crossing disciplinary boundaries when most machine learning tools are developed in different areas separately. We present Pykale - a Python library for knowledge-aware machine learning on graphs, images, texts, and videos to enable and accelerate interdisciplinary research. We formulate new green machine learning guidelines based on standard software engineering practices and propose a novel pipeline-based application programming interface (API). PyKale focuses on leveraging knowledge from multiple sources for accurate and interpretable prediction, thus supporting multimodal learning and transfer learning (particularly domain adaptation) with latest deep learning and dimensionality reduction models. We build PyKale on PyTorch and leverage the rich PyTorch ecosystem. Our pipeline-based API design enforces standardization and minimalism, embracing green machine learning concepts via reducing repetitions and redundancy, reusing existing resources, and recycling learning models across areas. We demonstrate its interdisciplinary nature via examples in bioinformatics, knowledge graph, image/video recognition, and medical imaging.",2021-06-17,http://arxiv.org/abs/2106.09756v1,"Haiping Lu, Xianyuan Liu, Robert Turner, Peizhen Bai, Raivo E Koot, Shuo Zhou, Mustafa Chasmai, Lawrence Schobs",arxiv.org,"cs.LG, cs.AI, cs.CV, stat.ML"
Is 'Unsupervised Learning' a Misconceived Term?,"Is all of machine learning supervised to some degree? The field of machine learning has traditionally been categorized pedagogically into $supervised~vs~unsupervised~learning$; where supervised learning has typically referred to learning from labeled data, while unsupervised learning has typically referred to learning from unlabeled data. In this paper, we assert that all machine learning is in fact supervised to some degree, and that the scope of supervision is necessarily commensurate to the scope of learning potential. In particular, we argue that clustering algorithms such as k-means, and dimensionality reduction algorithms such as principal component analysis, variational autoencoders, and deep belief networks are each internally supervised by the data itself to learn their respective representations of its features. Furthermore, these algorithms are not capable of external inference until their respective outputs (clusters, principal components, or representation codes) have been identified and externally labeled in effect. As such, they do not suffice as examples of unsupervised learning. We propose that the categorization `supervised vs unsupervised learning' be dispensed with, and instead, learning algorithms be categorized as either $internally~or~externally~supervised$ (or both). We believe this change in perspective will yield new fundamental insights into the structure and character of data and of learning algorithms.",2019-04-05,http://arxiv.org/abs/1904.03259v1,Stephen G. Odaibo,arxiv.org,"cs.LG, cs.AI, cs.CV, stat.ML"
Deep Learning and Its Applications to Machine Health Monitoring: A   Survey,"Since 2006, deep learning (DL) has become a rapidly growing research direction, redefining state-of-the-art performances in a wide range of areas such as object recognition, image segmentation, speech recognition and machine translation. In modern manufacturing systems, data-driven machine health monitoring is gaining in popularity due to the widespread deployment of low-cost sensors and their connection to the Internet. Meanwhile, deep learning provides useful tools for processing and analyzing these big machinery data. The main purpose of this paper is to review and summarize the emerging research work of deep learning on machine health monitoring. After the brief introduction of deep learning techniques, the applications of deep learning in machine health monitoring systems are reviewed mainly from the following aspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and its variants including Deep Belief Network (DBN) and Deep Boltzmann Machines (DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). Finally, some new trends of DL-based machine health monitoring methods are discussed.",2016-12-16,http://arxiv.org/abs/1612.07640v1,"Rui Zhao, Ruqiang Yan, Zhenghua Chen, Kezhi Mao, Peng Wang, Robert X. Gao",arxiv.org,"cs.LG, stat.ML"
Diversity in Machine Learning,"Machine learning methods have achieved good performance and been widely applied in various real-world applications. They can learn the model adaptively and be better fit for special requirements of different tasks. Generally, a good machine learning system is composed of plentiful training data, a good model training process, and an accurate inference. Many factors can affect the performance of the machine learning process, among which the diversity of the machine learning process is an important one. The diversity can help each procedure to guarantee a total good machine learning: diversity of the training data ensures that the training data can provide more discriminative information for the model, diversity of the learned model (diversity in parameters of each model or diversity among different base models) makes each parameter/model capture unique or complement information and the diversity in inference can provide multiple choices each of which corresponds to a specific plausible local optimal result. Even though the diversity plays an important role in machine learning process, there is no systematical analysis of the diversification in machine learning system. In this paper, we systematically summarize the methods to make data diversification, model diversification, and inference diversification in the machine learning process, respectively. In addition, the typical applications where the diversity technology improved the machine learning performance have been surveyed, including the remote sensing imaging tasks, machine translation, camera relocalization, image segmentation, object detection, topic modeling, and others. Finally, we discuss some challenges of the diversity technology in machine learning and point out some directions in future work.",2018-07-04,http://arxiv.org/abs/1807.01477v2,"Zhiqiang Gong, Ping Zhong, Weidong Hu",arxiv.org,cs.CV
Engineering problems in machine learning systems,"Fatal accidents are a major issue hindering the wide acceptance of safety-critical systems that employ machine learning and deep learning models, such as automated driving vehicles. In order to use machine learning in a safety-critical system, it is necessary to demonstrate the safety and security of the system through engineering processes. However, thus far, no such widely accepted engineering concepts or frameworks have been established for these systems. The key to using a machine learning model in a deductively engineered system is decomposing the data-driven training of machine learning models into requirement, design, and verification, particularly for machine learning models used in safety-critical systems. Simultaneously, open problems and relevant technical fields are not organized in a manner that enables researchers to select a theme and work on it. In this study, we identify, classify, and explore the open problems in engineering (safety-critical) machine learning systems --- that is, in terms of requirement, design, and verification of machine learning models and systems --- as well as discuss related works and research directions, using automated driving vehicles as an example. Our results show that machine learning models are characterized by a lack of requirements specification, lack of design specification, lack of interpretability, and lack of robustness. We also perform a gap analysis on a conventional system quality standard SQuARE with the characteristics of machine learning models to study quality models for machine learning systems. We find that a lack of requirements specification and lack of robustness have the greatest impact on conventional quality models.",2019-04-01,http://arxiv.org/abs/1904.00001v2,"Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae",arxiv.org,"cs.SE, cs.LG"
"Automated Graph Machine Learning: Approaches, Libraries, Benchmarks and   Directions","Graph machine learning has been extensively studied in both academic and industry. However, as the literature on graph learning booms with a vast number of emerging methods and techniques, it becomes increasingly difficult to manually design the optimal machine learning algorithm for different graph-related tasks. To tackle the challenge, automated graph machine learning, which aims at discovering the best hyper-parameter and neural architecture configuration for different graph tasks/data without manual design, is gaining an increasing number of attentions from the research community. In this paper, we extensively discuss automated graph machine learning approaches, covering hyper-parameter optimization (HPO) and neural architecture search (NAS) for graph machine learning. We briefly overview existing libraries designed for either graph machine learning or automated machine learning respectively, and further in depth introduce AutoGL, our dedicated and the world's first open-source library for automated graph machine learning. Also, we describe a tailored benchmark that supports unified, reproducible, and efficient evaluations. Last but not least, we share our insights on future research directions for automated graph machine learning. This paper is the first systematic and comprehensive discussion of approaches, libraries as well as directions for automated graph machine learning.",2022-01-04,http://arxiv.org/abs/2201.01288v2,"Xin Wang, Ziwei Zhang, Haoyang Li, Wenwu Zhu",arxiv.org,"cs.LG, cs.AI"
On the impact of measure pre-conditionings on general parametric ML   models and transfer learning via domain adaptation,We study a new technique for understanding convergence of learning agents under small modifications of data. We show that such convergence can be understood via an analogue of Fatou's lemma which yields gamma-convergence. We show it's relevance and applications in general machine learning tasks and domain adaptation transfer learning.,2024-03-04,http://arxiv.org/abs/2403.02432v1,Joaquín Sánchez García,arxiv.org,"stat.ML, cs.LG, math.OC"
Semi-supervised Learning on Large Graphs: is Poisson Learning a   Game-Changer?,"We explain Poisson learning on graph-based semi-supervised learning to see if it could avoid the problem of global information loss problem as Laplace-based learning methods on large graphs. From our analysis, Poisson learning is simply Laplace regularization with thresholding, cannot overcome the problem.",2022-02-28,http://arxiv.org/abs/2202.13608v2,Canh Hao Nguyen,arxiv.org,"stat.ML, cs.LG"
The Case for Meta-Cognitive Machine Learning: On Model Entropy and   Concept Formation in Deep Learning,"Machine learning is usually defined in behaviourist terms, where external validation is the primary mechanism of learning. In this paper, I argue for a more holistic interpretation in which finding more probable, efficient and abstract representations is as central to learning as performance. In other words, machine learning should be extended with strategies to reason over its own learning process, leading to so-called meta-cognitive machine learning. As such, the de facto definition of machine learning should be reformulated in these intrinsically multi-objective terms, taking into account not only the task performance but also internal learning objectives. To this end, we suggest a ""model entropy function"" to be defined that quantifies the efficiency of the internal learning processes. It is conjured that the minimization of this model entropy leads to concept formation. Besides philosophical aspects, some initial illustrations are included to support the claims.",2017-11-04,http://arxiv.org/abs/1711.01431v1,Johan Loeckx,arxiv.org,"cs.AI, cs.LG, stat.ML"
Low-Shot Classification: A Comparison of Classical and Deep Transfer   Machine Learning Approaches,"Despite the recent success of deep transfer learning approaches in NLP, there is a lack of quantitative studies demonstrating the gains these models offer in low-shot text classification tasks over existing paradigms. Deep transfer learning approaches such as BERT and ULMFiT demonstrate that they can beat state-of-the-art results on larger datasets, however when one has only 100-1000 labelled examples per class, the choice of approach is less clear, with classical machine learning and deep transfer learning representing valid options. This paper compares the current best transfer learning approach with top classical machine learning approaches on a trinary sentiment classification task to assess the best paradigm. We find that BERT, representing the best of deep transfer learning, is the best performing approach, outperforming top classical machine learning algorithms by 9.7% on average when trained with 100 examples per class, narrowing to 1.8% at 1000 labels per class. We also show the robustness of deep transfer learning in moving across domains, where the maximum loss in accuracy is only 0.7% in similar domain tasks and 3.2% cross domain, compared to classical machine learning which loses up to 20.6%.",2019-07-17,http://arxiv.org/abs/1907.07543v1,"Peter Usherwood, Steven Smit",arxiv.org,"cs.LG, stat.ML"
Machine Learning with a Reject Option: A survey,"Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with rejection recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake.   This survey aims to provide an overview on machine learning with rejection. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection, which we carefully formalize. Moreover, we review and categorize strategies to evaluate a model's predictive and rejective quality. Additionally, we define the existing architectures for models with rejection and describe the standard techniques for learning such models. Finally, we provide examples of relevant application domains and show how machine learning with rejection relates to other machine learning research areas.",2021-07-23,http://arxiv.org/abs/2107.11277v3,"Kilian Hendrickx, Lorenzo Perini, Dries Van der Plas, Wannes Meert, Jesse Davis",arxiv.org,"cs.LG, cs.AI, 68T02, I.2.6"
PSL is Dead. Long Live PSL,"Property Specification Language (PSL) is a form of temporal logic that has been mainly used in discrete domains (e.g. formal hardware verification). In this paper, we show that by merging machine learning techniques with PSL monitors, we can extend PSL to work on continuous domains. We apply this technique in machine learning-based anomaly detection to analyze scenarios of real-time streaming events from continuous variables in order to detect abnormal behaviors of a system. By using machine learning with formal models, we leverage the strengths of both machine learning methods and formal semantics of time. On one hand, machine learning techniques can produce distributions on continuous variables, where abnormalities can be captured as deviations from the distributions. On the other hand, formal methods can characterize discrete temporal behaviors and relations that cannot be easily learned by machine learning techniques. Interestingly, the anomalies detected by machine learning and the underlying time representation used are discrete events. We implemented a temporal monitoring package (TEF) that operates in conjunction with normal data science packages for anomaly detection machine learning systems, and we show that TEF can be used to perform accurate interpretation of temporal correlation between events.",2022-05-27,http://arxiv.org/abs/2205.14136v1,"Kevin Smith, Hai Lin, Praveen Tiwari, Marjorie Sayer, Claudionor Coelho",arxiv.org,"cs.LG, cs.FL"
A Declarative Query Language for Scientific Machine Learning,"The popularity of data science as a discipline and its importance in the emerging economy and industrial progress dictate that machine learning be democratized for the masses. This also means that the current practice of workforce training using machine learning tools, which requires low-level statistical and algorithmic details, is a barrier that needs to be addressed. Similar to data management languages such as SQL, machine learning needs to be practiced at a conceptual level to help make it a staple tool for general users. In particular, the technical sophistication demanded by existing machine learning frameworks is prohibitive for many scientists who are not computationally savvy or well versed in machine learning techniques. The learning curve to use the needed machine learning tools is also too high for them to take advantage of these powerful platforms to rapidly advance science. In this paper, we introduce a new declarative machine learning query language, called {\em MQL}, for naive users. We discuss its merit and possible ways of implementing it over a traditional relational database system. We discuss two materials science experiments implemented using MQL on a materials science workflow system called MatFlow.",2024-05-25,http://arxiv.org/abs/2405.16159v1,Hasan M Jamil,arxiv.org,"cs.LG, cs.DB"
Transfer Learning for Voice Activity Detection: A Denoising Deep Neural   Network Perspective,"Mismatching problem between the source and target noisy corpora severely hinder the practical use of the machine-learning-based voice activity detection (VAD). In this paper, we try to address this problem in the transfer learning prospective. Transfer learning tries to find a common learning machine or a common feature subspace that is shared by both the source corpus and the target corpus. The denoising deep neural network is used as the learning machine. Three transfer techniques, which aim to learn common feature representations, are used for analysis. Experimental results demonstrate the effectiveness of the transfer learning schemes on the mismatch problem.",2013-03-08,http://arxiv.org/abs/1303.2104v1,"Xiao-Lei Zhang, Ji Wu",arxiv.org,cs.LG
Learnable: Theory vs Applications,"Two different views on machine learning problem: Applied learning (machine learning with business applications) and Agnostic PAC learning are formalized and compared here. I show that, under some conditions, the theory of PAC Learnable provides a way to solve the Applied learning problem. However, the theory requires to have the training sets so large, that it would make the learning practically useless. I suggest shedding some theoretical misconceptions about learning to make the theory more aligned with the needs and experience of practitioners.",2018-07-27,http://arxiv.org/abs/1807.10681v1,Marina Sapir,arxiv.org,"cs.LG, stat.ML"
Minimal Achievable Sufficient Statistic Learning,"We introduce Minimal Achievable Sufficient Statistic (MASS) Learning, a training method for machine learning models that attempts to produce minimal sufficient statistics with respect to a class of functions (e.g. deep networks) being optimized over. In deriving MASS Learning, we also introduce Conserved Differential Information (CDI), an information-theoretic quantity that - unlike standard mutual information - can be usefully applied to deterministically-dependent continuous random variables like the input and output of a deep network. In a series of experiments, we show that deep networks trained with MASS Learning achieve competitive performance on supervised learning and uncertainty quantification benchmarks.",2019-05-19,http://arxiv.org/abs/1905.07822v2,"Milan Cvitkovic, Günther Koliander",arxiv.org,"cs.LG, stat.ML"
Sustainable Federated Learning,"Potential environmental impact of machine learning by large-scale wireless networks is a major challenge for the sustainability of future smart ecosystems. In this paper, we introduce sustainable machine learning in federated learning settings, using rechargeable devices that can collect energy from the ambient environment. We propose a practical federated learning framework that leverages intermittent energy arrivals for training, with provable convergence guarantees. Our framework can be applied to a wide range of machine learning settings in networked environments, including distributed and federated learning in wireless and edge networks. Our experiments demonstrate that the proposed framework can provide significant performance improvement over the benchmark energy-agnostic federated learning settings.",2021-02-22,http://arxiv.org/abs/2102.11274v1,"Basak Guler, Aylin Yener",arxiv.org,"cs.LG, cs.IT, math.IT"
The ART of Transfer Learning: An Adaptive and Robust Pipeline,"Transfer learning is an essential tool for improving the performance of primary tasks by leveraging information from auxiliary data resources. In this work, we propose Adaptive Robust Transfer Learning (ART), a flexible pipeline of performing transfer learning with generic machine learning algorithms. We establish the non-asymptotic learning theory of ART, providing a provable theoretical guarantee for achieving adaptive transfer while preventing negative transfer. Additionally, we introduce an ART-integrated-aggregating machine that produces a single final model when multiple candidate algorithms are considered. We demonstrate the promising performance of ART through extensive empirical studies on regression, classification, and sparse learning. We further present a real-data analysis for a mortality study.",2023-04-30,http://arxiv.org/abs/2305.00520v1,"Boxiang Wang, Yunan Wu, Chenglong Ye",arxiv.org,"stat.ML, cs.LG"
A Comparison of First-order Algorithms for Machine Learning,"Using an optimization algorithm to solve a machine learning problem is one of mainstreams in the field of science. In this work, we demonstrate a comprehensive comparison of some state-of-the-art first-order optimization algorithms for convex optimization problems in machine learning. We concentrate on several smooth and non-smooth machine learning problems with a loss function plus a regularizer. The overall experimental results show the superiority of primal-dual algorithms in solving a machine learning problem from the perspectives of the ease to construct, running time and accuracy.",2014-04-26,http://arxiv.org/abs/1404.6674v1,"Yu Wei, Pock Thomas",arxiv.org,cs.LG
Theoretical Impediments to Machine Learning With Seven Sparks from the   Causal Revolution,"Current machine learning systems operate, almost exclusively, in a statistical, or model-free mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference tasks. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal modeling.",2018-01-11,http://arxiv.org/abs/1801.04016v1,Judea Pearl,arxiv.org,"cs.LG, cs.AI, stat.ML"
Meaningful Models: Utilizing Conceptual Structure to Improve Machine   Learning Interpretability,"The last decade has seen huge progress in the development of advanced machine learning models; however, those models are powerless unless human users can interpret them. Here we show how the mind's construction of concepts and meaning can be used to create more interpretable machine learning models. By proposing a novel method of classifying concepts, in terms of 'form' and 'function', we elucidate the nature of meaning and offer proposals to improve model understandability. As machine learning begins to permeate daily life, interpretable models may serve as a bridge between domain-expert authors and non-expert users.",2016-07-01,http://arxiv.org/abs/1607.00279v1,Nick Condry,arxiv.org,"stat.ML, cs.AI"
When Machine Learning Meets Multiscale Modeling in Chemical Reactions,"Due to the intrinsic complexity and nonlinearity of chemical reactions, direct applications of traditional machine learning algorithms may face with many difficulties. In this study, through two concrete examples with biological background, we illustrate how the key ideas of multiscale modeling can help to reduce the computational cost of machine learning a lot, as well as how machine learning algorithms perform model reduction automatically in a time-scale separated system. Our study highlights the necessity and effectiveness of an integration of machine learning algorithms and multiscale modeling during the study of chemical reactions.",2020-06-01,http://arxiv.org/abs/2006.00700v1,"Wuyue Yang, Liangrong Peng, Yi Zhu, Liu Hong",arxiv.org,"q-bio.MN, cs.LG"
Seven Myths in Machine Learning Research,"We present seven myths commonly believed to be true in machine learning research, circa Feb 2019. This is an archival copy of the blog post at https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/   Myth 1: TensorFlow is a Tensor manipulation library   Myth 2: Image datasets are representative of real images found in the wild   Myth 3: Machine Learning researchers do not use the test set for validation   Myth 4: Every datapoint is used in training a neural network   Myth 5: We need (batch) normalization to train very deep residual networks   Myth 6: Attention $>$ Convolution   Myth 7: Saliency maps are robust ways to interpret neural networks",2019-02-18,http://arxiv.org/abs/1902.06789v2,"Oscar Chang, Hod Lipson",arxiv.org,"cs.LG, stat.ML"
Optimal Algorithms for Ski Rental with Soft Machine-Learned Predictions,"We consider a variant of the classic Ski Rental online algorithm with applications to machine learning. In our variant, we allow the skier access to a black-box machine-learning algorithm that provides an estimate of the probability that there will be at most a threshold number of ski-days. We derive a class of optimal randomized algorithms to determine the strategy that minimizes the worst-case expected competitive ratio for the skier given a prediction from the machine learning algorithm,and analyze the performance and robustness of these algorithms.",2019-02-28,http://arxiv.org/abs/1903.00092v2,Rohan Kodialam,arxiv.org,"cs.LG, cs.DS, stat.ML"
Towards Quantification of Bias in Machine Learning for Healthcare: A   Case Study of Renal Failure Prediction,"As machine learning (ML) models, trained on real-world datasets, become common practice, it is critical to measure and quantify their potential biases. In this paper, we focus on renal failure and compare a commonly used traditional risk score, Tangri, with a more powerful machine learning model, which has access to a larger variable set and trained on 1.6 million patients' EHR data. We will compare and discuss the generalization and applicability of these two models, in an attempt to quantify biases of status quo clinical practice, compared to ML-driven models.",2019-11-18,http://arxiv.org/abs/1911.07679v1,"Josie Williams, Narges Razavian",arxiv.org,"cs.LG, stat.AP, stat.ML"
On the computation of counterfactual explanations -- A survey,Due to the increasing use of machine learning in practice it becomes more and more important to be able to explain the prediction and behavior of machine learning models. An instance of explanations are counterfactual explanations which provide an intuitive and useful explanations of machine learning models. In this survey we review model-specific methods for efficiently computing counterfactual explanations of many different machine learning models and propose methods for models that have not been considered in literature so far.,2019-11-15,http://arxiv.org/abs/1911.07749v1,"André Artelt, Barbara Hammer",arxiv.org,"cs.LG, cs.AI, stat.ML"
"Computer Systems Have 99 Problems, Let's Not Make Machine Learning   Another One","Machine learning techniques are finding many applications in computer systems, including many tasks that require decision making: network optimization, quality of service assurance, and security. We believe machine learning systems are here to stay, and to materialize on their potential we advocate a fresh look at various key issues that need further attention, including security as a requirement and system complexity, and how machine learning systems affect them. We also discuss reproducibility as a key requirement for sustainable machine learning systems, and leads to pursuing it.",2019-11-28,http://arxiv.org/abs/1911.12593v1,"David Mohaisen, Songqing Chen",arxiv.org,"cs.CY, cs.CR, cs.LG"
Distributed Double Machine Learning with a Serverless Architecture,"This paper explores serverless cloud computing for double machine learning. Being based on repeated cross-fitting, double machine learning is particularly well suited to exploit the high level of parallelism achievable with serverless computing. It allows to get fast on-demand estimations without additional cloud maintenance effort. We provide a prototype Python implementation \texttt{DoubleML-Serverless} for the estimation of double machine learning models with the serverless computing platform AWS Lambda and demonstrate its utility with a case study analyzing estimation times and costs.",2021-01-11,http://arxiv.org/abs/2101.04025v2,Malte S. Kurz,arxiv.org,"cs.DC, cs.LG, stat.ML"
Optimizing for Generalization in Machine Learning with Cross-Validation   Gradients,"Cross-validation is the workhorse of modern applied statistics and machine learning, as it provides a principled framework for selecting the model that maximizes generalization performance. In this paper, we show that the cross-validation risk is differentiable with respect to the hyperparameters and training data for many common machine learning algorithms, including logistic regression, elastic-net regression, and support vector machines. Leveraging this property of differentiability, we propose a cross-validation gradient method (CVGM) for hyperparameter optimization. Our method enables efficient optimization in high-dimensional hyperparameter spaces of the cross-validation risk, the best surrogate of the true generalization ability of our learning algorithm.",2018-05-18,http://arxiv.org/abs/1805.07072v1,"Shane Barratt, Rishi Sharma",arxiv.org,"stat.ML, cs.LG"
Algebraic Expression of Subjective Spatial and Temporal Patterns,"Universal learning machine is a theory trying to study machine learning from mathematical point of view. The outside world is reflected inside an universal learning machine according to pattern of incoming data. This is subjective pattern of learning machine. In [2,4], we discussed subjective spatial pattern, and established a powerful tool -- X-form, which is an algebraic expression for subjective spatial pattern. However, as the initial stage of study, there we only discussed spatial pattern. Here, we will discuss spatial and temporal patterns, and algebraic expression for them.",2018-05-26,http://arxiv.org/abs/1805.11959v2,Chuyu Xiong,arxiv.org,"cs.LG, stat.ML"
Machine Learning in Official Statistics,"In the first half of 2018, the Federal Statistical Office of Germany (Destatis) carried out a ""Proof of Concept Machine Learning"" as part of its Digital Agenda. A major component of this was surveys on the use of machine learning methods in official statistics, which were conducted at selected national and international statistical institutions and among the divisions of Destatis. It was of particular interest to find out in which statistical areas and for which tasks machine learning is used and which methods are applied. This paper is intended to make the results of the surveys publicly accessible.",2018-12-13,http://arxiv.org/abs/1812.10422v1,"Martin Beck, Florian Dumpert, Joerg Feuerhake",arxiv.org,"cs.CY, cs.LG, stat.ML"
Addressing Privacy Threats from Machine Learning,"Every year at NeurIPS, machine learning researchers gather and discuss exciting applications of machine learning in areas such as public health, disaster response, climate change, education, and more. However, many of these same researchers are expressing growing concern about applications of machine learning for surveillance (Nanayakkara et al., 2021). This paper presents a brief overview of strategies for resisting these surveillance technologies and calls for greater collaboration between machine learning and human-computer interaction researchers to address the threats that these technologies pose.",2021-10-25,http://arxiv.org/abs/2111.04439v1,Mary Anne Smart,arxiv.org,"cs.CY, cs.CR, cs.LG"
Categories of Differentiable Polynomial Circuits for Machine Learning,"Reverse derivative categories (RDCs) have recently been shown to be a suitable semantic framework for studying machine learning algorithms. Whereas emphasis has been put on training methodologies, less attention has been devoted to particular \emph{model classes}: the concrete categories whose morphisms represent machine learning models. In this paper we study presentations by generators and equations of classes of RDCs. In particular, we propose \emph{polynomial circuits} as a suitable machine learning model. We give an axiomatisation for these circuits and prove a functional completeness result. Finally, we discuss the use of polynomial circuits over specific semirings to perform machine learning with discrete values.",2022-03-12,http://arxiv.org/abs/2203.06430v2,"Paul Wilson, Fabio Zanasi",arxiv.org,"cs.LG, math.CT"
When Physics Meets Machine Learning: A Survey of Physics-Informed   Machine Learning,"Physics-informed machine learning (PIML), referring to the combination of prior knowledge of physics, which is the high level abstraction of natural phenomenons and human behaviours in the long history, with data-driven machine learning models, has emerged as an effective way to mitigate the shortage of training data, to increase models' generalizability and to ensure the physical plausibility of results. In this paper, we survey an abundant number of recent works in PIML and summarize them from three aspects: (1) motivations of PIML, (2) physics knowledge in PIML, (3) methods of physics knowledge integration in PIML. We also discuss current challenges and corresponding research opportunities in PIML.",2022-03-31,http://arxiv.org/abs/2203.16797v1,"Chuizheng Meng, Sungyong Seo, Defu Cao, Sam Griesemer, Yan Liu",arxiv.org,"cs.LG, stat.ML"
Efficient Private Machine Learning by Differentiable Random   Transformations,"With the increasing demands for privacy protection, many privacy-preserving machine learning systems were proposed in recent years. However, most of them cannot be put into production due to their slow training and inference speed caused by the heavy cost of homomorphic encryption and secure multiparty computation(MPC) methods. To circumvent this, I proposed a privacy definition which is suitable for large amount of data in machine learning tasks. Based on that, I showed that random transformations like linear transformation and random permutation can well protect privacy. Merging random transformations and arithmetic sharing together, I designed a framework for private machine learning with high efficiency and low computation cost.",2020-08-18,http://arxiv.org/abs/2008.07758v1,Fei Zheng,arxiv.org,"cs.CR, cs.LG, stat.ML"
pystacked: Stacking generalization and machine learning in Stata,"pystacked implements stacked generalization (Wolpert, 1992) for regression and binary classification via Python's scikit-learn. Stacking combines multiple supervised machine learners -- the ""base"" or ""level-0"" learners -- into a single learner. The currently supported base learners include regularized regression, random forest, gradient boosted trees, support vector machines, and feed-forward neural nets (multi-layer perceptron). pystacked can also be used with as a `regular' machine learning program to fit a single base learner and, thus, provides an easy-to-use API for scikit-learn's machine learning algorithms.",2022-08-23,http://arxiv.org/abs/2208.10896v2,"Achim Ahrens, Christian B. Hansen, Mark E. Schaffer",arxiv.org,"econ.EM, stat.ML"
Nine tips for ecologists using machine learning,"Due to their high predictive performance and flexibility, machine learning models are an appropriate and efficient tool for ecologists. However, implementing a machine learning model is not yet a trivial task and may seem intimidating to ecologists with no previous experience in this area. Here we provide a series of tips to help ecologists in implementing machine learning models. We focus on classification problems as many ecological studies aim to assign data into predefined classes such as ecological states or biological entities. Each of the nine tips identifies a common error, trap or challenge in developing machine learning models and provides recommendations to facilitate their use in ecological studies.",2023-05-17,http://arxiv.org/abs/2305.10472v2,"Marine Desprez, Vincent Miele, Olivier Gimenez",arxiv.org,"q-bio.PE, cs.LG"
A Comparison of Machine Learning Methods for Data with High-Cardinality   Categorical Variables,"High-cardinality categorical variables are variables for which the number of different levels is large relative to the sample size of a data set, or in other words, there are few data points per level. Machine learning methods can have difficulties with high-cardinality variables. In this article, we empirically compare several versions of two of the most successful machine learning methods, tree-boosting and deep neural networks, and linear mixed effects models using multiple tabular data sets with high-cardinality categorical variables. We find that, first, machine learning models with random effects have higher prediction accuracy than their classical counterparts without random effects, and, second, tree-boosting with random effects outperforms deep neural networks with random effects.",2023-07-05,http://arxiv.org/abs/2307.02071v1,Fabio Sigrist,arxiv.org,"cs.LG, cs.AI, stat.ML"
Machine learning for accuracy in density functional approximations,"Machine learning techniques have found their way into computational chemistry as indispensable tools to accelerate atomistic simulations and materials design. In addition, machine learning approaches hold the potential to boost the predictive power of computationally efficient electronic structure methods, such as density functional theory, to chemical accuracy and to correct for fundamental errors in density functional approaches. Here, recent progress in applying machine learning to improve the accuracy of density functional and related approximations is reviewed. Promises and challenges in devising machine learning models transferable between different chemistries and materials classes are discussed with the help of examples applying promising models to systems far outside their training sets.",2023-11-01,http://arxiv.org/abs/2311.00196v1,Johannes Voss,arxiv.org,"physics.chem-ph, cs.LG"
Improving Radiography Machine Learning Workflows via Metadata Management   for Training Data Selection,"Most machine learning models require many iterations of hyper-parameter tuning, feature engineering, and debugging to produce effective results. As machine learning models become more complicated, this pipeline becomes more difficult to manage effectively. In the physical sciences, there is an ever-increasing pool of metadata that is generated by the scientific research cycle. Tracking this metadata can reduce redundant work, improve reproducibility, and aid in the feature and training dataset engineering process. In this case study, we present a tool for machine learning metadata management in dynamic radiography. We evaluate the efficacy of this tool against the initial research workflow and discuss extensions to general machine learning pipelines in the physical sciences.",2024-08-22,http://arxiv.org/abs/2408.12655v1,"Mirabel Reid, Christine Sweeney, Oleg Korobkin",arxiv.org,"cs.LG, cs.HC"
A method to benchmark high-dimensional process drift detection,"Process curves are multivariate finite time series data coming from manufacturing processes. This paper studies machine learning that detect drifts in process curve datasets. A theoretic framework to synthetically generate process curves in a controlled way is introduced in order to benchmark machine learning algorithms for process drift detection. An evaluation score, called the temporal area under the curve, is introduced, which allows to quantify how well machine learning models unveil curves belonging to drift segments. Finally, a benchmark study comparing popular machine learning approaches on synthetic data generated with the introduced framework is presented that shows that existing algorithms often struggle with datasets containing multiple drift segments.",2024-09-05,http://arxiv.org/abs/2409.03669v2,"Edgar Wolf, Tobias Windisch",arxiv.org,"stat.ML, cs.AI, cs.LG"
Inverse Problems and Data Assimilation: A Machine Learning Approach,"The aim of these notes is to demonstrate the potential for ideas in machine learning to impact on the fields of inverse problems and data assimilation. The perspective is one that is primarily aimed at researchers from inverse problems and/or data assimilation who wish to see a mathematical presentation of machine learning as it pertains to their fields. As a by-product, we include a succinct mathematical treatment of various topics in machine learning.",2024-10-14,http://arxiv.org/abs/2410.10523v1,"Eviatar Bach, Ricardo Baptista, Daniel Sanz-Alonso, Andrew Stuart",arxiv.org,"stat.ML, cs.LG, math.OC"
Proceedings of NIPS 2017 Workshop on Machine Learning for the Developing   World,"This is the Proceedings of NIPS 2017 Workshop on Machine Learning for the Developing World, held in Long Beach, California, USA on December 8, 2017",2017-11-27,http://arxiv.org/abs/1711.09522v2,"Maria De-Arteaga, William Herlands",arxiv.org,stat.ML
Classifying medical notes into standard disease codes using Machine   Learning,"We investigate the automatic classification of patient discharge notes into standard disease labels. We find that Convolutional Neural Networks with Attention outperform previous algorithms used in this task, and suggest further areas for improvement.",2018-02-01,http://arxiv.org/abs/1802.00382v1,Amitabha Karmakar,arxiv.org,"cs.LG, cs.CL, stat.AP, stat.ML"
Mixed integer programming formulation of unsupervised learning,"A novel formulation and training procedure for full Boltzmann machines in terms of a mixed binary quadratic feasibility problem is given. As a proof of concept, the theory is analytically and numerically tested on XOR patterns.",2020-01-20,http://arxiv.org/abs/2001.07278v1,Arturo Berrones-Santos,arxiv.org,"cs.LG, cond-mat.dis-nn, stat.ML"
"Linear, Machine Learning and Probabilistic Approaches for Time Series   Analysis","In this paper we study different approaches for time series modeling. The forecasting approaches using linear models, ARIMA alpgorithm, XGBoost machine learning algorithm are described. Results of different model combinations are shown. For probabilistic modeling the approaches using copulas and Bayesian inference are considered.",2017-02-26,http://arxiv.org/abs/1703.01977v1,B. M. Pavlyshenko,arxiv.org,"stat.AP, cs.LG, stat.ME"
Proceedings of NIPS 2016 Workshop on Interpretable Machine Learning for   Complex Systems,"This is the Proceedings of NIPS 2016 Workshop on Interpretable Machine Learning for Complex Systems, held in Barcelona, Spain on December 9, 2016",2016-11-28,http://arxiv.org/abs/1611.09139v1,"Andrew Gordon Wilson, Been Kim, William Herlands",arxiv.org,stat.ML
Elements of effective machine learning datasets in astronomy,"In this work, we identify elements of effective machine learning datasets in astronomy and present suggestions for their design and creation. Machine learning has become an increasingly important tool for analyzing and understanding the large-scale flood of data in astronomy. To take advantage of these tools, datasets are required for training and testing. However, building machine learning datasets for astronomy can be challenging. Astronomical data is collected from instruments built to explore science questions in a traditional fashion rather than to conduct machine learning. Thus, it is often the case that raw data, or even downstream processed data is not in a form amenable to machine learning. We explore the construction of machine learning datasets and we ask: what elements define effective machine learning datasets? We define effective machine learning datasets in astronomy to be formed with well-defined data points, structure, and metadata. We discuss why these elements are important for astronomical applications and ways to put them in practice. We posit that these qualities not only make the data suitable for machine learning, they also help to foster usable, reusable, and replicable science practices.",2022-11-25,http://arxiv.org/abs/2211.14401v2,"Bernie Boscoe, Tuan Do, Evan Jones, Yunqi Li, Kevin Alfaro, Christy Ma",arxiv.org,"astro-ph.IM, cs.LG"
Detection of brain tumors using machine learning algorithms,An algorithm capable of processing NMR images was developed for analysis using machine learning techniques to detect the presence of brain tumors.,2022-01-12,http://arxiv.org/abs/2201.04703v1,"Horacio Corral, Javier Melchor, Balam Sotelo, Jorge Vera",arxiv.org,"eess.IV, cs.LG"
Efficient Deep Learning on Multi-Source Private Data,"Machine learning models benefit from large and diverse datasets. Using such datasets, however, often requires trusting a centralized data aggregator. For sensitive applications like healthcare and finance this is undesirable as it could compromise patient privacy or divulge trade secrets. Recent advances in secure and privacy-preserving computation, including trusted hardware enclaves and differential privacy, offer a way for mutually distrusting parties to efficiently train a machine learning model without revealing the training data. In this work, we introduce Myelin, a deep learning framework which combines these privacy-preservation primitives, and use it to establish a baseline level of performance for fully private machine learning.",2018-07-17,http://arxiv.org/abs/1807.06689v1,"Nick Hynes, Raymond Cheng, Dawn Song",arxiv.org,"cs.LG, stat.ML"
Pymc-learn: Practical Probabilistic Machine Learning in Python,"$\textit{Pymc-learn}$ is a Python package providing a variety of state-of-the-art probabilistic models for supervised and unsupervised machine learning. It is inspired by $\textit{scikit-learn}$ and focuses on bringing probabilistic machine learning to non-specialists. It uses a general-purpose high-level language that mimics $\textit{scikit-learn}$. Emphasis is put on ease of use, productivity, flexibility, performance, documentation, and an API consistent with $\textit{scikit-learn}$. It depends on $\textit{scikit-learn}$ and $\textit{pymc3}$ and is distributed under the new BSD-3 license, encouraging its use in both academia and industry. Source code, binaries, and documentation are available on http://github.com/pymc-learn/pymc-learn.",2018-10-31,http://arxiv.org/abs/1811.00542v1,Daniel Emaasit,arxiv.org,"stat.ML, cs.LG"
Machine learning in physics: a short guide,"Machine learning is a rapidly growing field with the potential to revolutionize many areas of science, including physics. This review provides a brief overview of machine learning in physics, covering the main concepts of supervised, unsupervised, and reinforcement learning, as well as more specialized topics such as causal inference, symbolic regression, and deep learning. We present some of the principal applications of machine learning in physics and discuss the associated challenges and perspectives.",2023-10-16,http://arxiv.org/abs/2310.10368v1,Francisco A. Rodrigues,arxiv.org,"cs.LG, cond-mat.stat-mech, physics.app-ph"
Quantum-Classical Machine learning by Hybrid Tensor Networks,"Tensor networks (TN) have found a wide use in machine learning, and in particular, TN and deep learning bear striking similarities. In this work, we propose the quantum-classical hybrid tensor networks (HTN) which combine tensor networks with classical neural networks in a uniform deep learning framework to overcome the limitations of regular tensor networks in machine learning. We first analyze the limitations of regular tensor networks in the applications of machine learning involving the representation power and architecture scalability. We conclude that in fact the regular tensor networks are not competent to be the basic building blocks of deep learning. Then, we discuss the performance of HTN which overcome all the deficiency of regular tensor networks for machine learning. In this sense, we are able to train HTN in the deep learning way which is the standard combination of algorithms such as Back Propagation and Stochastic Gradient Descent. We finally provide two applicable cases to show the potential applications of HTN, including quantum states classification and quantum-classical autoencoder. These cases also demonstrate the great potentiality to design various HTN in deep learning way.",2020-05-15,http://arxiv.org/abs/2005.09428v2,"Ding Liu, Jiaqi Yao, Zekun Yao, Quan Zhang",arxiv.org,"cs.LG, quant-ph, stat.ML"
A Review on Machine Unlearning,"Recently, an increasing number of laws have governed the useability of users' privacy. For example, Article 17 of the General Data Protection Regulation (GDPR), the right to be forgotten, requires machine learning applications to remove a portion of data from a dataset and retrain it if the user makes such a request. Furthermore, from the security perspective, training data for machine learning models, i.e., data that may contain user privacy, should be effectively protected, including appropriate erasure. Therefore, researchers propose various privacy-preserving methods to deal with such issues as machine unlearning. This paper provides an in-depth review of the security and privacy concerns in machine learning models. First, we present how machine learning can use users' private data in daily life and the role that the GDPR plays in this problem. Then, we introduce the concept of machine unlearning by describing the security threats in machine learning models and how to protect users' privacy from being violated using machine learning platforms. As the core content of the paper, we introduce and analyze current machine unlearning approaches and several representative research results and discuss them in the context of the data lineage. Furthermore, we also discuss the future research challenges in this field.",2024-11-18,http://arxiv.org/abs/2411.11315v1,"Haibo Zhang, Toru Nakamura, Takamasa Isohara, Kouichi Sakurai",arxiv.org,cs.LG
Dex: Incremental Learning for Complex Environments in Deep Reinforcement   Learning,"This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments. We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention.",2017-06-19,http://arxiv.org/abs/1706.05749v1,"Nick Erickson, Qi Zhao",arxiv.org,"stat.ML, cs.AI, cs.LG"
Theoretical Robopsychology: Samu Has Learned Turing Machines,"From the point of view of a programmer, the robopsychology is a synonym for the activity is done by developers to implement their machine learning applications. This robopsychological approach raises some fundamental theoretical questions of machine learning. Our discussion of these questions is constrained to Turing machines. Alan Turing had given an algorithm (aka the Turing Machine) to describe algorithms. If it has been applied to describe itself then this brings us to Turing's notion of the universal machine. In the present paper, we investigate algorithms to write algorithms. From a pedagogy point of view, this way of writing programs can be considered as a combination of learning by listening and learning by doing due to it is based on applying agent technology and machine learning. As the main result we introduce the problem of learning and then we show that it cannot easily be handled in reality therefore it is reasonable to use machine learning algorithm for learning Turing machines.",2016-06-08,http://arxiv.org/abs/1606.02767v2,Norbert Bátfai,arxiv.org,"cs.AI, 68T05, I.2.6"
Model-Agnostic Interpretability of Machine Learning,"Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.",2016-06-16,http://arxiv.org/abs/1606.05386v1,"Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin",arxiv.org,"stat.ML, cs.LG"
On conditional parity as a notion of non-discrimination in machine   learning,"We identify conditional parity as a general notion of non-discrimination in machine learning. In fact, several recently proposed notions of non-discrimination, including a few counterfactual notions, are instances of conditional parity. We show that conditional parity is amenable to statistical analysis by studying randomization as a general mechanism for achieving conditional parity and a kernel-based test of conditional parity.",2017-06-26,http://arxiv.org/abs/1706.08519v1,"Ya'acov Ritov, Yuekai Sun, Ruofei Zhao",arxiv.org,"stat.ML, cs.CY, cs.LG"
Proceedings of the 2017 ICML Workshop on Human Interpretability in   Machine Learning (WHI 2017),"This is the Proceedings of the 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI 2017), which was held in Sydney, Australia, August 10, 2017. Invited speakers were Tony Jebara, Pang Wei Koh, and David Sontag.",2017-08-08,http://arxiv.org/abs/1708.02666v1,"Been Kim, Dmitry M. Malioutov, Kush R. Varshney, Adrian Weller",arxiv.org,"stat.ML, cs.LG"
Proceedings of the 2018 ICML Workshop on Human Interpretability in   Machine Learning (WHI 2018),"This is the Proceedings of the 2018 ICML Workshop on Human Interpretability in Machine Learning (WHI 2018), which was held in Stockholm, Sweden, July 14, 2018. Invited speakers were Barbara Engelhardt, Cynthia Rudin, Fernanda Vi\'egas, and Martin Wattenberg.",2018-07-03,http://arxiv.org/abs/1807.01308v1,"Been Kim, Kush R. Varshney, Adrian Weller",arxiv.org,"stat.ML, cs.LG"
TherML: Thermodynamics of Machine Learning,In this work we offer a framework for reasoning about a wide class of existing objectives in machine learning. We develop a formal correspondence between this work and thermodynamics and discuss its implications.,2018-07-11,http://arxiv.org/abs/1807.04162v3,"Alexander A. Alemi, Ian Fischer",arxiv.org,"cs.LG, cond-mat.stat-mech, stat.ML"
ML-Schema: Exposing the Semantics of Machine Learning with Schemas and   Ontologies,"The ML-Schema, proposed by the W3C Machine Learning Schema Community Group, is a top-level ontology that provides a set of classes, properties, and restrictions for representing and interchanging information on machine learning algorithms, datasets, and experiments. It can be easily extended and specialized and it is also mapped to other more domain-specific ontologies developed in the area of machine learning and data mining. In this paper we overview existing state-of-the-art machine learning interchange formats and present the first release of ML-Schema, a canonical format resulted of more than seven years of experience among different research institutions. We argue that exposing semantics of machine learning algorithms, models, and experiments through a canonical format may pave the way to better interpretability and to realistically achieve the full interoperability of experiments regardless of platform or adopted workflow solution.",2018-07-14,http://arxiv.org/abs/1807.05351v1,"Gustavo Correa Publio, Diego Esteves, Agnieszka Ławrynowicz, Panče Panov, Larisa Soldatova, Tommaso Soru, Joaquin Vanschoren, Hamid Zafar",arxiv.org,"cs.LG, cs.DB, cs.IR, stat.ML"
Analysis of Software Engineering for Agile Machine Learning Projects,"The number of machine learning, artificial intelligence or data science related software engineering projects using Agile methodology is increasing. However, there are very few studies on how such projects work in practice. In this paper, we analyze project issues tracking data taken from Scrum (a popular tool for Agile) for several machine learning projects. We compare this data with corresponding data from non-machine learning projects, in an attempt to analyze how machine learning projects are executed differently from normal software engineering projects. On analysis, we find that machine learning project issues use different kinds of words to describe issues, have higher number of exploratory or research oriented tasks as compared to implementation tasks, and have a higher number of issues in the product backlog after each sprint, denoting that it is more difficult to estimate the duration of machine learning project related tasks in advance. After analyzing this data, we propose a few ways in which Agile machine learning projects can be better logged and executed, given their differences with normal software engineering projects.",2019-12-16,http://arxiv.org/abs/1912.07323v1,"Kushal Singla, Joy Bose, Chetan Naik",arxiv.org,"cs.SE, cs.LG, D.2"
Proceedings of the 2016 ICML Workshop on Human Interpretability in   Machine Learning (WHI 2016),"This is the Proceedings of the 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016.   Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang, and Hanna Wallach.",2016-07-08,http://arxiv.org/abs/1607.02531v2,"Been Kim, Dmitry M. Malioutov, Kush R. Varshney",arxiv.org,"stat.ML, cs.LG"
DriveML: An R Package for Driverless Machine Learning,"In recent years, the concept of automated machine learning has become very popular. Automated Machine Learning (AutoML) mainly refers to the automated methods for model selection and hyper-parameter optimization of various algorithms such as random forests, gradient boosting, neural networks, etc. In this paper, we introduce a new package i.e. DriveML for automated machine learning. DriveML helps in implementing some of the pillars of an automated machine learning pipeline such as automated data preparation, feature engineering, model building and model explanation by running the function instead of writing lengthy R codes. The DriveML package is available in CRAN. We compare the DriveML package with other relevant packages in CRAN/Github and find that DriveML performs the best across different parameters. We also provide an illustration by applying the DriveML package with default configuration on a real world dataset. Overall, the main benefits of DriveML are in development time savings, reduce developer's errors, optimal tuning of machine learning models and reproducibility.",2020-05-01,http://arxiv.org/abs/2005.00478v3,"Sayan Putatunda, Dayananda Ubrangala, Kiran Rama, Ravi Kondapalli",arxiv.org,"cs.LG, stat.ML"
A combinatorial conjecture from PAC-Bayesian machine learning,We present a proof of a combinatorial conjecture from the second author's Ph.D. thesis. The proof relies on binomial and multinomial sums identities. We also discuss the relevance of the conjecture in the context of PAC-Bayesian machine learning.,2020-06-02,http://arxiv.org/abs/2006.01387v2,"M. Younsi, A. Lacasse",arxiv.org,"stat.ML, cs.LG, math.CO"
Integrating Machine Learning with Physics-Based Modeling,"Machine learning is poised as a very powerful tool that can drastically improve our ability to carry out scientific research. However, many issues need to be addressed before this becomes a reality. This article focuses on one particular issue of broad interest: How can we integrate machine learning with physics-based modeling to develop new interpretable and truly reliable physical models? After introducing the general guidelines, we discuss the two most important issues for developing machine learning-based physical models: Imposing physical constraints and obtaining optimal datasets. We also provide a simple and intuitive explanation for the fundamental reasons behind the success of modern machine learning, as well as an introduction to the concurrent machine learning framework needed for integrating machine learning with physics-based modeling. Molecular dynamics and moment closure of kinetic equations are used as examples to illustrate the main issues discussed. We end with a general discussion on where this integration will lead us to, and where the new frontier will be after machine learning is successfully integrated into scientific modeling.",2020-06-04,http://arxiv.org/abs/2006.02619v1,"Weinan E, Jiequn Han, Linfeng Zhang",arxiv.org,"physics.comp-ph, cs.LG, cs.NA, math.NA"
Power Consumption Variation over Activation Functions,"The power that machine learning models consume when making predictions can be affected by a model's architecture. This paper presents various estimates of power consumption for a range of different activation functions, a core factor in neural network model architecture design. Substantial differences in hardware performance exist between activation functions. This difference informs how power consumption in machine learning models can be reduced.",2020-06-12,http://arxiv.org/abs/2006.07237v1,Leon Derczynski,arxiv.org,"cs.LG, cs.NE, stat.ML"
Classification with Quantum Machine Learning: A Survey,"Due to the superiority and noteworthy progress of Quantum Computing (QC) in a lot of applications such as cryptography, chemistry, Big data, machine learning, optimization, Internet of Things (IoT), Blockchain, communication, and many more. Fully towards to combine classical machine learning (ML) with Quantum Information Processing (QIP) to build a new field in the quantum world is called Quantum Machine Learning (QML) to solve and improve problems that displayed in classical machine learning (e.g. time and energy consumption, kernel estimation). The aim of this paper presents and summarizes a comprehensive survey of the state-of-the-art advances in Quantum Machine Learning (QML). Especially, recent QML classification works. Also, we cover about 30 publications that are published lately in Quantum Machine Learning (QML). we propose a classification scheme in the quantum world and discuss encoding methods for mapping classical data to quantum data. Then, we provide quantum subroutines and some methods of Quantum Computing (QC) in improving performance and speed up of classical Machine Learning (ML). And also some of QML applications in various fields, challenges, and future vision will be presented.",2020-06-22,http://arxiv.org/abs/2006.12270v1,"Zainab Abohashima, Mohamed Elhosen, Essam H. Houssein, Waleed M. Mohamed",arxiv.org,"quant-ph, cs.LG"
"Radiological images and machine learning: trends, perspectives, and   prospects","The application of machine learning to radiological images is an increasingly active research area that is expected to grow in the next five to ten years. Recent advances in machine learning have the potential to recognize and classify complex patterns from different radiological imaging modalities such as x-rays, computed tomography, magnetic resonance imaging and positron emission tomography imaging. In many applications, machine learning based systems have shown comparable performance to human decision-making. The applications of machine learning are the key ingredients of future clinical decision making and monitoring systems. This review covers the fundamental concepts behind various machine learning techniques and their applications in several radiological imaging areas, such as medical image segmentation, brain function studies and neurological disease diagnosis, as well as computer-aided systems, image registration, and content-based image retrieval systems. Synchronistically, we will briefly discuss current challenges and future directions regarding the application of machine learning in radiological imaging. By giving insight on how take advantage of machine learning powered applications, we expect that clinicians can prevent and diagnose diseases more accurately and efficiently.",2019-03-27,http://arxiv.org/abs/1903.11726v1,"Zhenwei Zhang, Ervin Sejdic",arxiv.org,"eess.IV, cs.LG"
Collaborative Machine Learning Markets with Data-Replication-Robust   Payments,"We study the problem of collaborative machine learning markets where multiple parties can achieve improved performance on their machine learning tasks by combining their training data. We discuss desired properties for these machine learning markets in terms of fair revenue distribution and potential threats, including data replication. We then instantiate a collaborative market for cases where parties share a common machine learning task and where parties' tasks are different. Our marketplace incentivizes parties to submit high quality training and true validation data. To this end, we introduce a novel payment division function that is robust-to-replication and customized output models that perform well only on requested machine learning tasks. In experiments, we validate the assumptions underlying our theoretical analysis and show that these are approximately satisfied for commonly used machine learning models.",2019-11-08,http://arxiv.org/abs/1911.09052v1,"Olga Ohrimenko, Shruti Tople, Sebastian Tschiatschek",arxiv.org,"cs.GT, cs.LG, stat.ML"
Adversarial Machine Learning Attacks on Condition-Based Maintenance   Capabilities,"Condition-based maintenance (CBM) strategies exploit machine learning models to assess the health status of systems based on the collected data from the physical environment, while machine learning models are vulnerable to adversarial attacks. A malicious adversary can manipulate the collected data to deceive the machine learning model and affect the CBM system's performance. Adversarial machine learning techniques introduced in the computer vision domain can be used to make stealthy attacks on CBM systems by adding perturbation to data to confuse trained models. The stealthy nature causes difficulty and delay in detection of the attacks. In this paper, adversarial machine learning in the domain of CBM is introduced. A case study shows how adversarial machine learning can be used to attack CBM capabilities. Adversarial samples are crafted using the Fast Gradient Sign method, and the performance of a CBM system under attack is investigated. The obtained results reveal that CBM systems are vulnerable to adversarial machine learning attacks and defense strategies need to be considered.",2021-01-28,http://arxiv.org/abs/2101.12097v1,Hamidreza Habibollahi Najaf Abadi,arxiv.org,"cs.LG, cs.AI, cs.CR"
Confronting Machine Learning With Financial Research,"This study aims to examine the challenges and applications of machine learning for financial research. Machine learning algorithms have been developed for certain data environments which substantially differ from the one we encounter in finance. Not only do difficulties arise due to some of the idiosyncrasies of financial markets, there is a fundamental tension between the underlying paradigm of machine learning and the research philosophy in financial economics. Given the peculiar features of financial markets and the empirical framework within social science, various adjustments have to be made to the conventional machine learning methodology. We discuss some of the main challenges of machine learning in finance and examine how these could be accounted for. Despite some of the challenges, we argue that machine learning could be unified with financial research to become a robust complement to the econometrician's toolbox. Moreover, we discuss the various applications of machine learning in the research process such as estimation, empirical discovery, testing, causal inference and prediction.",2021-02-28,http://arxiv.org/abs/2103.00366v2,"Kristof Lommers, Ouns El Harzli, Jack Kim",arxiv.org,"q-fin.ST, cs.LG, econ.EM"
Electre Tri-Machine Learning Approach to the Record Linkage Problem,"In this short paper, the Electre Tri-Machine Learning Method, generally used to solve ordinal classification problems, is proposed for solving the Record Linkage problem. Preliminary experimental results show that, using the Electre Tri method, high accuracy can be achieved and more than 99% of the matches and nonmatches were correctly identified by the procedure.",2015-05-25,http://arxiv.org/abs/1505.06614v1,"Renato De Leone, Valentina Minnetti",arxiv.org,"stat.ML, cs.LG"
The Top 10 Topics in Machine Learning Revisited: A Quantitative   Meta-Study,"Which topics of machine learning are most commonly addressed in research? This question was initially answered in 2007 by doing a qualitative survey among distinguished researchers. In our study, we revisit this question from a quantitative perspective. Concretely, we collect 54K abstracts of papers published between 2007 and 2016 in leading machine learning journals and conferences. We then use machine learning in order to determine the top 10 topics in machine learning. We not only include models, but provide a holistic view across optimization, data, features, etc. This quantitative approach allows reducing the bias of surveys. It reveals new and up-to-date insights into what the 10 most prolific topics in machine learning research are. This allows researchers to identify popular topics as well as new and rising topics for their research.",2017-03-29,http://arxiv.org/abs/1703.10121v1,"Patrick Glauner, Manxing Du, Victor Paraschiv, Andrey Boytsov, Isabel Lopez Andrade, Jorge Meira, Petko Valtchev, Radu State",arxiv.org,"cs.LG, cs.AI, stat.ML"
An $O(N)$ Sorting Algorithm: Machine Learning Sort,"We propose an $O(N\cdot M)$ sorting algorithm by Machine Learning method, which shows a huge potential sorting big data. This sorting algorithm can be applied to parallel sorting and is suitable for GPU or TPU acceleration. Furthermore, we discuss the application of this algorithm to sparse hash table.",2018-05-11,http://arxiv.org/abs/1805.04272v2,"Hanqing Zhao, Yuehan Luo",arxiv.org,"cs.LG, cs.DS, stat.ML"
A Game of Dice: Machine Learning and the Question Concerning Art,"We review some practical and philosophical questions raised by the use of machine learning in creative practice. Beyond the obvious problems regarding plagiarism and authorship, we argue that the novelty in AI Art relies mostly on a narrow machine learning contribution : manifold approximation. Nevertheless, this contribution creates a radical shift in the way we have to consider this movement. Is this omnipotent tool a blessing or a curse for the artists?",2019-04-02,http://arxiv.org/abs/1904.01957v1,Paul Todorov,arxiv.org,"cs.AI, cs.LG"
Bridging belief function theory to modern machine learning,"Machine learning is a quickly evolving field which now looks really different from what it was 15 years ago, when classification and clustering were major issues. This document proposes several trends to explore the new questions of modern machine learning, with the strong afterthought that the belief function framework has a major role to play.",2015-04-15,http://arxiv.org/abs/1504.03874v1,Thomas Burger,arxiv.org,"cs.AI, cs.LG"
Privacy Preserving Machine Learning: Threats and Solutions,"For privacy concerns to be addressed adequately in current machine learning systems, the knowledge gap between the machine learning and privacy communities must be bridged. This article aims to provide an introduction to the intersection of both fields with special emphasis on the techniques used to protect the data.",2018-03-27,http://arxiv.org/abs/1804.11238v1,"Mohammad Al-Rubaie, J. Morris Chang",arxiv.org,"cs.CR, cs.LG"
Characterizing machine learning process: A maturity framework,"Academic literature on machine learning modeling fails to address how to make machine learning models work for enterprises. For example, existing machine learning processes cannot address how to define business use cases for an AI application, how to convert business requirements from offering managers into data requirements for data scientists, and how to continuously improve AI applications in term of accuracy and fairness, and how to customize general purpose machine learning models with industry, domain, and use case specific data to make them more accurate for specific situations etc. Making AI work for enterprises requires special considerations, tools, methods and processes. In this paper we present a maturity framework for machine learning model lifecycle management for enterprises. Our framework is a re-interpretation of the software Capability Maturity Model (CMM) for machine learning model development process. We present a set of best practices from our personal experience of building large scale real-world machine learning models to help organizations achieve higher levels of maturity independent of their starting point.",2018-11-12,http://arxiv.org/abs/1811.04871v1,"Rama Akkiraju, Vibha Sinha, Anbang Xu, Jalal Mahmud, Pritam Gundecha, Zhe Liu, Xiaotong Liu, John Schumacher",arxiv.org,"cs.LG, cs.SE"
Towards Identifying and Managing Sources of Uncertainty in AI and   Machine Learning Models - An Overview,Quantifying and managing uncertainties that occur when data-driven models such as those provided by AI and machine learning methods are applied is crucial. This whitepaper provides a brief motivation and first overview of the state of the art in identifying and quantifying sources of uncertainty for data-driven components as well as means for analyzing their impact.,2018-11-28,http://arxiv.org/abs/1811.11669v1,Michael Kläs,arxiv.org,"cs.LG, stat.ML, 68T01"
Proceedings of NeurIPS 2018 Workshop on Machine Learning for the   Developing World: Achieving Sustainable Impact,"This is the Proceedings of NeurIPS 2018 Workshop on Machine Learning for the Developing World: Achieving Sustainable Impact, held in Montreal, Canada on December 8, 2018",2018-12-21,http://arxiv.org/abs/1812.10398v2,"Maria De-Arteaga, Amanda Coston, William Herlands",arxiv.org,"cs.CY, cs.AI, cs.LG, stat.ML"
Machine Learning and Computational Mathematics,"Neural network-based machine learning is capable of approximating functions in very high dimension with unprecedented efficiency and accuracy. This has opened up many exciting new possibilities, not just in traditional areas of artificial intelligence, but also in scientific computing and computational science. At the same time, machine learning has also acquired the reputation of being a set of ""black box"" type of tricks, without fundamental principles. This has been a real obstacle for making further progress in machine learning. In this article, we try to address the following two very important questions: (1) How machine learning has already impacted and will further impact computational mathematics, scientific computing and computational science? (2) How computational mathematics, particularly numerical analysis, {can} impact machine learning? We describe some of the most important progress that has been made on these issues. Our hope is to put things into a perspective that will help to integrate machine learning with computational mathematics.",2020-09-23,http://arxiv.org/abs/2009.14596v1,Weinan E,arxiv.org,"math.NA, cs.LG, cs.NA, stat.ML, 68T07, 46E15, 26B35, 26B40"
Risk Assessment for Machine Learning Models,"In this paper we propose a framework for assessing the risk associated with deploying a machine learning model in a specified environment. For that we carry over the risk definition from decision theory to machine learning. We develop and implement a method that allows to define deployment scenarios, test the machine learning model under the conditions specified in each scenario, and estimate the damage associated with the output of the machine learning model under test. Using the likelihood of each scenario together with the estimated damage we define \emph{key risk indicators} of a machine learning model.   The definition of scenarios and weighting by their likelihood allows for standardized risk assessment in machine learning throughout multiple domains of application. In particular, in our framework, the robustness of a machine learning model to random input corruptions, distributional shifts caused by a changing environment, and adversarial perturbations can be assessed.",2020-11-09,http://arxiv.org/abs/2011.04328v1,"Paul Schwerdtner, Florens Greßner, Nikhil Kapoor, Felix Assion, René Sass, Wiebke Günther, Fabian Hüger, Peter Schlicht",arxiv.org,"cs.LG, cs.AI"
New Trends in Quantum Machine Learning,"Here we will give a perspective on new possible interplays between Machine Learning and Quantum Physics, including also practical cases and applications. We will explore the ways in which machine learning could benefit from new quantum technologies and algorithms to find new ways to speed up their computations by breakthroughs in physical hardware, as well as to improve existing models or devise new learning schemes in the quantum domain. Moreover, there are lots of experiments in quantum physics that do generate incredible amounts of data and machine learning would be a great tool to analyze those and make predictions, or even control the experiment itself. On top of that, data visualization techniques and other schemes borrowed from machine learning can be of great use to theoreticians to have better intuition on the structure of complex manifolds or to make predictions on theoretical models. This new research field, named as Quantum Machine Learning, is very rapidly growing since it is expected to provide huge advantages over its classical counterpart and deeper investigations are timely needed since they can be already tested on the already commercially available quantum machines.",2021-08-22,http://arxiv.org/abs/2108.09664v1,"Lorenzo Buffoni, Filippo Caruso",arxiv.org,"quant-ph, cond-mat.dis-nn, cs.LG, stat.ML"
Practical Attacks on Machine Learning: A Case Study on Adversarial   Windows Malware,"While machine learning is vulnerable to adversarial examples, it still lacks systematic procedures and tools for evaluating its security in different application contexts. In this article, we discuss how to develop automated and scalable security evaluations of machine learning using practical attacks, reporting a use case on Windows malware detection.",2022-07-12,http://arxiv.org/abs/2207.05548v1,"Luca Demetrio, Battista Biggio, Fabio Roli",arxiv.org,"cs.CR, cs.LG"
Fairness and Randomness in Machine Learning: Statistical Independence   and Relativization,"Fair Machine Learning endeavors to prevent unfairness arising in the context of machine learning applications embedded in society. Despite the variety of definitions of fairness and proposed ""fair algorithms"", there remain unresolved conceptual problems regarding fairness. In this paper, we dissect the role of statistical independence in fairness and randomness notions regularly used in machine learning. Thereby, we are led to a suprising hypothesis: randomness and fairness can be considered equivalent concepts in machine learning.   In particular, we obtain a relativized notion of randomness expressed as statistical independence by appealing to Von Mises' century-old foundations for probability. This notion turns out to be ""orthogonal"" in an abstract sense to the commonly used i.i.d.-randomness. Using standard fairness notions in machine learning, which are defined via statistical independence, we then link the ex ante randomness assumptions about the data to the ex post requirements for fair predictions. This connection proves fruitful: we use it to argue that randomness and fairness are essentially relative and that both concepts should reflect their nature as modeling assumptions in machine learning.",2022-07-27,http://arxiv.org/abs/2207.13596v2,"Rabanus Derr, Robert C. Williamson",arxiv.org,"cs.LG, cs.CY"
Systematic Training and Testing for Machine Learning Using Combinatorial   Interaction Testing,"This paper demonstrates the systematic use of combinatorial coverage for selecting and characterizing test and training sets for machine learning models. The presented work adapts combinatorial interaction testing, which has been successfully leveraged in identifying faults in software testing, to characterize data used in machine learning. The MNIST hand-written digits data is used to demonstrate that combinatorial coverage can be used to select test sets that stress machine learning model performance, to select training sets that lead to robust model performance, and to select data for fine-tuning models to new domains. Thus, the results posit combinatorial coverage as a holistic approach to training and testing for machine learning. In contrast to prior work which has focused on the use of coverage in regard to the internal of neural networks, this paper considers coverage over simple features derived from inputs and outputs. Thus, this paper addresses the case where the supplier of test and training sets for machine learning models does not have intellectual property rights to the models themselves. Finally, the paper addresses prior criticism of combinatorial coverage and provides a rebuttal which advocates the use of coverage metrics in machine learning applications.",2022-01-28,http://arxiv.org/abs/2201.12428v1,"Tyler Cody, Erin Lanus, Daniel D. Doyle, Laura Freeman",arxiv.org,"cs.LG, cs.SE, stat.ML"
Software Testing for Machine Learning,"Machine learning has become prevalent across a wide variety of applications. Unfortunately, machine learning has also shown to be susceptible to deception, leading to errors, and even fatal failures. This circumstance calls into question the widespread use of machine learning, especially in safety-critical applications, unless we are able to assure its correctness and trustworthiness properties. Software verification and testing are established technique for assuring such properties, for example by detecting errors. However, software testing challenges for machine learning are vast and profuse - yet critical to address. This summary talk discusses the current state-of-the-art of software testing for machine learning. More specifically, it discusses six key challenge areas for software testing of machine learning systems, examines current approaches to these challenges and highlights their limitations. The paper provides a research agenda with elaborated directions for making progress toward advancing the state-of-the-art on testing of machine learning.",2022-04-30,http://arxiv.org/abs/2205.00210v1,"Dusica Marijan, Arnaud Gotlieb",arxiv.org,"cs.SE, cs.AI, cs.LG"
PSI Draft Specification,"This document presents the draft specification for delivering machine learning services over HTTP, developed as part of the Protocols and Structures for Inference project, which concluded in 2013. It presents the motivation for providing machine learning as a service, followed by a description of the essential and optional components of such a service.",2022-05-02,http://arxiv.org/abs/2205.09488v1,"Mark Reid, James Montgomery, Barry Drake, Avraham Ruderman",arxiv.org,"cs.SE, cs.LG, cs.NI"
The structure of the climate debate,"First-best climate policy is a uniform carbon tax which gradually rises over time. Civil servants have complicated climate policy to expand bureaucracies, politicians to create rents. Environmentalists have exaggerated climate change to gain influence, other activists have joined the climate bandwagon. Opponents to climate policy have attacked the weaknesses in climate research. The climate debate is convoluted and polarized as a result, and climate policy complex. Climate policy should become easier and more rational as the Paris Agreement has shifted climate policy back towards national governments. Changing political priorities, austerity, and a maturing bureaucracy should lead to a more constructive climate debate.",2016-08-19,http://arxiv.org/abs/1608.05597v1,Richard S. J. Tol,arxiv.org,"q-fin.EC, econ.EM, q-fin.GN"
Baumol's Climate Disease,"We investigate optimal carbon abatement in a dynamic general equilibrium climate-economy model with endogenous structural change. By differentiating the production of investment from consumption, we show that social cost of carbon can be conceived as a reduction in physical capital. In addition, we distinguish two final sectors in terms of productivity growth and climate vulnerability. We theoretically show that heterogeneous climate vulnerability results in a climate-induced version of Baumol's cost disease. Further, if climate-vulnerable sectors have high (low) productivity growth, climate impact can either ameliorate (aggravate) the Baumol's cost disease, call for less (more) stringent climate policy. We conclude that carbon abatement should not only factor in unpriced climate capital, but also be tailored to Baumol's cost and climate diseases.",2023-11-30,http://arxiv.org/abs/2312.00160v1,"Fangzhi Wang, Hua Liao, Richard S. J. Tol",arxiv.org,econ.TH
"Climate Science and Control Engineering: Insights, Parallels, and   Connections","Climate science is the multidisciplinary field that studies the Earth's climate and its evolution. At the very core of climate science are indispensable climate models that predict future climate scenarios, inform policy decisions, and dictate how a country's economy should change in light of the changing climate. Climate models capture a wide range of interacting dynamic processes via extremely complex ordinary and partial differential equations. To model these large-scale complex processes, climate science leverages supercomputers, advanced simulations, and statistical methods to predict future climate. An area of engineering that is rarely studied in climate science is control engineering. Given that climate systems are inherently dynamic, it is intuitive to analyze them within the framework of dynamic system science. This perspective has been underexplored in the literature. In this manuscript, we provide a tutorial that: (i) introduces the control engineering community to climate dynamics and modeling, including spatiotemporal scales and challenges in climate modeling; (ii) offers a fresh perspective on climate models from a control systems viewpoint; and (iii) explores the relevance and applicability of various advanced graph and network control-based approaches in building a physics-informed framework for learning, control and estimation in climate systems. We also present simple and then more complex climate models, depicting fundamental ideas and processes that are instrumental in building climate change projections. This tutorial also builds parallels and observes connections between various contemporary problems at the forefront of climate science and their control theoretic counterparts. We specifically observe that an abundance of climate science problems can be linguistically reworded and mathematically framed as control theoretic ones.",2025-04-29,http://arxiv.org/abs/2504.21153v2,"Salma M. Elsherif, Ahmad F. Taha",arxiv.org,"eess.SY, cs.SY"
Mapping the Climate Change Landscape on TikTok,"Social media platforms shape climate action discourse. Mapping these online conversations is essential for effective communication strategies. TikTok's climate discussions are particularly relevant given its young, climate-concerned audience. In this work, we collect the first TikTok dataset on climate topics. We collected 590K videos from 14K creators along with their follower networks. By applying topic modeling to the video descriptions, we map the topics discussed on the platform on a climate taxonomy that we construct by consolidating existing categorizations. Results show TikTok creators primarily approach climate through the angle of lifestyle and dietary choices. By examining semantic connections between topics, we identified non-climate ""gateway"" topics that could draw new audiences into climate discussions.",2025-05-02,http://arxiv.org/abs/2505.03813v1,"Alessia Galdeman, Luca Maria Aiello",arxiv.org,"cs.SI, cs.CY"
Climate modification directed by control theory,"Climate modification measures to counteract global warming receive some more new attentions in these years. Most current researches only discuss the impact of these measures to climate, but how to design such a climate regulator is still unknown. This paper shows the control theory could give the systematic direction for climate modification. But the control analyzing also reveals that climate modifications should only be regarded as a last-ditch measure.",2008-05-05,http://arxiv.org/abs/0805.0541v2,Wang Liang,arxiv.org,cs.OH
Uncertainty in climate science and climate policy,"This essay, written by a statistician and a climate scientist, describes our view of the gap that exists between current practice in mainstream climate science, and the practical needs of policymakers charged with exploring possible interventions in the context of climate change. By `mainstream' we mean the type of climate science that dominates in universities and research centres, which we will term `academic' climate science, in contrast to `policy' climate science; aspects of this distinction will become clearer in what follows.   In a nutshell, we do not think that academic climate science equips climate scientists to be as helpful as they might be, when involved in climate policy assessment. Partly, we attribute this to an over-investment in high resolution climate simulators, and partly to a culture that is uncomfortable with the inherently subjective nature of climate uncertainty.",2014-11-05,http://arxiv.org/abs/1411.6878v1,"Jonathan Rougier, Michel Crucifix",arxiv.org,physics.soc-ph
You are right. I am ALARMED -- But by Climate Change Counter Movement,"The world is facing the challenge of climate crisis. Despite the consensus in scientific community about anthropogenic global warming, the web is flooded with articles spreading climate misinformation. These articles are carefully constructed by climate change counter movement (cccm) organizations to influence the narrative around climate change. We revisit the literature on climate misinformation in social sciences and repackage it to introduce in the community of NLP. Despite considerable work in detection of fake news, there is no misinformation dataset available that is specific to the domain.of climate change. We try to bridge this gap by scraping and releasing articles with known climate change misinformation.",2020-04-30,http://arxiv.org/abs/2004.14907v1,"Shraey Bhatia, Jey Han Lau, Timothy Baldwin",arxiv.org,cs.CL
Financial climate risk: a review of recent advances and key challenges,"The document provides an overview of financial climate risks. It delves into how climate change impacts the global financial system, distinguishing between physical risks (such as extreme weather events) and transition risks (stemming from policy changes and economic transitions towards low carbon technologies). The paper underlines the complexity of accurately defining financial climate risk, citing the integration of climate science with financial risk analysis as a significant challenge. The paper highlights the pivotal role of microfinance institutions (MFIs) in addressing financial climate risk, especially for populations vulnerable to climate change. The document emphasizes the importance of updating risk management practices within MFIs to explicitly include climate risk assessments and suggests leveraging technology to improve these practices.",2024-04-10,http://arxiv.org/abs/2404.07331v1,Victor Cardenas,arxiv.org,"econ.GN, q-fin.EC"
NLP for Climate Policy: Creating a Knowledge Platform for Holistic and   Effective Climate Action,"Climate change is a burning issue of our time, with the Sustainable Development Goal (SDG) 13 of the United Nations demanding global climate action. Realizing the urgency, in 2015 in Paris, world leaders signed an agreement committing to taking voluntary action to reduce carbon emissions. However, the scale, magnitude, and climate action processes vary globally, especially between developed and developing countries. Therefore, from parliament to social media, the debates and discussions on climate change gather data from wide-ranging sources essential to the policy design and implementation. The downside is that we do not currently have the mechanisms to pool the worldwide dispersed knowledge emerging from the structured and unstructured data sources.   The paper thematically discusses how NLP techniques could be employed in climate policy research and contribute to society's good at large. In particular, we exemplify symbiosis of NLP and Climate Policy Research via four methodologies. The first one deals with the major topics related to climate policy using automated content analysis. We investigate the opinions (sentiments) of major actors' narratives towards climate policy in the second methodology. The third technique explores the climate actors' beliefs towards pro or anti-climate orientation. Finally, we discuss developing a Climate Knowledge Graph.   The present theme paper further argues that creating a knowledge platform would help in the formulation of a holistic climate policy and effective climate action. Such a knowledge platform would integrate the policy actors' varied opinions from different social sectors like government, business, civil society, and the scientific community. The research outcome will add value to effective climate action because policymakers can make informed decisions by looking at the diverse public opinion on a comprehensive platform.",2021-05-12,http://arxiv.org/abs/2105.05621v1,"Pradip Swarnakar, Ashutosh Modi",arxiv.org,"cs.CL, cs.AI"
ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning,"Climate models have been key for assessing the impact of climate change and simulating future climate scenarios. The machine learning (ML) community has taken an increased interest in supporting climate scientists' efforts on various tasks such as climate model emulation, downscaling, and prediction tasks. Many of those tasks have been addressed on datasets created with single climate models. However, both the climate science and ML communities have suggested that to address those tasks at scale, we need large, consistent, and ML-ready climate model datasets. Here, we introduce ClimateSet, a dataset containing the inputs and outputs of 36 climate models from the Input4MIPs and CMIP6 archives. In addition, we provide a modular dataset pipeline for retrieving and preprocessing additional climate models and scenarios. We showcase the potential of our dataset by using it as a benchmark for ML-based climate model emulation. We gain new insights about the performance and generalization capabilities of the different ML models by analyzing their performance across different climate models. Furthermore, the dataset can be used to train an ML emulator on several climate models instead of just one. Such a ""super emulator"" can quickly project new climate change scenarios, complementing existing scenarios already provided to policymakers. We believe ClimateSet will create the basis needed for the ML community to tackle climate-related tasks at scale.",2023-11-07,http://arxiv.org/abs/2311.03721v1,"Julia Kaltenborn, Charlotte E. E. Lange, Venkatesh Ramesh, Philippe Brouillard, Yaniv Gurwicz, Chandni Nagda, Jakob Runge, Peer Nowack, David Rolnick",arxiv.org,"cs.LG, cs.AI, cs.CE, physics.ao-ph"
"Detection, attribution, and modeling of climate change: key open issues","The CMIP global climate models (GCMs) assess that nearly 100% of global surface warming observed between 1850-1900 and 2011-2020 is attributable to anthropogenic drivers like greenhouse gas emissions. These models also generate future climate projections based on shared socioeconomic pathways (SSPs), aiding in risk assessment and the development of costly Net-Zero climate mitigation strategies. Yet, the CMIP GCMs face significant scientific challenges in attributing and modeling climate change, particularly in capturing natural climate variability over multiple timescales throughout the Holocene. Other key concerns include the reliability of global surface temperature records, the accuracy of solar irradiance models, and the robustness of climate sensitivity estimates. Global warming estimates may be overstated due to uncorrected non-climatic biases, and the GCMs may significantly underestimate solar and astronomical influences on climate variations. The equilibrium climate sensitivity (ECS) to radiative forcing could be lower than commonly assumed; empirical findings suggest ECS values lower than 3 K and possibly even closer to 1.1 +/- 0.4 K. Empirical models incorporating natural variability suggest that the 21st-century global warming may remain moderate, even under SSP scenarios that do not necessitate Net-Zero emission policies. These findings raise important questions regarding the necessity and urgency of implementing aggressive climate mitigation strategies. While GCMs remain essential tools for climate research and policymaking, their scientific limitations underscore the need for more refined modeling approaches to ensure accurate future climate assessments. Addressing uncertainties related to climate change detection, natural variability, solar influences, and climate sensitivity to radiative forcing will enhance predictions and better inform sustainable climate strategies.",2025-05-21,http://arxiv.org/abs/2506.13994v1,Nicola Scafetta,arxiv.org,"physics.soc-ph, physics.ao-ph, physics.data-an"
Hurricanes Increase Climate Change Conversations on Twitter,"The public understanding of climate change plays a critical role in translating climate science into climate action. In the public discourse, climate impacts are often discussed in the context of extreme weather events. Here, we analyse 65 million Twitter posts and 240 thousand news media articles related to 18 major hurricanes from 2010 to 2022 to clarify how hurricanes impact the public discussion around climate change. First, we analyse news content and show that climate change is the most prominent non-hurricane specific topic discussed by the news media in relation to hurricanes. Second, we perform a comparative analysis between reliable and questionable news media outlets, finding that the language around climate change varies between news media providers. Finally, using geolocated data, we show that accounts in regions affected by hurricanes discuss climate change at a significantly higher rate than accounts in unaffected areas, with references to climate change increasing by, on average, 80% after impact, and up to 200% for the largest hurricanes. Our findings demonstrate how hurricanes have a key impact on the public awareness of climate change.",2023-05-12,http://arxiv.org/abs/2305.07529v1,"Maddalena Torricelli, Max Falkenberg, Alessandro Galeazzi, Fabiana Zollo, Walter Quattrociocchi, Andrea Baronchelli",arxiv.org,"physics.soc-ph, cs.SI"
Predictability of climate tipping focusing on the internal variability   of the Earth system,"Prediction of climate tipping is challenging due to the lack of recent observation of actual climate tipping. Despite many previous efforts to accurately predict the existence and timing of climate tippings under specific climate scenarios, the predictability of climate tipping, the necessary conditions under which climate tipping can be predicted, has yet to be explored. In this study, the predictability of climate tipping is analyzed by Observation System Simulation Experiment (OSSE), in which the value of observation for prediction is assessed through the idealized experiment of data assimilation, using a simplified dynamic vegetation model and an Atlantic Meridional Overturning Circulation (AMOC) two box model. We find that the ratio of internal variability to observation error, or signal-to-noise ratio, should be large enough to accurately predict climate tippings. When observation can accurately resolve the internal variability of the system, assimilating these observations into process-based models can effectively improve the skill of predicting climate tippings. Our quantitative estimation of required observation accuracy to predict climate tipping implies that the existing observation network is not always sufficient to accurately project climate tipping.",2024-06-28,http://arxiv.org/abs/2406.19639v1,"Amane Kubo, Yohei Sawada",arxiv.org,"physics.ao-ph, stat.AP"
The Cost of Climate Action: Experimental Evidence on the Impact of   Climate Information on Charitable Donations to Climate Activism,"We examine the propensity of individuals to donate to climate activism, evaluating the impact of different informational treatments on an incentive compatible charitable donation and stated climate change-related concerns. Participants were evaluated on climate literacy and general climate attitudes before being randomly assigned to a treatment which provided either education or neutral language about climate change, either with or without images of protest. After the treatment, participants engaged in an incentive compatible dictator game. We find that participants gave more to climate activism than seen in previous dictator game and charitable giving experiments, in both average amount given and proportion of participants who gave their entire endowment. However, we determine that climate activism information negatively influenced the amount of money donated. We also found that protest imagery moderated this negative effect and had a positive significant effect of increasing participants' climate concern. Finally, we found that the climate concern was significantly positively correlated with donations, while being a male was significantly negatively associated with donation amounts.",2024-09-25,http://arxiv.org/abs/2409.17378v1,"Samantha Gonsalves Wetherell, Anna Josephson",arxiv.org,"econ.GN, q-fin.EC"
Link Climate: An Interoperable Knowledge Graph Platform for Climate Data,"Climate science has become more ambitious in recent years as global awareness about the environment has grown. To better understand climate, historical climate (e.g. archived meteorological variables such as temperature, wind, water, etc.) and climate-related data (e.g. geographical features and human activities) are widely used by today's climate research to derive models for an explainable climate change and its effects. However, such data sources are often dispersed across a multitude of disconnected data silos on the Web. Moreover, there is a lack of advanced climate data platforms to enable multi-source heterogeneous climate data analysis, therefore, researchers must face a stern challenge in collecting and analyzing multi-source data. In this paper, we address this problem by proposing a climate knowledge graph for the integration of multiple climate data and other data sources into one service, leveraging Web technologies (e.g. HTTP) for multi-source climate data analysis. The proposed knowledge graph is primarily composed of data from the National Oceanic and Atmospheric Administration's daily climate summaries, OpenStreetMap, and Wikidata, and it supports joint data queries on these widely used databases. This paper shows, with a use case in Ireland and the United Kingdom, how climate researchers could benefit from this platform as it allows them to easily integrate datasets from different domains and geographical locations.",2022-10-28,http://arxiv.org/abs/2210.16050v1,"Jiantao Wu, Fabrizio Orlandi, Declan O'Sullivan, Soumyabrata Dev",arxiv.org,cs.DB
The physics of climate change: simple models in climate science,There is a perception that climate science can only be approached with complex computer simulations. But working climate scientists often use simple models to understand their simulations and make order-of-magnitude estimates. This article presents some of these simple models with the goal of making climate science more accessible and comprehensible.,2018-02-08,http://arxiv.org/abs/1802.02695v2,Nadir Jeevanjee,arxiv.org,physics.ao-ph
The Snowball Earth transition in a climate model with drifting   parameters,"Using an intermediate complexity climate model (Planet Simulator), we investigate the so-called Snowball Earth transition. For certain values of the solar constant, the climate system allows two different stable states: one of them is the Snowball Earth, covered by ice and snow, and the other one is today's climate. In our setup, we consider the case when the climate system starts from its warm attractor (the stable climate we experience today), and the solar constant is decreased continuously in finite time, according to a parameter drift scenario, to a state, where only the Snowball Earth's attractor remains stable. This induces an inevitable transition, or climate tipping from the warm climate. The reverse transition is also discussed. Increasing the solar constant back to its original value on individual simulations, we find that the system stays stuck in the Snowball state. However, using ensemble methods i.e., using an ensemble of climate realizations differing only slightly in their initial conditions we show that the transition from the Snowball Earth to the warm climate is also possible with a certain probability. From the point of view of dynamical systems theory, we can say that the system's snapshot attractor splits between the warm climate's and the Snowball Earth's attractor.",2019-05-31,http://arxiv.org/abs/1906.00952v1,"Bálint Kaszás, Tímea Haszpra, Mátyás Herein",arxiv.org,physics.ao-ph
Climate Impact Assessment Requires Weighting: Introducing the Weighted   Climate Dataset,"High-resolution gridded climate data are readily available from multiple sources, yet climate research and decision-making increasingly require country and region-specific climate information weighted by socio-economic factors. Moreover, the current landscape of disparate data sources and inconsistent weighting methodologies exacerbates the reproducibility crisis and undermines scientific integrity. To address these issues, we have developed a globally comprehensive dataset at both country (GADM0) and region (GADM1) levels, encompassing various climate indicators (precipitation, temperature, SPEI, wind gust). Our methodology involves weighting gridded climate data by population density, night-time light intensity, cropland area, and concurrent population count -- all proxies for socio-economic activity -- before aggregation. We process data from multiple sources, offering daily, monthly, and annual climate variables spanning from 1900 to 2023. A unified framework streamlines our preprocessing steps, and rigorous validation against leading climate impact studies ensures data reliability. The resulting Weighted Climate Dataset is publicly accessible through an online dashboard at https://weightedclimatedata.streamlit.app/.",2024-12-20,http://arxiv.org/abs/2412.15699v1,"Marco Gortan, Lorenzo Testa, Giorgio Fagiolo, Francesco Lamperti",arxiv.org,stat.AP
EpiClim: Weekly District-Wise all-India multi-epidemics Climate-Health   Dataset for accelerated GeoHealth research,"Climate change significantly impacts public health, driving the emergence and spread of epidemics. Climate health models are essential for assessing and predicting disease outbreaks influenced by climatic variables like temperature and precipitation. For instance, dengue and malaria correlate with temperature changes, while cholera is linked to precipitation anomalies. Advances in AI-enabled weather prediction (AI-NWP) have improved forecasting, but integrating climate models with health systems is hindered by the lack of comprehensive, granular health datasets. This study introduces EpiClim: India's Epidemic-Climate Dataset, the first weekly district-wise dataset for major epidemics in India from 2009 to the present, sourced from the Integrated Disease Surveillance Programme (IDSP). The dataset, covering diseases like dengue, malaria, and acute-diarrheal disease, bridges the gap between climate and health data, enabling the integration of climate forecasts with epidemic prediction models. This work lays the foundation for coupling predictive climate health models with weather and climate models, advancing efforts to mitigate climate-induced public health crises.",2025-01-17,http://arxiv.org/abs/2501.18602v2,"Gurleen Kaur, Shubham Ghoshal, Reena Marbate, Neetiraj Malviya, Arshmehar Kaur, Vaisakh SB, Amit Kumar Srivastava, Manmeet Singh",arxiv.org,physics.soc-ph
Climate Modeling and Bifurcation,"Many papers and monographs were written about the modeling the Earth climate and its variability. However there is still an obvious need for a module that presents the fundamentals of climate modeling to students at the undergraduate level. The present educational paper attempts to fill in this gap. To this end we collect in this paper the relevant climate data and present a simple zero and one dimensional models for the mean temperature of the Earth. These models can exhibit bifurcations from the present Earth climate to an ice age or a ""Venus type of climate"". The models are accompanied by Matlab programs which enable the user to change the models parameters and explore the impact that these changes might have on their predictions on Earth climate.",2019-07-15,http://arxiv.org/abs/1907.11067v2,Mayer Humi,arxiv.org,"physics.pop-ph, 86.01"
Climate Change Conspiracy Theories on Social Media,"One of the critical emerging challenges in climate change communication is the prevalence of conspiracy theories. This paper discusses some of the major conspiracy theories related to climate change found in a large Twitter corpus. We use a state-of-the-art stance detection method to find whether conspiracy theories are more popular among Disbelievers or Believers of climate change. We then analyze which conspiracy theory is more popular than the others and how popularity changes with climate change belief. We find that Disbelievers of climate change are overwhelmingly responsible for sharing messages with conspiracy theory-related keywords, and not all conspiracy theories are equally shared. Lastly, we discuss the implications of our findings for climate change communication.",2021-07-07,http://arxiv.org/abs/2107.03318v1,"Aman Tyagi, Kathleen M. Carley",arxiv.org,"cs.SI, cs.CY"
Trend and Thoughts: Understanding Climate Change Concern using Machine   Learning and Social Media Data,"Nowadays social media platforms such as Twitter provide a great opportunity to understand public opinion of climate change compared to traditional survey methods. In this paper, we constructed a massive climate change Twitter dataset and conducted comprehensive analysis using machine learning. By conducting topic modeling and natural language processing, we show the relationship between the number of tweets about climate change and major climate events; the common topics people discuss climate change; and the trend of sentiment. Our dataset was published on Kaggle (\url{https://www.kaggle.com/leonshangguan/climate-change-tweets-ids-until-aug-2021}) and can be used in further research.",2021-11-06,http://arxiv.org/abs/2111.14929v1,"Zhongkai Shangguan, Zihe Zheng, Lei Lin",arxiv.org,cs.CL
State-dependence of climate sensitivity: attractor constraints and   palaeoclimate regimes,"Equilibrium climate sensitivity (ECS) is a key predictor of climate change. However, it is not very well constrained, either by climate models or by observational data. The reasons for this include strong internal variability and forcing on many time scales. In practise this means that the 'equilibrium' will only be relative to fixing the slow feedback processes before comparing palaeoclimate sensitivity estimates with estimates from model simulations. In addition, information from the late Pleistocene ice age cycles indicates that the climate cycles between cold and warm regimes, and the climate sensitivity varies considerably between regime because of fast feedback processes changing relative strength and time scales over one cycle.   In this paper we consider climate sensitivity for quite general climate dynamics. Using a conceptual Earth system model of Gildor and Tziperman (2001) (with Milankovich forcing and dynamical ocean biogeochemistry) we explore various ways of quantifying the state-dependence of climate sensitivity from unperturbed and perturbed model time series. Even without considering any perturbations, we suggest that climate sensitivity can be usefully thought of as a distribution that quantifies variability within the 'climate attractor' and where there is a strong dependence on climate state and more specificially on the 'climate regime' where fast processes are approximately in equilibrium. We also consider perturbations by instantaneous doubling of CO$_2$ and similarly find a strong dependence on the climate state using our approach.",2016-04-12,http://arxiv.org/abs/1604.03311v2,"Anna S. von der Heydt, Peter Ashwin",arxiv.org,physics.ao-ph
A Climate Change Vulnerability Assessment Framework: A Spatial Approach,"Climate change is affecting every known society, especially for small farmers in Low-Income Countries because they depend heavily on rain, seasonality patterns, and known temperature ranges. To build climate change resilient communities among rural farmers, the first step is to understand the impact of climate change on the population. This paper proposes a Climate Change Vulnerability Assessment Framework (CCVAF) to assess climate change vulnerabilities among rural farmers. The CCVAF framework uses information and communication technology (ICT) to assess climate change vulnerabilities among rural farmers by integrating both community level and individual household level indicators. The CCVAF was instantiated into a GIS-based web application named THRIVE for different decision-makers to better assess how climate change is affecting rural farmers in Western Honduras. Qualitative evaluation of the THRIVE showed that it is an innovative and useful tool. The CCVAF contributes to not only the knowledge base of the climate change vulnerability assessment but also the design science literature by providing guidelines to design a class of climate change vulnerability assessment solutions.",2021-08-22,http://arxiv.org/abs/2108.09762v1,"Claudia Cáceres, Yan Li, Brian Hilton",arxiv.org,cs.CY
GCM Simulations of Unstable Climates in the Habitable Zone,"It has recently been proposed that Earth-like planets in the outer regions of the habitable zone experience unstable climates, repeatedly cycling between glaciated and deglaciated climatic states (Menou 2015). While this result has been confirmed and also extended to explain early Mars climate records (Haqq-Misra et al. 2016; Batalha et al. 2016), all existing work relies on highly idealized low-dimensional climate models. Here, we confirm that the phenomenology of climate cycles remains in 3D Earth climate models with considerably more degrees of freedom. To circumvent the computational barrier of integrating climate on Gyr timescales, we design a hybrid 0D-3D integrator which uses a general circulation model (GCM) as a short relaxation step along a long evolutionary climate sequence. We find that GCM climate cycles are qualitatively consistent with reported low-dimensional results. This establishes on a firmer ground the notion that outer habitable zone planets may be preferentially found in transiently glaciated states.",2017-04-14,http://arxiv.org/abs/1704.04535v4,"Adiv Paradise, Kristen Menou",arxiv.org,astro-ph.EP
What shapes climate change perceptions in Africa? A random forest   approach,"Climate change perceptions are fundamental for adaptation and environmental policy support. Although Africa is one of the most vulnerable regions to climate change, little research has focused on how climate change is perceived in the continent. Using random forest methodology, we analyse Afrobarometer data (N = 45,732), joint with climatic data, to explore what shapes climate change perceptions in Africa. We include 5 different dimensions of climate change perceptions: awareness, belief in its human cause, risk perception, need to stop it and self-efficacy. Results indicate that perceived agriculture conditions are crucial for perceiving climate change. Country-level factors and long-term changes in local weather conditions are among the most important predictors. Moreover, education level, access to information, poverty, authoritarian values, and trust in institutions shape individual climate change perceptions. Demographic effects -- including religion -- seem negligible. These findings suggest policymakers and environmental communicators how to frame climate change in Africa to raise awareness, gather public support and induce adaptation.",2021-05-17,http://arxiv.org/abs/2105.07867v1,"Juan B Gonzalez, Alfonso Sanchez",arxiv.org,"econ.GN, q-fin.EC"
Graph-based Local Climate Classification in Iran,"In this paper, we introduce a novel graph-based method to classify the regions with similar climate in a local area. We refer our proposed method as Graph Partition Based Method (GPBM). Our proposed method attempts to overcome the shortcomings of the current state-of-the-art methods in the literature. It has no limit on the number of variables that can be used and also preserves the nature of climate data. To illustrate the capability of our proposed algorithm, we benchmark its performance with other state-of-the-art climate classification techniques. The climate data is collected from 24 synoptic stations in Fars province in southern Iran. The data includes seven climate variables stored as time series from 1951 to 2017. Our results exhibit that our proposed method performs a more realistic climate classification with less computational time. It can save more information during the climate classification process and is therefore efficient in further data analysis. Furthermore, using our method, we can introduce seasonal graphs to better investigate seasonal climate changes. To the best of our knowledge, our proposed method is the first graph-based climate classification system.",2021-10-18,http://arxiv.org/abs/2110.09209v1,"Neda Akrami, Koorush Ziarati, Soumyabrata Dev",arxiv.org,"physics.ao-ph, cs.LG"
Dimensional analysis identifies contrasting dynamics of past climate   states and critical transitions,"While one can unequivocally identify past climate transitions, we lack comprehensive knowledge about their underlying mechanisms and timescales. Our study employs a dimensional analysis of benthic stable isotope records to uncover, across different timescales, how the climatic fluctuation of the Cenozoic are associated with changes in the number of effective degrees of freedom. Precession timescales dominate the Hothouse and Warmhouse states, while the Icehouse climate is primarily influenced by obliquity and eccentricity timescales. Notably, the Coolhouse state lacks dominant timescales. Our analysis proves effective in objectively identifying abrupt climate shifts and extremes. This is also demonstrated using high-resolution data from the last glacial cycle, revealing abrupt climate shifts within a single climate state. These findings significantly impact our understanding of the inherent stability of each climate state and the evaluation of (paleo-)climate models' ability to replicate key features of past/future climate states and transitions.",2023-09-22,http://arxiv.org/abs/2309.12693v1,"Tommaso Alberti, Fabio Florindo, Eelco J. Rohling, Valerio Lucarini, Davide Faranda",arxiv.org,"physics.geo-ph, nlin.CD, physics.data-an"
Leveraging machine learning to enhance climate models: a review,"Recent achievements in machine learning (Ml) have had a significant impact on various fields, including climate science. Climate modeling is very important and plays a crucial role in shaping the decisions of governments and individuals in mitigating the impact of climate change. Climate change poses a serious threat to humanity, however, current climate models are limited by computational costs, uncertainties, and biases, affecting their prediction accuracy. The vast amount of climate data generated by satellites, radars, and earth system models (ESMS) poses a significant challenge. ML techniques can be effectively employed to analyze this data and extract valuable insights that aid in our understanding of the earth climate. This review paper focuses on how ml has been utilized in the last 5 years to boost the current state-of-the-art climate models. We invite the ml community to join in the global effort to accurately model the earth climate by collaborating with other fields to leverage ml as a powerful tool in this endeavor.",2023-11-15,http://arxiv.org/abs/2311.09413v1,"Ahmed Elsayed, Shrouk Wally, Islam Alkabbany, Asem Ali, Aly Farag",arxiv.org,eess.IV
Machine learning for climate physics and simulations,"We discuss the emerging advances and opportunities at the intersection of machine learning (ML) and climate physics, highlighting the use of ML techniques, including supervised, unsupervised, and equation discovery, to accelerate climate knowledge discoveries and simulations. We delineate two distinct yet complementary aspects: (1) ML for climate physics and (2) ML for climate simulations. While physics-free ML-based models, such as ML-based weather forecasting, have demonstrated success when data is abundant and stationary, the physics knowledge and interpretability of ML models become crucial in the small-data/non-stationary regime to ensure generalizability. Given the absence of observations, the long-term future climate falls into the small-data regime. Therefore, ML for climate physics holds a critical role in addressing the challenges of ML for climate simulations. We emphasize the need for collaboration among climate physics, ML theory, and numerical analysis to achieve reliable ML-based models for climate applications.",2024-04-20,http://arxiv.org/abs/2404.13227v2,"Ching-Yao Lai, Pedram Hassanzadeh, Aditi Sheshadri, Maike Sonnewald, Raffaele Ferrari, Venkatramani Balaji",arxiv.org,"physics.ao-ph, nlin.CD, physics.comp-ph, physics.flu-dyn, physics.geo-ph"
Indexing and Visualization of Climate Change Narratives Using BERT and   Causal Extraction,"In this study, we propose a methodology to extract, index, and visualize ``climate change narratives'' (stories about the connection between causal and consequential events related to climate change). We use two natural language processing methods, BERT (Bidirectional Encoder Representations from Transformers) and causal extraction, to textually analyze newspaper articles on climate change to extract ``climate change narratives.'' The novelty of the methodology could extract and quantify the causal relationships assumed by the newspaper's writers. Looking at the extracted climate change narratives over time, we find that since 2018, an increasing number of narratives suggest the impact of the development of climate change policy discussion and the implementation of climate change-related policies on corporate behaviors, macroeconomics, and price dynamics. We also observed the recent emergence of narratives focusing on the linkages between climate change-related policies and monetary policy. Furthermore, there is a growing awareness of the negative impacts of natural disasters (e.g., abnormal weather and severe floods) related to climate change on economic activities, and this issue might be perceived as a new challenge for companies and governments. The methodology of this study is expected to be applied to a wide range of fields, as it can analyze causal relationships among various economic topics, including analysis of inflation expectation or monetary policy communication strategy.",2024-08-03,http://arxiv.org/abs/2408.01745v1,"Hiroki Sakaji, Noriyasu Kaneda",arxiv.org,cs.CL
A new framework for climate sensitivity and prediction: a modelling   perspective,"The sensitivity of climate models to increasing CO2 concentration and the climate response at decadal time scales are still major factors of uncertainty for the assessment of the long and short term effects of anthropogenic climate change. While the relative slow progress on these issues is partly due to the inherent inaccuracies of numerical climate models, this also hints at the need for stronger theoretical foundations to the problem of studying climate sensitivity and performing climate change predictions with numerical models. Here we demonstrate that it is possible to use Ruelle's response theory to predict the impact of an arbitrary CO2 forcing scenario on the global surface temperature of a general circulation model. Response theory puts the concept of climate sensitivity on firm theoretical grounds, and addresses rigorously the problem of predictability at different time scales. Conceptually, our results show that performing climate change experiments with general circulation models is a well defined problem from a physical and mathematical point of view. Practically, our results show that considering one single CO2 forcing scenario is enough to construct operators able to predict the response of climatic observables to any other CO2 forcing scenario, without the need to perform additional numerical simulations. We also introduce a general relationship between climate sensitivity and climate response at different time scales, thus providing an explicit definition of the inertia of the system at different time scales. While what we report here refers to the linear response, the general theory allows for treating nonlinear effects as well. Our results pave the way for redesigning and interpreting climate change experiments from a radically new perspective.",2014-03-19,http://arxiv.org/abs/1403.4908v2,"Francesco Ragone, Valerio Lucarini, Frank Lunkeit",arxiv.org,"physics.ao-ph, cond-mat.stat-mech, physics.flu-dyn"
A large-scale bibliometric analysis of global climate change research   between 2001 and 2018,"Global climate change is attracting widespread scientific, political, and public attention owing to the involvement of international initiatives such as the Paris Agreement and the Intergovernmental Panel on Climate Change. We present a large-scale bibliometric analysis based on approximately 120,000 climate change publications between 2001 and 2018 to examine how climate change is studied in scientific research. Our analysis provides an overview of scientific knowledge, shifts of research hotspots, global geographical distribution of research, and focus of individual countries. In our analysis, we identify five key fields in climate change research: physical sciences, paleoclimatology, climate-change ecology, climate technology, and climate policy. We draw the following key conclusions: (1) Over the investigated time period, the focus of climate change research has shifted from understanding the climate system toward climate technologies and policies, such as efficient energy use and legislation. (2) There is an imbalance in scientific production between developed and developing countries. (3) Geography, national demands, and national strategies have been important drivers that influence the research interests and concerns of researchers in different countries. Our study can be used by researchers and policy makers to reflect on the directions in which climate change research is developing and discuss priorities for future research.",2021-07-17,http://arxiv.org/abs/2107.08214v1,"Hui-Zhen Fu, Ludo Waltman",arxiv.org,cs.DL
Social dynamics can delay or prevent climate tipping points by speeding   the adoption of climate change mitigation,"Social behaviour models are increasingly integrated into climate change studies, and the significance of climate tipping points for `runaway' climate change is well recognised. However, there has been insufficient focus on tipping points in social-climate dynamics. We developed a coupled social-climate model consisting of an Earth system model and a social behaviour model, both with tipping elements. The social model explores opinion formation by analysing social learning rates, the net cost of mitigation, and the strength of social norms. Our results indicate that the net cost of mitigation and social norms have minimal impact on tipping points when social norms are weak. As social norms strengthen, the climate tipping point can trigger a tipping element in the social model. However, faster social learning can delay or prevent the climate tipping point: sufficiently fast social learning means growing climate change mitigation can outpace the oncoming climate tipping point, despite social-climate feedback. By comparing high- and low-risk scenarios, we demonstrated high-risk scenarios increase the likelihood of tipping points. We also illustrate the role of a critical temperature anomaly in triggering tipping points. In conclusion, understanding social behaviour dynamics is vital for predicting climate tipping points and mitigating their impacts.",2025-01-23,http://arxiv.org/abs/2501.14096v1,"Yazdan Babazadeh Maghsoodlo, Madhur Anand, Chris T. Bauch",arxiv.org,"math.DS, physics.soc-ph"
Climate uncertainty impacts on optimal mitigation pathways and social   cost of carbon,"Emissions pathways used in climate policy analysis are often derived from integrated assessment models. However, such emissions pathways do not typically include climate feedbacks on socioeconomic systems and by extension do not consider climate uncertainty in their construction. We use a well-known cost-benefit integrated assessment model, the Dynamic Integrated Climate-Economy (DICE) model, with its climate component replaced by the Finite-amplitude Impulse Response (FaIR) model (v2.1). The climate uncertainty in FaIR is sampled with an ensemble that is consistent with historically observed climate and Intergovernmental Panel on Climate Change (IPCC) assessed ranges of key climate variables such as equilibrium climate sensitivity. Three scenarios are produced: a pathway similar to the ""optimal welfare"" scenario of DICE that has similar warming outcomes to current policies, and pathways that limit warming to ""well-below"" 2C and 1.5C with low overshoot, in line with Paris Agreement long-term temperature goals. Climate uncertainty alone is responsible for a factor of five variation (5-95% range) in the social cost of carbon in the 1.5C scenario. CO2 emissions trajectories resulting from the optimal level of emissions abatement in all pathways are also sensitive to climate uncertainty, with 2050 emissions ranging from -12 to +14 GtCO2/yr in the 1.5C scenario. Equilibrium climate sensitivity and the strength of present-day aerosol effective radiative forcing are strong determinants of social cost of carbon and mid-century CO2 emissions. This shows that narrowing climate uncertainty leads to more refined estimates for the social cost of carbon and provides more certainty about the optimal rate of emissions abatement. Including climate and climate uncertainty in integrated assessment model derived emissions scenarios would address a key missing feedback in scenario construction.",2023-04-18,http://arxiv.org/abs/2304.08957v3,"Christopher J. Smith, Alaa Al Khourdajie, Pu Yang, Doris Folini",arxiv.org,"econ.GN, physics.ao-ph, q-fin.EC"
Climate Prediction through Statistical Methods,"Climate change is a reality of today. Paleoclimatic proxies and climate predictions based on coupled atmosphere-ocean general circulation models provide us with temperature data. Using Detrended Fluctuation Analysis, we are investigating the statistical connection between the climate types of the present and these local temperatures. We are relating this issue to some well-known historic climate shifts. Our main result is that the temperature fluctuations with or without a temperature scale attached to them, can be used to classify climates in the absence of other indicators such as pan evaporation and precipitation.",2008-03-04,http://arxiv.org/abs/0803.0382v1,"Bora Akgun, Zeynep Isvan, Levent Tuter, Mehmet Levent Kurnaz",arxiv.org,"physics.ao-ph, physics.geo-ph"
Introduction to the Special Issue on the Statistical Mechanics of   Climate,"We introduce the special issue on the Statistical Mechanics of Climate published on the Journal of Statistical Physics by presenting an informal discussion of some theoretical aspects of climate dynamics that make it a topic of great interest for mathematicians and theoretical physicists. In particular, we briefly discuss its nonequilibrium and multiscale properties, the relationship between natural climate variability and climate change, the different regimes of climate response to perturbations, and critical transitions.",2020-06-24,http://arxiv.org/abs/2006.13495v1,Valerio Lucarini,arxiv.org,"physics.ao-ph, cond-mat.stat-mech, nlin.CD, physics.data-an, physics.geo-ph"
Towards Specialized Supercomputers for Climate Sciences: Computational   Requirements of the Icosahedral Nonhydrostatic Weather and Climate Model,"We discuss the computational challenges and requirements for high-resolution climate simulations using the Icosahedral Nonhydrostatic Weather and Climate Model (ICON). We define a detailed requirements model for ICON which emphasizes the need for specialized supercomputers to accurately predict climate change impacts and extreme weather events. Based on the requirements model, we outline computational demands for km-scale simulations, and suggests machine learning techniques to enhance model accuracy and efficiency. Our findings aim to guide the design of future supercomputers for advanced climate science.",2024-05-18,http://arxiv.org/abs/2405.13043v1,"Torsten Hoefler, Alexandru Calotoiu, Anurag Dipankar, Thomas Schulthess, Xavier Lapillonne, Oliver Fuhrer",arxiv.org,"physics.ao-ph, cs.AR, cs.DC, physics.comp-ph"
Managing Financial Climate Risk in Banking Services: A Review of Current   Practices and the Challenges Ahead,"The document discusses the financial climate risk in the context of the banking industry, emphasizing the need for a comprehensive understanding of climate change across different spatial and temporal scales. It highlights the challenges in estimating physical and transition risks, specifically extreme events and limitations of current climate models. The document also reviews current gaps in assessing physical and transition risks, including the development, improvement of modeling frameworks, highlighting the need for detailed databases of exposed physical assets and climatic hazard modeling. It also emphasizes the importance of integrating financial climate risks into financial risk management practices, particularly in smaller banks and lending organizations.",2024-05-27,http://arxiv.org/abs/2405.17682v1,Victor Cardenas,arxiv.org,"econ.GN, q-fin.EC"
The Role of Uncertainty in Controlling Climate Change,"Integrated Assessment Models (IAMs) of the climate and economy aim to analyze the impact and efficacy of policies that aim to control climate change, such as carbon taxes and subsidies. A major characteristic of IAMs is that their geophysical sector determines the mean surface temperature increase over the preindustrial level, which in turn determines the damage function. Most of the existing IAMs are perfect-foresight forward-looking models, assuming that we know all of the future information. However, there are significant uncertainties in the climate and economic system, including parameter uncertainty, model uncertainty, climate tipping risks, economic risks, and ambiguity. For example, climate damages are uncertain: some researchers assume that climate damages are proportional to instantaneous output, while others assume that climate damages have a more persistent impact on economic growth. Climate tipping risks represent (nearly) irreversible climate events that may lead to significant changes in the climate system, such as the Greenland ice sheet collapse, while the conditions, probability of tipping, duration, and associated damage are also uncertain. Technological progress in carbon capture and storage, adaptation, renewable energy, and energy efficiency are uncertain too. In the face of these uncertainties, policymakers have to provide a decision that considers important factors such as risk aversion, inequality aversion, and sustainability of the economy and ecosystem. Solving this problem may require richer and more realistic models than standard IAMs, and advanced computational methods. The recent literature has shown that these uncertainties can be incorporated into IAMs and may change optimal climate policies significantly.",2020-03-03,http://arxiv.org/abs/2003.01615v2,Yongyang Cai,arxiv.org,"econ.GN, q-fin.EC"
Future Climate Change Projections over the Indian Region,"Assessments of impacts of climate change and future projections over the Indian region, have so far relied on a single regional climate model (RCM) - eg., the PRECIS RCM of the Hadley Centre, UK. While these assessments have provided inputs to various reports (e.g., INCCA 2010; NATCOMM2 2012), it is important to have an ensemble of climate projections drawn from multiple RCMs due to large uncertainties in regional-scale climate projections. Ensembles of multi-RCM projections driven under different perceivable socio-economic scenarios are required to capture the probable path of growth, and provide the behavior of future climate and impacts on various biophysical systems and economic sectors dependent on such systems.   The Centre for Climate Change Research, Indian Institute of Tropical Meteorology (CCCR-IITM) has generated an ensemble of high resolution downscaled projections of regional climate and monsoon over South Asia until 2100 for the Intergovernmental Panel for Climate Change (IPCC)using a RCM (ICTP-RegCM4) at 50 km horizontal resolution, by driving the regional model with lateral and lower boundary conditions from multiple global atmosphere-ocean coupled models from the Coupled Model Intercomparison Project Phase 5 (CMIP5). The future projections are based on three Representation Concentration Pathway (RCP) scenarios (viz., RCP2.6, RCP4.5, RCP8.5) of the IPCC.",2020-12-02,http://arxiv.org/abs/2012.10386v1,"J. Sanjay, R. Krishnan, M. V. S. Ramarao, R. Mahesh, Bhupendra Bahadur Singh, Jayashri Patel, Sandip Ingle, Preethi Bhaskar, J. V. Revadekar, T. P. Sabin, M. Mujumdar",arxiv.org,"physics.ao-ph, physics.geo-ph"
Climate-Contingent Finance,"Climate adaptation could yield significant benefits. However, the uncertainty of which future climate scenarios will occur decreases the feasibility of proactively adapting. Climate adaptation projects could be underwritten by benefits paid for in the climate scenarios that each adaptation project is designed to address because other entities would like to hedge the financial risk of those scenarios. Because the return on investment is a function of the level of climate change, it is optimal for the adapting entity to finance adaptation with repayment as a function of the climate. It is also optimal for entities with more financial downside under a more extreme climate to serve as an investing counterparty because they can obtain higher than market rates of return when they need it most.   In this way, parties proactively adapting would reduce the risk they over-prepare, while their investors would reduce the risk they under-prepare. This is superior to typical insurance because, by investing in climate-contingent mechanisms, investors are not merely financially hedging but also outright preventing physical damage, and therefore creating economic value. This coordinates capital through time and place according to parties' risk reduction capabilities and financial profiles, while also providing a diversifying investment return.   Climate-contingent finance can be generalized to any situation where entities share exposure to a risk where they lack direct control over whether it occurs (e.g., climate change, or a natural pandemic), and one type of entity can take proactive actions to benefit from addressing the effects of the risk if it occurs (e.g., through innovating on crops that would do well under extreme climate change or vaccination technology that could address particular viruses) with funding from another type of entity that seeks a targeted return to ameliorate the downside.",2022-07-05,http://arxiv.org/abs/2207.02064v1,John Nay,arxiv.org,"q-fin.GN, econ.GN, q-fin.EC"
A Survey on Exploratory Spatiotemporal Visual Analytics Approaches for   Climate Science,"Climate science produces a wealth of complex, high-dimensional, multivariate data from observations and numerical models. These data are critical for understanding climate changes and their socioeconomic impacts. Climate scientists are continuously evaluating output from numerical models against observations. This model evaluation process provides useful guidance to improve the numerical models and subsequent climate projections. Exploratory visual analytics systems possess the potential to significantly reduce the burden on scientists for traditional spatiotemporal analyses. In addition, technology and infrastructure advancements are further facilitating broader access to climate data. Climate scientists today can access climate data in distributed analytic environments and render exploratory visualizations for analyses. Efforts are ongoing to optimize the computational efficiency of spatiotemporal analyses to enable efficient exploration of massive data. These advances present further opportunities for the visualization community to innovate over the full landscape of challenges and requirements raised by scientists. In this report, we provide a comprehensive review of the challenges, requirements, and current approaches for exploratory spatiotemporal visual analytics solutions for climate data. We categorize the visual analytic techniques, systems, and tools presented in the relevant literature based on task requirements, data sources, statistical techniques, interaction methods, visualization techniques, performance evaluation methods, and application domains. Moreover, our analytic review identifies trends, limitations, and key challenges in visual analysis. This report will advance future research activities in climate visualizations and enables the end-users of climate data to identify effective climate change mitigation strategies.",2024-07-30,http://arxiv.org/abs/2407.21199v1,"Abdullah-Al-Raihan Nayeem, Dongyun Han, Huikyo Lee, Donghoon Kim, Daniel Feldman, William J. Tolone, Daniel Crichton, Isaac Cho",arxiv.org,cs.HC
Analysis of Climatic Trends and Variability in Indian Topography,"The climatic change is one of the serious concerns nowadays. The impacts of climate change are global in scope and unprecedented in scale. Moreover, a small perturbation in climatic changes affects not only the pristine ecosystem but also the socioeconomic sectors. Specifically, the affect of climatic changes is related to frequent casualties. This makes it essential to dwelve deeper into analyzing the socio-climatic trends and variability. This work provides a comprehensive analysis of India's climatic trends, emphasizing on regional variations and specifically delving into the unique climate of Delhi. Specifically, this research unveils the temporal and spatial variations in temperature patterns by amalgamating extensive datasets encompassing India's diverse landscapes. The study uses advanced statistical tools and methodologies to scrutinize temperature's annual and seasonal variability. The insights drawn from this rigorous analysis may offer invaluable contributions to regional planning strategies, adaptive measures, and informed decision-making amidst the complex impacts of climate change. By bridging the gap between broader climatic trends and localized impacts, this research aims to facilitate more effective measures to mitigate and adapt to the multifaceted challenges of climate change, ensuring a more nuanced and tailored approaches. We utilized the Mann-Kendall test and Theil-Sen's slope estimator to analyze the trends and variability of the climatic conditions over the decades. The results demonstrate that temperature variations have increased over 0.58oC on average over the last decade. Moreover, over last decade the variability of Indian states shows that Lakshadweep faced the highest change (0.87oC), highlighting coastal vulnerability, while Tripura observed the least change of 0.07oC.",2025-01-08,http://arxiv.org/abs/2501.04578v1,"Ayush Prusty, Akshita Gupta, Vivek Ashok Bohara",arxiv.org,cs.SI
Enhancing LLMs for Governance with Human Oversight: Evaluating and   Aligning LLMs on Expert Classification of Climate Misinformation for   Detecting False or Misleading Claims about Climate Change,"Climate misinformation is a problem that has the potential to be substantially aggravated by the development of Large Language Models (LLMs). In this study we evaluate the potential for LLMs to be part of the solution for mitigating online dis/misinformation rather than the problem. Employing a public expert annotated dataset and a curated sample of social media content we evaluate the performance of proprietary vs. open source LLMs on climate misinformation classification task, comparing them to existing climate-focused computer-assisted tools and expert assessments. Results show (1) open-source models substantially under-perform in classifying climate misinformation compared to proprietary models, (2) existing climate-focused computer-assisted tools leveraging expert-annotated datasets continues to outperform many of proprietary models, including GPT-4o, and (3) demonstrate the efficacy and generalizability of fine-tuning GPT-3.5-turbo on expert annotated dataset in classifying claims about climate change at the equivalency of climate change experts with over 20 years of experience in climate communication. These findings highlight 1) the importance of incorporating human-oversight, such as incorporating expert-annotated datasets in training LLMs, for governance tasks that require subject-matter expertise like classifying climate misinformation, and 2) the potential for LLMs in facilitating civil society organizations to engage in various governance tasks such as classifying false or misleading claims in domains beyond climate change such as politics and health science.",2025-01-23,http://arxiv.org/abs/2501.13802v2,"Mowafak Allaham, Ayse D. Lokmanoglu, P. Sol Hart, Erik C. Nisbet",arxiv.org,cs.CY
ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to   Answer Climate Change Queries,"As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.",2025-06-12,http://arxiv.org/abs/2506.13796v1,"Zhou Chen, Xiao Wang, Yuanhong Liao, Ming Lin, Yuqi Bai",arxiv.org,"cs.CL, cs.AI"
Short note on the Sirenia disappearance from the Euro-North African   realm during the Cenozoic: a link between climate and Supernovae?,"Sirenia are marine mammals that colonized the European shores up to 2.7 Ma. Their biodiversity evolution follows the climate evolution of the Cenozoic. However, several climate events, as well as the global climate trend of this Era are still struggling to be understood. When considering only Earth processes, the climate evolution of the Cenozoic is hard to understand. If the galactic environment is taken into account, some of these climate events, as well as the global climate trend, became more easily understood. The Milky Way, through Supernovae, may bring some answers to why Cenozoic climate had this evolution. With the assumption that SN can induced changes in Earth climate in long time scales, Sirenia disappearance from Europe would be a side effect of this process.",2014-09-26,http://arxiv.org/abs/1409.7589v1,"Francisco Cabral, Mario Cachao, Rui Jorge Agostinho, Goncalo Prista",arxiv.org,q-bio.PE
Transitions in climate and energy discourse between Hurricanes Katrina   and Sandy,"Although climate change and energy are intricately linked, their explicit connection is not always prominent in public discourse and the media. Disruptive extreme weather events, including hurricanes, focus public attention in new and different ways, offering a unique window of opportunity to analyze how a focusing event influences public discourse. Media coverage of extreme weather events simultaneously shapes and reflects public discourse on climate issues. Here we analyze climate and energy newspaper coverage of Hurricanes Katrina (2005) and Sandy (2012) using topic models, mathematical techniques used to discover abstract topics within a set of documents. Our results demonstrate that post-Katrina media coverage does not contain a climate change topic, and the energy topic is limited to discussion of energy prices, markets, and the economy with almost no explicit linkages made between energy and climate change. In contrast, post-Sandy media coverage does contain a prominent climate change topic, a distinct energy topic, as well as integrated representation of climate change and energy, indicating a shift in climate and energy reporting between Hurricane Katrina and Hurricane Sandy.",2015-10-19,http://arxiv.org/abs/1510.07494v2,"Emily M. Cody, Jennie C. Stephens, James P. Bagrow, Peter Sheridan Dodds, Christopher M. Danforth",arxiv.org,"physics.soc-ph, physics.pop-ph"
Climate Monitoring using Internet of X-Things,Global climate change is significantly affecting the life on planet Earth. Predicting timely changes in the climate is a big challenge and requires great attention from the scientific community. Research now suggests that using the internet of X-things (X-IoT) helps in monitoring global climate change.,2020-05-30,http://arxiv.org/abs/2006.00231v1,"Nasir Saeed, Tareq Y. Al-Naffouri, Mohamed-Slim Alouini",arxiv.org,eess.SP
"Divergent Perspectives on Expert Disagreement: Preliminary Evidence from   Climate Science, Climate Policy, Astrophysics, and Public Opinion","We report the results of an exploratory study that examines the judgments of climate scientists, climate policy experts, astrophysicists, and non-experts (N = 3,367) about the factors that contribute to the creation and persistence of disagreement within climate science and astrophysics. We found that, as compared to educated non-experts, climate experts believe that there is less disagreement within climate science about climate change and that methodological factors and personal or institutional biases play less significant roles in generating existing disagreements than is commonly reported or assumed. We also found that, commensurate with the greater inherent uncertainty and data lacunae in their field, astrophysicists working on cosmic rays were generally more willing to acknowledge expert disagreement, more open to the idea that a set of data can have multiple valid interpretations, and generally less quick to dismiss someone articulating a non-standard view as non-expert, than climate scientists.",2018-02-06,http://arxiv.org/abs/1802.01889v1,"James R. Beebe, Maria Baghramian, Luke O'C. Drury, Finnur Dellsen",arxiv.org,"physics.ao-ph, physics.soc-ph"
CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims,"We introduce CLIMATE-FEVER, a new publicly available dataset for verification of climate change-related claims. By providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-specific claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinformation on climate change. We adapt the methodology of FEVER [1], the largest dataset of artificially designed claims, to real-life claims collected from the Internet. While during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. We discuss the surprising, subtle complexity of modeling real-world climate-related claims within the \textsc{fever} framework, which we believe provides a valuable challenge for general natural language understanding. We hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and AI community.",2020-12-01,http://arxiv.org/abs/2012.00614v2,"Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, Markus Leippold",arxiv.org,"cs.CL, cs.AI"
A review of effects of climate change on Agriculture in Africa,"Currently, agriculture in Africa contributes only a tenth to global Green House Gas (GHG) emissions from agriculture. Despite its relatively low contribution to GHG, a conundrum of ""climate justice"", adverse impacts of climate change disproportionately threaten Africa's agriculture, the Continent's main economic sector. Consequently, we seek to review the effects of climate change on Agriculture.",2021-08-25,http://arxiv.org/abs/2108.12267v1,"Samuel Asante Gyamerah, Dennis Ikpe",arxiv.org,"physics.soc-ph, 62P12"
Revisiting the secondary climate attributes for transportation   infrastructure management: A Redux and Update for 2020,"Environmental conditions in various regions can have a severely negative impact on the longevity and durability of the civil engineering infrastructures. In 2018, a published paper used 1971 to 2010 NOAA data from the contiguous United States to examine the temporal changes in secondary climate attributes (freeze-thaw cycles and freeze index) using the climate normals from two time windows, 1971-2000 and 1981-2010. That paper investigated whether there have been statistically significant changes in climate attribute levels across the two time windows, and used GIS-based interpolation methods to develop isarithmic maps of the climate attributes to facilitate their interpretation and application. This paper updates that study. In this study, we use NOAA climatic data from 1991 to 2020 to construct continuous surface maps showing the values of the primary and secondary climate attributes at the 48 continental states. The new maps provide an updated picture of the freeze index values and freeze-thaw cycles for the most recent climate condition. These new values provide a better picture of the freezing season characteristics of the United States, and will provide information necessary for better winter maintenance procedures and infrastructure design to accommodate regional climate differences.",2022-02-25,http://arxiv.org/abs/2202.12480v1,"Tao Liao, Paul Kepley, Indraneel Kumar, Samuel Labi",arxiv.org,stat.OT
Towards Answering Climate Questionnaires from Unstructured Climate   Reports,"The topic of Climate Change (CC) has received limited attention in NLP despite its urgency. Activists and policymakers need NLP tools to effectively process the vast and rapidly growing unstructured textual climate reports into structured form. To tackle this challenge we introduce two new large-scale climate questionnaire datasets and use their existing structure to train self-supervised models. We conduct experiments to show that these models can learn to generalize to climate disclosures of different organizations types than seen during training. We then use these models to help align texts from unstructured climate documents to the semi-structured questionnaires in a human pilot study. Finally, to support further NLP research in the climate domain we introduce a benchmark of existing climate text classification datasets to better evaluate and compare existing models.",2023-01-11,http://arxiv.org/abs/2301.04253v2,"Daniel Spokoyny, Tanmay Laud, Tom Corringham, Taylor Berg-Kirkpatrick",arxiv.org,"cs.CL, cs.AI, cs.IR, cs.LG"
A unified repository for pre-processed climate data weighted by gridded   economic activity,"Although high-resolution gridded climate variables are provided by multiple sources, the need for country and region-specific climate data weighted by indicators of economic activity is becoming increasingly common in environmental and economic research. We process available information from different climate data sources to provide spatially aggregated data with global coverage for both countries (GADM0 resolution) and regions (GADM1 resolution) and for a variety of climate indicators (average precipitations, average temperatures, average SPEI). We weigh gridded climate data by population density or by night light intensity -- both proxies of economic activity -- before aggregation. Climate variables are measured daily, monthly, and annually, covering (depending on the data source) a time window from 1900 (at the earliest) to 2023. We pipeline all the preprocessing procedures in a unified framework, which we share in the open-access Weighted Climate Data Repository web app. Finally, we validate our data through a systematic comparison with those employed in leading climate impact studies.",2023-12-10,http://arxiv.org/abs/2312.05971v1,"Marco Gortan, Lorenzo Testa, Giorgio Fagiolo, Francesco Lamperti",arxiv.org,"econ.GN, q-fin.EC, stat.AP"
Generating High-Resolution Regional Precipitation Using Conditional   Diffusion Model,"Climate downscaling is a crucial technique within climate research, serving to project low-resolution (LR) climate data to higher resolutions (HR). Previous research has demonstrated the effectiveness of deep learning for downscaling tasks. However, most deep learning models for climate downscaling may not perform optimally for high scaling factors (i.e., 4x, 8x) due to their limited ability to capture the intricate details required for generating HR climate data. Furthermore, climate data behaves differently from image data, necessitating a nuanced approach when employing deep generative models. In response to these challenges, this paper presents a deep generative model for downscaling climate data, specifically precipitation on a regional scale. We employ a denoising diffusion probabilistic model (DDPM) conditioned on multiple LR climate variables. The proposed model is evaluated using precipitation data from the Community Earth System Model (CESM) v1.2.2 simulation. Our results demonstrate significant improvements over existing baselines, underscoring the effectiveness of the conditional diffusion model in downscaling climate data.",2023-12-12,http://arxiv.org/abs/2312.07112v1,"Naufal Shidqi, Chaeyoon Jeong, Sungwon Park, Elke Zeller, Arjun Babu Nellikkattil, Karandeep Singh",arxiv.org,"cs.LG, cs.AI, physics.ao-ph"
Climate Change from Large Language Models,"Climate change poses grave challenges, demanding widespread understanding and low-carbon lifestyle awareness. Large language models (LLMs) offer a powerful tool to address this crisis, yet comprehensive evaluations of their climate-crisis knowledge are lacking. This paper proposes an automated evaluation framework to assess climate-crisis knowledge within LLMs. We adopt a hybrid approach for data acquisition, combining data synthesis and manual collection, to compile a diverse set of questions encompassing various aspects of climate change. Utilizing prompt engineering based on the compiled questions, we evaluate the model's knowledge by analyzing its generated answers. Furthermore, we introduce a comprehensive set of metrics to assess climate-crisis knowledge, encompassing indicators from 10 distinct perspectives. These metrics provide a multifaceted evaluation, enabling a nuanced understanding of the LLMs' climate crisis comprehension. The experimental results demonstrate the efficacy of our proposed method. In our evaluation utilizing diverse high-performing LLMs, we discovered that while LLMs possess considerable climate-related knowledge, there are shortcomings in terms of timeliness, indicating a need for continuous updating and refinement of their climate-related content.",2023-12-19,http://arxiv.org/abs/2312.11985v3,"Hongyin Zhu, Prayag Tiwari",arxiv.org,"cs.CL, cs.CY"
Augmented CARDS: A machine learning approach to identifying triggers of   climate change misinformation on Twitter,"Misinformation about climate change poses a significant threat to societal well-being, prompting the urgent need for effective mitigation strategies. However, the rapid proliferation of online misinformation on social media platforms outpaces the ability of fact-checkers to debunk false claims. Automated detection of climate change misinformation offers a promising solution. In this study, we address this gap by developing a two-step hierarchical model, the Augmented CARDS model, specifically designed for detecting contrarian climate claims on Twitter. Furthermore, we apply the Augmented CARDS model to five million climate-themed tweets over a six-month period in 2022. We find that over half of contrarian climate claims on Twitter involve attacks on climate actors or conspiracy theories. Spikes in climate contrarianism coincide with one of four stimuli: political events, natural events, contrarian influencers, or convinced influencers. Implications for automated responses to climate misinformation are discussed.",2024-04-24,http://arxiv.org/abs/2404.15673v1,"Cristian Rojas, Frank Algra-Maschio, Mark Andrejevic, Travis Coan, John Cook, Yuan-Fang Li",arxiv.org,cs.LG
PACER: Physics Informed Uncertainty Aware Climate Emulator,"Climate models serve as critical tools for evaluating the effects of climate change and projecting future climate scenarios. However, the reliance on numerical simulations of physical equations renders them computationally intensive and inefficient. While deep learning methodologies have made significant progress in weather forecasting, they are still unstable for climate emulation tasks. Here, we propose PACER, a lightweight 684K parameter Physics Informed Uncertainty Aware Climate Emulator. PACER emulates temperature and precipitation stably for 86 years while only being trained on greenhouse gas emissions data. We incorporate a fundamental physical law of advection-diffusion in PACER accounting for boundary conditions and empirically estimating the diffusion co-efficient and flow velocities from emissions data. PACER has been trained on 15 climate models provided by ClimateSet outperforming baselines across most of the climate models and advancing a new state of the art in a climate diagnostic task.",2024-10-29,http://arxiv.org/abs/2410.21657v2,"Hira Saleem, Flora Salim, Cormac Purcell",arxiv.org,"physics.ao-ph, cs.AI, cs.LG"
Exploring Large Language Models for Climate Forecasting,"With the increasing impacts of climate change, there is a growing demand for accessible tools that can provide reliable future climate information to support planning, finance, and other decision-making applications. Large language models (LLMs), such as GPT-4, present a promising approach to bridging the gap between complex climate data and the general public, offering a way for non-specialist users to obtain essential climate insights through natural language interaction. However, an essential challenge remains under-explored: evaluating the ability of LLMs to provide accurate and reliable future climate predictions, which is crucial for applications that rely on anticipating climate trends. In this study, we investigate the capability of GPT-4 in predicting rainfall at short-term (15-day) and long-term (12-month) scales. We designed a series of experiments to assess GPT's performance under different conditions, including scenarios with and without expert data inputs. Our results indicate that GPT, when operating independently, tends to generate conservative forecasts, often reverting to historical averages in the absence of clear trend signals. This study highlights both the potential and challenges of applying LLMs for future climate predictions, providing insights into their integration with climate-related applications and suggesting directions for enhancing their predictive capabilities in the field.",2024-11-20,http://arxiv.org/abs/2411.13724v1,"Yang Wang, Hassan A. Karimi",arxiv.org,"cs.LG, cs.AI"
Shocking concerns: public perception about climate change and the   macroeconomy,"Public perceptions of climate change arguably contribute to shaping private adaptation and support for policy intervention. In this paper, we propose a novel Climate Concern Index (CCI), based on disaggregated web-search volumes related to climate change topics, to gauge the intensity and dynamic evolution of collective climate perceptions, and evaluate its impacts on the business cycle. Using data from the United States over the 2004:2024 span, we capture widespread shifts in perceived climate-related risks, particularly those consistent with the postcognitive interpretation of affective responses to extreme climate events. To assess the aggregate implications of evolving public concerns about the climate, we estimate a proxy-SVAR model and find that exogenous variation in the CCI entails a statistically significant drop in both employment and private consumption and a persistent surge in stock market volatility, while core inflation remains largely unaffected. These results suggest that, even in the absence of direct physical risks, heightened concerns for climate-related phenomena can trigger behavioral adaptation with nontrivial consequences for the macroeconomy, thereby demanding attention from institutional players in the macro-financial field.",2025-05-07,http://arxiv.org/abs/2505.04669v1,"Giovanni Angelini, Maria Elena Bontempi, Luca De Angelis, Paolo Neri, Marco Maria Sorge",arxiv.org,"econ.GN, econ.EM, q-fin.EC"
"Using machine learning to parameterize moist convection: potential for   modeling of climate, climate change and extreme events","The parameterization of moist convection contributes to uncertainty in climate modeling and numerical weather prediction. Machine learning (ML) can be used to learn new parameterizations directly from high-resolution model output, but it remains poorly understood how such parameterizations behave when fully coupled in a general circulation model (GCM) and whether they are useful for simulations of climate change or extreme events. Here, we focus on these issues using idealized tests in which an ML-based parameterization is trained on output from a conventional parameterization and its performance is assessed in simulations with a GCM. We use an ensemble of decision trees (random forest) as the ML algorithm, and this has the advantage that it automatically ensures conservation of energy and non-negativity of surface precipitation. The GCM with the ML convective parameterization runs stably and accurately captures important climate statistics including precipitation extremes without the need for special training on extremes. Climate change between a control climate and a warm climate is not captured if the ML parameterization is only trained on the control climate, but it is captured if the training includes samples from both climates. Remarkably, climate change is also captured when training only on the warm climate, and this is because the extratropics of the warm climate provides training samples for the tropics of the control climate. In addition to being potentially useful for the simulation of climate, we show that ML parameterizations can be interrogated to provide diagnostics of the interaction between convection and the large-scale environment.",2018-06-28,http://arxiv.org/abs/1806.11037v2,"Paul A. O'Gorman, John G. Dwyer",arxiv.org,physics.ao-ph
How relevant is climate change research for climate change policy? An   empirical analysis based on Overton data,"Climate change is an ongoing topic in nearly all areas of society since many years. A discussion of climate change without referring to scientific results is not imaginable. This is especially the case for policies since action on the macro scale is required to avoid costly consequences for society. In this study, we deal with the question of how research on climate change and policy are connected. In 2019, the new Overton database of policy documents was released including links to research papers that are cited by policy documents. The use of results and recommendations from research on climate change might be reflected in citations of scientific papers in policy documents. Although we suspect a lot of uncertainty related to the coverage of policy documents in Overton, there seems to be an impact of international climate policy cycles on policy document publication. We observe local peaks in climate policy documents around major decisions in international climate diplomacy. Our results point out that IGOs and think tanks -- with a focus on climate change -- have published more climate change policy documents than expected. We found that climate change papers that are cited in climate change policy documents received significantly more citations on average than climate change papers that are not cited in these documents. Both areas of society (science and policy) focus on similar climate change research fields: biology, earth sciences, engineering, and disease sciences. Based on these and other empirical results in this study, we propose a simple model of policy impact considering a chain of different document types: the chain starts with scientific assessment reports (systematic reviews) that lead via science communication documents (policy briefs, policy reports or plain language summaries) and government reports to legislative documents.",2022-03-10,http://arxiv.org/abs/2203.05358v1,"Lutz Bornmann, Robin Haunschild, Kevin Boyack, Werner Marx, Jan C. Minx",arxiv.org,physics.soc-ph
Regional climate risk assessment from climate models using probabilistic   machine learning,"Accurate, actionable climate information at km scales is crucial for robust natural hazard risk assessment and infrastructure planning. Simulating climate at these resolutions remains intractable, forcing reliance on downscaling: either physics-based or statistical methods that transform climate simulations from coarse to impact-relevant resolutions. One major challenge for downscaling is to comprehensively capture the interdependency among climate processes of interest, a prerequisite for representing climate hazards. However, current approaches either lack the desired scalability or are bespoke to specific types of hazards. We introduce GenFocal, a computationally efficient, general-purpose, end-to-end generative framework that gives rise to full probabilistic characterizations of complex climate processes interacting at fine spatiotemporal scales. GenFocal more accurately assesses extreme risk in the current climate than leading approaches, including one used in the US 5th National Climate Assessment. It produces plausible tracks of tropical cyclones, providing accurate statistics of their genesis and evolution, even when they are absent from the corresponding climate simulations. GenFocal also shows compelling results that are consistent with the literature on projecting climate impact on decadal timescales. GenFocal revolutionizes how climate simulations can be efficiently augmented with observations and harnessed to enable future climate impact assessments at the spatiotemporal scales relevant to local and regional communities. We believe this work establishes genAI as an effective paradigm for modeling complex, high-dimensional multivariate statistical correlations that have deterred precise quantification of climate risks associated with hazards such as wildfires, extreme heat, tropical cyclones, and flooding; thereby enabling the evaluation of adaptation strategies.",2024-12-11,http://arxiv.org/abs/2412.08079v2,"Zhong Yi Wan, Ignacio Lopez-Gomez, Robert Carver, Tapio Schneider, John Anderson, Fei Sha, Leonardo Zepeda-Núñez",arxiv.org,"cs.LG, cs.NA, math.NA, physics.ao-ph"
"Climate Sensitivity, Sea Level, and Atmospheric CO2","Cenozoic temperature, sea level and CO2 co-variations provide insights into climate sensitivity to external forcings and sea level sensitivity to climate change. Climate sensitivity depends on the initial climate state, but potentially can be accurately inferred from precise paleoclimate data. Pleistocene climate oscillations yield a fast-feedback climate sensitivity 3 +/- 1{\deg}C for 4 W/m2 CO2 forcing if Holocene warming relative to the Last Glacial Maximum (LGM) is used as calibration, but the error (uncertainty) is substantial and partly subjective because of poorly defined LGM global temperature and possible human influences in the Holocene. Glacial-to-interglacial climate change leading to the prior (Eemian) interglacial is less ambiguous and implies a sensitivity in the upper part of the above range, i.e., 3-4{\deg}C for 4 W/m2 CO2 forcing. Slow feedbacks, especially change of ice sheet size and atmospheric CO2, amplify total Earth system sensitivity by an amount that depends on the time scale considered. Ice sheet response time is poorly defined, but we show that the slow response and hysteresis in prevailing ice sheet models are exaggerated. We use a global model, simplified to essential processes, to investigate state-dependence of climate sensitivity, finding an increased sensitivity towards warmer climates, as low cloud cover is diminished and increased water vapor elevates the tropopause. Burning all fossil fuels, we conclude, would make much of the planet uninhabitable by humans, thus calling into question strategies that emphasize adaptation to climate change.",2012-11-20,http://arxiv.org/abs/1211.4846v2,"James Hansen, Makiko Sato, Gary Russell, Pushker Kharecha",arxiv.org,physics.ao-ph
Tipping elements and climate-economic shocks: Pathways toward integrated   assessment,"The literature on the costs of climate change often draws a link between climatic 'tipping points' and large economic shocks, frequently called 'catastrophes'. The use of the phrase 'tipping points' in this context can be misleading. In popular and social scientific discourse, 'tipping points' involve abrupt state changes. For some climatic 'tipping points,' the commitment to a state change may occur abruptly, but the change itself may be rate-limited and take centuries or longer to realize. Additionally, the connection between climatic 'tipping points' and economic losses is tenuous, though emerging empirical and process-model-based tools provide pathways for investigating it. We propose terminology to clarify the distinction between 'tipping points' in the popular sense, the critical thresholds exhibited by climatic and social 'tipping elements,' and 'economic shocks'. The last may be associated with tipping elements, gradual climate change, or non-climatic triggers. We illustrate our proposed distinctions by surveying the literature on climatic tipping elements, climatically sensitive social tipping elements, and climate-economic shocks, and we propose a research agenda to advance the integrated assessment of all three.",2016-03-02,http://arxiv.org/abs/1603.00850v3,"Robert E. Kopp, Rachael Shwom, Gernot Wagner, Jiacan Yuan",arxiv.org,"q-fin.EC, physics.soc-ph"
The Effect of Time Series Distance Functions on Functional Climate   Networks,"Complex network theory provides an important tool for the analysis of complex systems such as the Earth's climate. In this context, functional climate networks can be constructed using a spatiotemporal climate dataset and a suitable time series distance function. The resulting coarse-grained view on climate variability consists of representing distinct areas on the globe (i.e., grid cells) by nodes and connecting pairs of nodes that present similar time series. One fundamental concern when constructing such a functional climate network is the definition of a metric that captures the mutual similarity between time series. Here we study systematically the effect of 29 time series distance functions on functional climate network construction based on global temperature data. We observe that the distance functions previously used in the literature commonly generate very similar networks while alternative ones result in rather distinct network structures and reveal different long-distance connection patterns. These patterns are highly important for the study of climate dynamics since they generally represent pathways for the long-distance transportation of energy and can be used to forecast climate variability on subseasonal to interannual or even decadal scales. Therefore, we propose the measures studied here as alternatives for the analysis of climate variability and to further exploit their complementary capability of capturing different aspects of the underlying dynamics that may help gaining a more holistic empirical understanding of the global climate system.",2019-02-08,http://arxiv.org/abs/1902.03298v2,"Leonardo N. Ferreira, Nicole C. R. Ferreira, Elbert E. N. Macau, Reik V. Donner",arxiv.org,"physics.data-an, physics.ao-ph"
Using the past to constrain the future: how the palaeorecord can improve   estimates of global warming,"Climate sensitivity is defined as the change in global mean equilibrium temperature after a doubling of atmospheric CO2 concentration and provides a simple measure of global warming. An early estimate of climate sensitivity, 1.5-4.5{\deg}C, has changed little subsequently, including the latest assessment by the Intergovernmental Panel on Climate Change.   The persistence of such large uncertainties in this simple measure casts doubt on our understanding of the mechanisms of climate change and our ability to predict the response of the climate system to future perturbations. This has motivated continued attempts to constrain the range with climate data, alone or in conjunction with models. The majority of studies use data from the instrumental period (post-1850) but recent work has made use of information about the large climate changes experienced in the geological past.   In this review, we first outline approaches that estimate climate sensitivity using instrumental climate observations and then summarise attempts to use the record of climate change on geological timescales. We examine the limitations of these studies and suggest ways in which the power of the palaeoclimate record could be better used to reduce uncertainties in our predictions of climate sensitivity.",2012-04-21,http://arxiv.org/abs/1204.4807v1,"Tamsin L. Edwards, Michel Crucifix, Sandy P. Harrison",arxiv.org,physics.ao-ph
Emergent constraints on climate sensitivities,"Despite major advances in climate science over the last 30 years, persistent uncertainties in projections of future climate change remain. Climate projections are produced with increasingly complex models which attempt to represent key processes in the Earth system, including atmospheric and oceanic circulations, convection, clouds, snow, sea-ice, vegetation and interactions with the carbon cycle. Uncertainties in the representation of these processes feed through into a range of projections from the many state-of-the-art climate models now being developed and used worldwide. For example, despite major improvements in climate models, the range of equilibrium global warming due to doubling carbon dioxide still spans a range of more than three. Here we review a promising way to make use of the ensemble of climate models to reduce the uncertainties in the sensitivities of the real climate system. The emergent constraint approach uses the model ensemble to identify a relationship between an uncertain aspect of the future climate and an observable variation or trend in the contemporary climate. This review summarises previous published work on emergent constraints, and discusses the huge promise and potential dangers of the approach. Most importantly, it argues that emergent constraints should be based on well-founded physical principles such as the fluctuation-dissipation theorem. It is hoped that this review will stimulate physicists to contribute to the rapidly developing field of emergent constraints on climate projections, bringing to it much needed rigour and physical insights.",2020-12-17,http://arxiv.org/abs/2012.09468v1,"Mark S. Williamson, Chad W. Thackeray, Peter M. Cox, Alex Hall, Chris Huntingford, Femke J. M. M. Nijsse",arxiv.org,"physics.ao-ph, physics.geo-ph"
Regional Climate Change Datasets for South Asia,"The Centre for Climate Change Research (CCCR;http://cccr.tropmet.res.in) at the Indian Institute of Tropical Meteorology (IITM; http://www.tropmet.res.in), Pune, launched in 2009 with the support of the Ministry of Earth Sciences (MoES), Government of India, focuses on the development of new climate modelling capabilities in India and South Asia to address issues concerning the science of climate change. CCCR-IITM has the mandate of developing an Earth System Model and to make the regional climate projections. An important achievement was made by developing an Earth System Model at IITM, which is an important step towards understanding global and regional climate response to long-term climate variability and climate change. CCCR-IITM has also generated an ensemble of high resolution dynamically downscaled future projections of regional climate over South Asia and Indian monsoon, which are found useful for impact assessment studies and for quantifying uncertainties in the regional projections. A brief overview of these core climate change modeling activities of CCCR-IITM was presented in an Interim Report on Climate Change over India (available at http://cccr.tropmet.res.in/home/reports.jsp)",2020-12-02,http://arxiv.org/abs/2012.10387v1,"J. Sanjay, M. V. S. Ramarao, R. Mahesh, Sandip Ingle, Bhupendra Bahadur Singh, R. Krishnan",arxiv.org,physics.ao-ph
Wave Climate from Spectra and its Connections with Local and Remote Wind   Climate,"Because wind-generated waves can propagate over large distances, wave spectra from a fixed point can record information about air-sea interactions in distant areas. In this study, the spectral wave climate is computed for a specific location in the tropical Eastern Pacific Ocean. Several well-defined partitions independent of each other, referred to as wave-climate systems, are observed in the annual mean wave spectrum. Significant seasonal cycling, long-term trends, and correlations with climate indices are observed in the local wave spectra, showing the abundant climatic information they contain. The projections of the wind vector on the direction pointing to the target location are used to link the spectral wave climate and basin-scale wind climate. The origins of all the identified wave climate systems are clearly shown in the wind projection maps and some are thousands of kilometers away from the target point, demonstrating the validity of this connection. Comparisons are made between wave spectra and the corresponding local and remote wind fields with respect to seasonal and interannual variability, as well as the long-term trends. The results show that each frequency and direction of ocean wave spectra at a certain location can be linked to the wind field for a geographical area from a climatological point of view, implying that it is feasible to reconstruct a spectral wave climate from global observational wind field data and wind climate monitoring using observations of wave spectrum geographically far away.",2018-10-27,http://arxiv.org/abs/1810.11587v1,"Haoyu Jiang, Lin Mu",arxiv.org,physics.ao-ph
Machine learning for weather and climate are worlds apart,"Modern weather and climate models share a common heritage, and often even components, however they are used in different ways to answer fundamentally different questions. As such, attempts to emulate them using machine learning should reflect this. While the use of machine learning to emulate weather forecast models is a relatively new endeavour there is a rich history of climate model emulation. This is primarily because while weather modelling is an initial condition problem which intimately depends on the current state of the atmosphere, climate modelling is predominantly a boundary condition problem. In order to emulate the response of the climate to different drivers therefore, representation of the full dynamical evolution of the atmosphere is neither necessary, or in many cases, desirable. Climate scientists are typically interested in different questions also. Indeed emulating the steady-state climate response has been possible for many years and provides significant speed increases that allow solving inverse problems for e.g. parameter estimation. Nevertheless, the large datasets, non-linear relationships and limited training data make Climate a domain which is rich in interesting machine learning challenges.   Here I seek to set out the current state of climate model emulation and demonstrate how, despite some challenges, recent advances in machine learning provide new opportunities for creating useful statistical models of the climate.",2020-08-24,http://arxiv.org/abs/2008.10679v2,Duncan Watson-Parris,arxiv.org,"physics.ao-ph, stat.AP"
A Stochastic Climate Model -- An approach to calibrate the   Climate-Extended Risk Model (CERM),"The initial Climate-Extended Risk Model (CERM) addresses the estimate of climate-related financial risk embedded within a bank loan portfolio, through a climatic extension of the Basel II IRB model. It uses a Gaussian copula model calibrated with non stationary macro-correlations in order to reflect the future evolution of climate-related financial risks. In this complementary article, we propose a stochastic forward-looking methodology to calibrate climate macro-correlation evolution from scientific climate data, for physical and transition efforts specifically. We assume a global physical and transition risk, likened to persistent greenhouse gas (GHG) concentration in the atmosphere. The economic risk is considered stationary and can therefore be calibrated with a backward-looking methodology. We present 4 key principles to model the GDP and we propose to model the economic, physical and transition effort factors with three interdependent stochastic processes allowing for a calibration with seven well defined parameters. These parameters can be calibrated using public data. This new approach means not only to evaluate climate risks without picking any specific scenario but also allows to fill the gap between current one year approach of regulatory and economic capital models and the necessarily long-term view of climate risks by designing a framework to evaluate the resulting credit loss on each step (typically yearly) of the transition path. This new approach could prove instrumental in the 2022 context of central banks weighing the pros and cons of a climate capital charge.",2022-05-05,http://arxiv.org/abs/2205.02581v1,"Jean-Baptiste Gaudemet, Jules Deschamps, Olivier Vinciguerra",arxiv.org,q-fin.RM
A Radiative-Convective Model for Terrestrial Planets with   Self-Consistent Patchy Clouds,"Clouds are ubiquitous\, -- \,they arise for every solar system planet that possesses an atmosphere and have also been suggested as a leading mechanism for obscuring spectral features in exoplanet observations. As exoplanet observations continue to improve, there is a need for efficient and general planetary climate models that appropriately handle the possible cloudy atmospheric environments that arise on these worlds. We generate a new 1D radiative-convective terrestrial planet climate model that self-consistently handles patchy clouds through a parameterized microphysical treatment of condensation and sedimentation processes. Our model is general enough to recreate Earth's atmospheric radiative environment without over-parameterization, while also maintaining a simple implementation that is applicable to a wide range of atmospheric compositions and physical planetary properties. We first validate this new 1D patchy cloud radiative-convective climate model by comparing it to Earth thermal structure data and to existing climate and radiative transfer tools. We produce partially-clouded Earth-like climates with cloud structures that are representative of deep tropospheric convection and are adequate 1D representations of clouds within rocky planet atmospheres. After validation against Earth, we then use our partially clouded climate model and explore the potential climates of super-Earth exoplanets with secondary nitrogen-dominated atmospheres which we assume are abiotic. We also couple the partially clouded climate model to a full-physics, line-by-line radiative transfer model and generate high-resolution spectra of simulated climates. These self-consistent climate-to-spectral models bridge the gap between climate modeling efforts and observational studies of rocky worlds.",2022-10-18,http://arxiv.org/abs/2210.10004v1,"James D. Windsor, Tyler D. Robinson, Ravi kumar Kopparapu, David E. Trilling, Joe LLama, Amber Young",arxiv.org,astro-ph.EP
Theoretical tools for understanding the climate crisis from Hasselmann's   program and beyond,"Klaus Hasselmann's revolutionary intuition in climate science was to take advantage of the stochasticity associated with fast weather processes to probe the slow dynamics of the climate system. This has led to fundamentally new ways to study the response of climate models to perturbations, and to perform detection and attribution for climate change signals. Hasselmann's program has been extremely influential in climate science and beyond. We first summarise the main aspects of such a program using modern concepts and tools of statistical physics and applied mathematics. We then provide an overview of some promising scientific perspectives that might better clarify the science behind the climate crisis and that stem from Hasselmann's ideas. We show how to perform rigorous model reduction by constructing parametrizations in systems that do not necessarily feature a time-scale separation between unresolved and resolved processes. We propose a general framework for explaining the relationship between climate variability and climate change, and for performing climate change projections. This leads us seamlessly to explain some key general aspects of climatic tipping points. Finally, we show that response theory provides a solid framework supporting optimal fingerprinting methods for detection and attribution.",2023-03-21,http://arxiv.org/abs/2303.12009v3,"Valerio Lucarini, Mickaël Chekroun",arxiv.org,"physics.ao-ph, cond-mat.stat-mech, nlin.CD, physics.geo-ph, physics.soc-ph"
Characterizing climate pathways using feature importance on echo state   networks,"The 2022 National Defense Strategy of the United States listed climate change as a serious threat to national security. Climate intervention methods, such as stratospheric aerosol injection, have been proposed as mitigation strategies, but the downstream effects of such actions on a complex climate system are not well understood. The development of algorithmic techniques for quantifying relationships between source and impact variables related to a climate event (i.e., a climate pathway) would help inform policy decisions. Data-driven deep learning models have become powerful tools for modeling highly nonlinear relationships and may provide a route to characterize climate variable relationships. In this paper, we explore the use of an echo state network (ESN) for characterizing climate pathways. ESNs are a computationally efficient neural network variation designed for temporal data, and recent work proposes ESNs as a useful tool for forecasting spatio-temporal climate data. Like other neural networks, ESNs are non-interpretable black-box models, which poses a hurdle for understanding variable relationships. We address this issue by developing feature importance methods for ESNs in the context of spatio-temporal data to quantify variable relationships captured by the model. We conduct a simulation study to assess and compare the feature importance techniques, and we demonstrate the approach on reanalysis climate data. In the climate application, we select a time period that includes the 1991 volcanic eruption of Mount Pinatubo. This event was a significant stratospheric aerosol injection, which we use as a proxy for an artificial stratospheric aerosol injection. Using the proposed approach, we are able to characterize relationships between pathway variables associated with this event.",2023-10-12,http://arxiv.org/abs/2310.08495v1,"Katherine Goode, Daniel Ries, Kellie McClernon",arxiv.org,"stat.ML, cs.LG, stat.AP"
"Stress-testing the coupled behavior of hybrid physics-machine learning   climate simulations on an unseen, warmer climate","Accurate and computationally-viable representations of clouds and turbulence are a long-standing challenge for climate model development. Traditional parameterizations that crudely but efficiently approximate these processes are a leading source of uncertainty in long-term projected warming and precipitation patterns. Machine Learning (ML)-based parameterizations have long been hailed as a promising alternative with the potential to yield higher accuracy at a fraction of the cost of more explicit simulations. However, these ML variants are often unpredictably unstable and inaccurate in \textit{coupled} testing (i.e. in a downstream hybrid simulation task where they are dynamically interacting with the large-scale climate model). These issues are exacerbated in out-of-distribution climates. Certain design decisions such as ``climate-invariant"" feature transformation for moisture inputs, input vector expansion, and temporal history incorporation have been shown to improve coupled performance, but they may be insufficient for coupled out-of-distribution generalization. If feature selection and transformations can inoculate hybrid physics-ML climate models from non-physical, out-of-distribution extrapolation in a changing climate, there is far greater potential in extrapolating from observational data. Otherwise, training on multiple simulated climates becomes an inevitable necessity. While our results show generalization benefits from these design decisions, the obtained improvment does not sufficiently preclude the necessity of using multi-climate simulated training data.",2024-01-04,http://arxiv.org/abs/2401.02098v1,"Jerry Lin, Mohamed Aziz Bhouri, Tom Beucler, Sungduk Yu, Michael Pritchard",arxiv.org,physics.ao-ph
Identifying Climate Targets in National Laws and Policies using Machine   Learning,"Quantified policy targets are a fundamental element of climate policy, typically characterised by domain-specific and technical language. Current methods for curating comprehensive views of global climate policy targets entail significant manual effort. At present there are few scalable methods for extracting climate targets from national laws or policies, which limits policymakers' and researchers' ability to (1) assess private and public sector alignment with global goals and (2) inform policy decisions. In this paper we present an approach for extracting mentions of climate targets from national laws and policies. We create an expert-annotated dataset identifying three categories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable energy targets)) and train a classifier to reliably identify them in text. We investigate bias and equity impacts related to our model and identify specific years and country names as problematic features. Finally, we investigate the characteristics of the dataset produced by running this classifier on the Climate Policy Radar (CPR) dataset of global national climate laws and policies and UNFCCC submissions, highlighting the potential of automated and scalable data collection for existing climate policy databases and supporting further research. Our work represents a significant upgrade in the accessibility of these key climate policy elements for policymakers and researchers. We publish our model at https://huggingface.co/ClimatePolicyRadar/national-climate-targets and related dataset at https://huggingface.co/datasets/ClimatePolicyRadar/national-climate-targets.",2024-04-03,http://arxiv.org/abs/2404.02822v2,"Matyas Juhasz, Tina Marchand, Roshan Melwani, Kalyan Dutia, Sarah Goodenough, Harrison Pim, Henry Franks",arxiv.org,"cs.CY, cs.CL, cs.LG"
From Opinion Polarization to Climate Action: A Social-Climate Model of   the Opinion Spectrum,"We developed a coupled social-climate network model to understand the interaction between climate change opinion spread and the climate system and determine the role of this interaction in shaping collective actions and global temperature changes. In contrast to previous social-climate models that discretized opinions, we assumed opinions on climate change form a continuum, and were thereby able to capture more nuanced interactions. The model shows that resistance to behaviour change, elevated mitigation costs, and slow response to climate events can result in a global temperature anomaly in excess of 2{\deg}C. However, this outcome could be avoided by lowering mitigation costs and increasing the rate of interactions between individuals with differing opinions (social learning). Our model is the first to demonstrate the emergence of opinion polarization in a human-environment system. We predict that polarization of opinions in a population can be extinguished, and the population will adopt mitigation practices, when the response to temperature change is sensitive, even at higher mitigation costs. It also indicates that even with polarized opinion, an average pro-mitigative opinion in the population can reduce emissions. Finally, our model underscores how frequent and unexpected social or environmental changes, such as policy changes or extreme weather events, can slow climate change mitigation. This analysis helps identify the factors that support achieving international climate goals, such as leveraging peer influence and decreasing stubbornness in individuals, reducing mitigation costs, and encouraging climate-friendly lifestyles. Our model offers a valuable new framework for exploring the integration of social and natural sciences, particularly in the domain of human behavioural change.",2025-03-06,http://arxiv.org/abs/2503.04689v1,"Athira Satheesh Kumar, Krešimir Josić, Chris T Bauch, Madhur Anand",arxiv.org,math.DS
Computational Analysis of Climate Policy,"This thesis explores the impact of the Climate Emergency movement on local government climate policy, using computational methods. The Climate Emergency movement sought to accelerate climate action at local government level through the mechanism of Climate Emergency Declarations (CEDs), resulting in a series of commitments from councils to treat climate change as an emergency. With the aim of assessing the potential of current large language models to answer complex policy questions, I first built and configured a system named PALLM (Policy Analysis with a Large Language Model), using the OpenAI model GPT-4. This system is designed to apply a conceptual framework for climate emergency response plans to a dataset of climate policy documents. I validated the performance of this system with the help of local government policymakers, by generating analyses of the climate policies of 11 local governments in Victoria and assessing the policymakers' level of agreement with PALLM's responses. Having established that PALLM's performance is satisfactory, I used it to conduct a large-scale analysis of current policy documents from local governments in the state of Victoria, Australia. This thesis presents the methodology and results of this analysis, comparing the results for councils which have passed a CED to those which did not. This study finds that GPT-4 is capable of high-level policy analysis, with limitations including a lack of reliable attribution, and can also enable more nuanced analysis by researchers. Its use in this research shows that councils which have passed a CED are more likely to have a recent and climate-specific policy, and show more attention to urgency, prioritisation, and equity and social justice, than councils which have not. It concludes that the ability to assess policy documents at scale opens up exciting new opportunities for policy researchers.",2025-06-13,http://arxiv.org/abs/2506.22449v1,Carolyn Hicks,arxiv.org,"cs.CY, cs.CL"
Water vapor and the dynamics of climate changes,"Water vapor is not only Earth's dominant greenhouse gas. Through the release of latent heat when it condenses, it also plays an active role in dynamic processes that shape the global circulation of the atmosphere and thus climate. Here we present an overview of how latent heat release affects atmosphere dynamics in a broad range of climates, ranging from extremely cold to extremely warm. Contrary to widely held beliefs, atmospheric circulation statistics can change non-monotonically with global-mean surface temperature, in part because of dynamic effects of water vapor. For example, the strengths of the tropical Hadley circulation and of zonally asymmetric tropical circulations, as well as the kinetic energy of extratropical baroclinic eddies, can be lower than they presently are both in much warmer climates and in much colder climates. We discuss how latent heat release is implicated in such circulation changes, particularly through its effect on the atmospheric static stability, and we illustrate the circulation changes through simulations with an idealized general circulation model. This allows us to explore a continuum of climates, constrain macroscopic laws governing this climatic continuum, and place past and possible future climate changes in a broader context.",2009-08-30,http://arxiv.org/abs/0908.4410v2,"Tapio Schneider, Paul A. O'Gorman, Xavier Levine",arxiv.org,"physics.ao-ph, physics.flu-dyn"
On the state dependency of fast feedback processes in (palaeo) climate   sensitivity,"Palaeo data have been frequently used to determine the equilibrium (Charney) climate sensitivity $S^a$, and - if slow feedback processes (e.g. land ice-albedo) are adequately taken into account - they indicate a similar range as estimates based on instrumental data and climate model results. Most studies implicitly assume the (fast) feedback processes to be independent of the background climate state, e.g., equally strong during warm and cold periods. Here we assess the dependency of the fast feedback processes on the background climate state using data of the last 800 kyr and a conceptual climate model for interpretation. Applying a new method to account for background state dependency, we find $S^a=0.61\pm0.06$ K(Wm$^{-2}$)$^{-1}$ using the latest LGM temperature reconstruction and significantly lower climate sensitivity during glacial climates. Due to uncertainties in reconstructing the LGM temperature anomaly, $S^a$ is estimated in the range $S^a=0.55-0.95$ K(Wm$^{-2}$)$^{-1}$.",2014-03-21,http://arxiv.org/abs/1403.5391v2,"Anna S. von der Heydt, Peter Köhler, Roderik S. W. van de Wal, Henk A. Dijkstra",arxiv.org,physics.ao-ph
The Physics of Climate Variability and Climate Change,"The climate system is a forced, dissipative, nonlinear, complex and heterogeneous system that is out of thermodynamic equilibrium. The system exhibits natural variability on many scales of motion, in time as well as space, and it is subject to various external forcings, natural as well as anthropogenic. This paper reviews the observational evidence on climate phenomena and the governing equations of planetary-scale flow, as well as presenting the key concept of a hierarchy of models as used in the climate sciences. Recent advances in the application of dynamical systems theory, on the one hand, and of nonequilibrium statistical physics, on the other, are brought together for the first time and shown to complement each other in helping understand and predict the system's behavior. These complementary points of view permit a self-consistent handling of subgrid-scale phenomena as stochastic processes, as well as a unified handling of natural climate variability and forced climate change, along with a treatment of the crucial issues of climate sensitivity, response, and predictability.",2019-10-01,http://arxiv.org/abs/1910.00583v2,"Michael Ghil, Valerio Lucarini",arxiv.org,"physics.ao-ph, cond-mat.stat-mech, nlin.CD, physics.flu-dyn, physics.geo-ph"
Global regularity for a class of 3D tropical climate model without   thermal diffusion,"In this paper, we establish the global regularity for the 3D tropical climate model with fractional dissipation.",2019-05-13,http://arxiv.org/abs/1905.04816v1,"Jinlu Li, Yanghai Yu",arxiv.org,math.AP
The Human Effect Requires Affect: Addressing Social-Psychological   Factors of Climate Change with Machine Learning,"Machine learning has the potential to aid in mitigating the human effects of climate change. Previous applications of machine learning to tackle the human effects in climate change include approaches like informing individuals of their carbon footprint and strategies to reduce it. For these methods to be the most effective they must consider relevant social-psychological factors for each individual. Of social-psychological factors at play in climate change, affect has been previously identified as a key element in perceptions and willingness to engage in mitigative behaviours. In this work, we propose an investigation into how affect could be incorporated to enhance machine learning based interventions for climate change. We propose using affective agent-based modelling for climate change as well as the use of a simulated climate change social dilemma to explore the potential benefits of affective machine learning interventions. Behavioural and informational interventions can be a powerful tool in helping humans adopt mitigative behaviours. We expect that utilizing affective ML can make interventions an even more powerful tool and help mitigative behaviours become widely adopted.",2020-11-24,http://arxiv.org/abs/2011.12443v1,"Kyle Tilbury, Jesse Hoey",arxiv.org,cs.AI
Quantifying uncertainty about global and regional economic impacts of   climate change,"The economic impacts of climate change are highly uncertain. Two of the most important uncertainties are the sensitivity of the climate system and the so-called damage functions, which relate climate change to economic damages and benefits. Despite broad awareness of these uncertainties, it is unclear which of them is most important, both on the global as well as the regional level. Here we apply different damage functions to data from climate models with vastly different climate sensitivities, and find that uncertainty in both climate sensitivity and economic damage per degree of warming are of similar importance for the global economic impact. Increasing the climate sensitivity or the sensitivity of the damage function both increases the economic damages globally. Yet, at the country-level the effect varies depending on the initial temperature as well as how much the country warms. Our findings emphasise the importance of including these uncertainties in estimates of future economic impacts, as they both are vital for the resulting impacts and thus policy implications.",2021-02-25,http://arxiv.org/abs/2102.13051v1,"Jenny Bjordal, Trude Storelvmo, Anthony A. Smith Jr",arxiv.org,physics.ao-ph
Handling Climate Change Using Counterfactuals: Using Counterfactuals in   Data Augmentation to Predict Crop Growth in an Uncertain Climate Future,"Climate change poses a major challenge to humanity, especially in its impact on agriculture, a challenge that a responsible AI should meet. In this paper, we examine a CBR system (PBI-CBR) designed to aid sustainable dairy farming by supporting grassland management, through accurate crop growth prediction. As climate changes, PBI-CBRs historical cases become less useful in predicting future grass growth. Hence, we extend PBI-CBR using data augmentation, to specifically handle disruptive climate events, using a counterfactual method (from XAI). Study 1 shows that historical, extreme climate-events (climate outlier cases) tend to be used by PBI-CBR to predict grass growth during climate disrupted periods. Study 2 shows that synthetic outliers, generated as counterfactuals on a outlier-boundary, improve the predictive accuracy of PBICBR, during the drought of 2018. This study also shows that an instance-based counterfactual method does better than a benchmark, constraint-guided method.",2021-04-08,http://arxiv.org/abs/2104.04008v1,"Mohammed Temraz, Eoin Kenny, Elodie Ruelle, Laurence Shalloo, Barry Smyth, Mark T Keane",arxiv.org,cs.AI
AIRCC-Clim: a user-friendly tool for generating regional probabilistic   climate change scenarios and risk measures,"Complex physical models are the most advanced tools available for producing realistic simulations of the climate system. However, such levels of realism imply high computational cost and restrictions on their use for policymaking and risk assessment. Two central characteristics of climate change are uncertainty and that it is a dynamic problem in which international actions can significantly alter climate projections and information needs, including partial and full compliance of global climate goals. Here we present AIRCC-Clim, a simple climate model emulator that produces regional probabilistic climate change projections of monthly and annual temperature and precipitation, as well as risk measures, based both on standard and user-defined emissions scenarios for six greenhouse gases. AIRCC-Clim emulates 37 atmosphere-ocean coupled general circulation models with low computational and technical requirements for the user. This standalone, user-friendly software is designed for a variety of applications including impact assessments, climate policy evaluation and integrated assessment modelling.",2021-10-30,http://arxiv.org/abs/2111.01762v1,"Francisco Estrada, Oscar Calderón-Bustamante, Wouter Botzen, Julián A. Velasco, Richard S. J. Tol",arxiv.org,"physics.ao-ph, econ.GN, q-fin.EC, stat.AP"
TSUBASA: Climate Network Construction on Historical and Real-Time Data,"A climate network represents the global climate system by the interactions of a set of anomaly time-series. Network science has been applied on climate data to study the dynamics of a climate network. The core task and first step to enable interactive network science on climate data is the efficient construction and update of a climate network on user-defined time-windows. We present TSUBASA, an algorithm for the efficient construction of climate networks based on the exact calculation of Pearsons correlation of large time-series. By pre-computing simple and low-overhead statistics, TSUBASA can efficiently compute the exact pairwise correlation of time-series on arbitrary time windows at query time. For real-time data, TSUBASA proposes a fast and incremental way of updating a network at interactive speed. Our experiments show that TSUBASA is faster than approximate solutions at least one order of magnitude for both historical and real-time data and outperforms a baseline for time-series correlation calculation up to two orders of magnitude.",2022-03-23,http://arxiv.org/abs/2203.16457v1,"Yunlong Xu, Jinshu Liu, Fatemeh Nargesian",arxiv.org,"physics.data-an, physics.ao-ph"
Testing the free-rider hypothesis in climate policy,"Free-riding is widely perceived as a key obstacle for effective climate policy. In the game-theoretic literature on non-cooperative climate policy and on climate cooperation, the free-rider hypothesis is ubiquitous. Yet, the free-rider hypothesis has not been tested empirically in the climate policy context. With the help of a theoretical model, we demonstrate that if free-riding were the main driver of lax climate policies around the globe, then there should be a pronounced country-size effect: Countries with a larger share of the world's population should, all else equal, internalize more climate damages and thus set higher carbon prices. We use this theoretical prediction for testing the free-rider hypothesis empirically. Drawing on data on emission-weighted carbon prices from 2020, while controlling for a host of other potential explanatory variables of carbon pricing, we find that the free-rider hypothesis cannot be supported empirically, based on the criterion that we propose. Hence, other issues may be more important for explaining climate policy stringency or the lack thereof in many countries.",2022-11-11,http://arxiv.org/abs/2211.06209v1,"Robert C. Schmidt, Moritz Drupp, Frikk Nesje, Hendrik Hoegen",arxiv.org,"econ.GN, q-fin.EC"
Climate Change and Open Science,Obtaining reliable answers to the major scientific questions raised by climate change in time to take appropriate action gives added urgency to the open access program.,2013-08-26,http://arxiv.org/abs/1308.5533v1,Ian Percival,arxiv.org,physics.gen-ph
HECT: High-Dimensional Ensemble Consistency Testing for Climate Models,"Climate models play a crucial role in understanding the effect of environmental and man-made changes on climate to help mitigate climate risks and inform governmental decisions. Large global climate models such as the Community Earth System Model (CESM), developed by the National Center for Atmospheric Research, are very complex with millions of lines of code describing interactions of the atmosphere, land, oceans, and ice, among other components. As development of the CESM is constantly ongoing, simulation outputs need to be continuously controlled for quality. To be able to distinguish a ""climate-changing"" modification of the code base from a true climate-changing physical process or intervention, there needs to be a principled way of assessing statistical reproducibility that can handle both spatial and temporal high-dimensional simulation outputs. Our proposed work uses probabilistic classifiers like tree-based algorithms and deep neural networks to perform a statistically rigorous goodness-of-fit test of high-dimensional spatio-temporal data.",2020-10-08,http://arxiv.org/abs/2010.04051v2,"Niccolò Dalmasso, Galen Vincent, Dorit Hammerling, Ann B. Lee",arxiv.org,"stat.AP, stat.ML"
Analysis of the Evolution of Parametric Drivers of High-End Sea-Level   Hazards,"Climate models are critical tools for developing strategies to manage the risks posed by sea-level rise to coastal communities. While these models are necessary for understanding climate risks, there is a level of uncertainty inherent in each parameter in the models. This model parametric uncertainty leads to uncertainty in future climate risks. Consequently, there is a need to understand how those parameter uncertainties impact our assessment of future climate risks and the efficacy of strategies to manage them. Here, we use random forests to examine the parametric drivers of future climate risk and how the relative importances of those drivers change over time. We find that the equilibrium climate sensitivity and a factor that scales the effect of aerosols on radiative forcing are consistently the most important climate model parametric uncertainties throughout the 2020 to 2150 interval for both low and high radiative forcing scenarios. The near-term hazards of high-end sea-level rise are driven primarily by thermal expansion, while the longer-term hazards are associated with mass loss from the Antarctic and Greenland ice sheets. Our results highlight the practical importance of considering time-evolving parametric uncertainties when developing strategies to manage future climate risks.",2021-06-11,http://arxiv.org/abs/2106.12041v1,"Alana Hough, Tony E. Wong",arxiv.org,"physics.ao-ph, cs.LG, physics.geo-ph"
Automated Climate Analyses Using Knowledge Graph,"The FAIR (Findable, Accessible, Interoperable, Reusable) data principles are fundamental for climate researchers and all stakeholders in the current digital ecosystem. In this paper, we demonstrate how relational climate data can be ""FAIR"" and modeled using RDF, in line with Semantic Web technologies and our Climate Analysis ontology. Thus, heterogeneous climate data can be stored in graph databases and offered as Linked Data on the Web. As a result, climate researchers will be able to use the standard SPARQL query language to query these sources directly on the Web. In this paper, we demonstrate the usefulness of our SPARQL endpoint for automated climate analytics. We illustrate two sample use cases that establish the advantage of representing climate data as knowledge graphs.",2021-10-21,http://arxiv.org/abs/2110.11039v1,"Jiantao Wu, Huan Chen, Fabrizio Orlandi, Yee Hui Lee, Declan O'Sullivan, Soumyabrata Dev",arxiv.org,cs.DB
On the Financing of Climate Change Adaptation in Developing Countries,"I offer reflections on adaptation to climate change, with emphasis on developing areas.",2022-10-20,http://arxiv.org/abs/2210.11525v1,Francis X. Diebold,arxiv.org,"econ.GN, q-fin.EC"
Dynamic Grouping for Climate Change Negotiation: Facilitating   Cooperation and Balancing Interests through Effective Strategies,"The current framework for climate change negotiation models presents several limitations that warrant further research and development. In this track, we discuss mainly two key areas for improvement, focusing on the geographical impacts and utility framework. In the aspects of geographical impacts, We explore five critical aspects: (1) the shift from local to global impact, (2) variability in climate change effects across regions, (3) heterogeneity in geographical location and political structures, and (4) collaborations between adjacent nations, (5) the importance of including historical and cultural factors influencing climate negotiations. Furthermore, we emphasize the need to refine the utility and rewards framework to reduce the homogeneity and the level of overestimating the climate mitigation by integrating the positive effects of saving rates into the reward function and heterogeneity among all regions. By addressing these limitations, we hope to enhance the accuracy and effectiveness of climate change negotiation models, enabling policymakers and stakeholders to devise targeted and appropriate strategies to tackle climate change at both regional and global levels.",2023-07-26,http://arxiv.org/abs/2307.13886v1,"Duo Zhang, Yuren Pang, Yu Qin",arxiv.org,"cs.CY, cs.AI"
Encoding Seasonal Climate Predictions for Demand Forecasting with   Modular Neural Network,"Current time-series forecasting problems use short-term weather attributes as exogenous inputs. However, in specific time-series forecasting solutions (e.g., demand prediction in the supply chain), seasonal climate predictions are crucial to improve its resilience. Representing mid to long-term seasonal climate forecasts is challenging as seasonal climate predictions are uncertain, and encoding spatio-temporal relationship of climate forecasts with demand is complex.   We propose a novel modeling framework that efficiently encodes seasonal climate predictions to provide robust and reliable time-series forecasting for supply chain functions. The encoding framework enables effective learning of latent representations -- be it uncertain seasonal climate prediction or other time-series data (e.g., buyer patterns) -- via a modular neural network architecture. Our extensive experiments indicate that learning such representations to model seasonal climate forecast results in an error reduction of approximately 13\% to 17\% across multiple real-world data sets compared to existing demand forecasting methods.",2023-09-05,http://arxiv.org/abs/2309.02248v1,"Smit Marvaniya, Jitendra Singh, Nicolas Galichet, Fred Ochieng Otieno, Geeth De Mel, Kommy Weldemariam",arxiv.org,"cs.LG, cs.AI"
CMIP X-MOS: Improving Climate Models with Extreme Model Output   Statistics,"Climate models are essential for assessing the impact of greenhouse gas emissions on our changing climate and the resulting increase in the frequency and severity of natural disasters. Despite the widespread acceptance of climate models produced by the Coupled Model Intercomparison Project (CMIP), they still face challenges in accurately predicting climate extremes, which pose most significant threats to both people and the environment. To address this limitation and improve predictions of natural disaster risks, we introduce Extreme Model Output Statistics (X-MOS). This approach utilizes deep regression techniques to precisely map CMIP model outputs to real measurements obtained from weather stations, which results in a more accurate analysis of the XXI climate extremes. In contrast to previous research, our study places a strong emphasis on enhancing the estimation of the tails of future climate parameter distributions. The latter supports decision-makers, enabling them to better assess climate-related risks across the globe.",2023-10-24,http://arxiv.org/abs/2311.03370v1,"Vsevolod Morozov, Artem Galliamov, Aleksandr Lukashevich, Antonina Kurdukova, Yury Maximov",arxiv.org,"physics.ao-ph, cs.LG, stat.AP"
Understanding the Impact of Seasonal Climate Change on Canada's Economy   by Region and Sector,"To assess the impact of climate change on the Canadian economy, we investigate and model the relationship between seasonal climate variables and economic growth across provinces and economic sectors. We further provide projections of climate change impacts up to the year 2050, taking into account the diverse climate change patterns and economic conditions across Canada. Our results indicate that rising Fall temperature anomalies have a notable adverse impact on Canadian economic growth. Province-wide, Saskatchewan and Manitoba are anticipated to experience the most substantial declines, whereas British Columbia and the Maritime provinces will be less impacted. Industry-wide, Mining is projected to see the greatest benefits, while Agriculture and Manufacturing are projected to have the most significant downturns. The disparities of climate change effects between provinces and industries highlight the need for governments to tailor their policies accordingly, and offer targeted assistance to regions and industries that are particularly vulnerable in the face of climate change. Targeted approaches to climate change mitigation are likely to be more effective than one-size-fits-all policies for the whole economy.",2023-11-06,http://arxiv.org/abs/2311.03497v1,"Shiyu He, Trang Bui, Yuying Huang, Wenling Zhang, Jie Jian, Samuel W. K. Wong, Tony S. Wirjanto",arxiv.org,stat.AP
ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate   Statements?,"Evaluating the accuracy of outputs generated by Large Language Models (LLMs) is especially important in the climate science and policy domain. We introduce the Expert Confidence in Climate Statements (ClimateX) dataset, a novel, curated, expert-labeled dataset consisting of 8094 climate statements collected from the latest Intergovernmental Panel on Climate Change (IPCC) reports, labeled with their associated confidence levels. Using this dataset, we show that recent LLMs can classify human expert confidence in climate-related statements, especially in a few-shot learning setting, but with limited (up to 47%) accuracy. Overall, models exhibit consistent and significant over-confidence on low and medium confidence statements. We highlight implications of our results for climate communication, LLMs evaluation strategies, and the use of LLMs in information retrieval systems.",2023-11-28,http://arxiv.org/abs/2311.17107v1,"Romain Lacombe, Kerrie Wu, Eddie Dilworth",arxiv.org,"cs.LG, cs.AI, cs.CL, cs.CY, cs.IR"
Identifying high resolution benchmark data needs and Novel data-driven   methodologies for Climate Downscaling,"We address the essential role of information retrieval in enhancing climate downscaling, focusing on the need for high-resolution datasets and the application of deep learning models. We explore the requirements for acquiring detailed spatial and temporal climate data, crucial for accurate local forecasts, and discuss how deep learning (DL) techniques can significantly improve downscaling precision by modelling the complex relationships between climate variables. Additionally, we examine the specific challenges related to the retrieval of relevant climatic data, emphasizing methods for efficient data extraction and utilization to support advanced model training. This research underscores an integrated approach, combining information retrieval, deep learning, and climate science to refine the process of climate downscaling, aiming to produce more accurate and actionable local climate projections.",2024-05-23,http://arxiv.org/abs/2405.20346v1,"Declan Curran, Hira Saleem, Flora Salim",arxiv.org,physics.ao-ph
The impact of climate and wealth on energy consumption in small tropical   islands,"Anthropic activities have a significant causal effect on climatic change but climate has also major impact on human societies. Population vulnerability to natural hazards and limited natural resources are deemed problematic, particularly on small tropical islands. Lifestyles and activities are heavily reliant on energy consumption. The relationship between climatic variations and energy consumption must be clearly understood. We demonstrate that it is possible to determine the impact of climate change on energy consumption. In small tropical islands, the relationship between climate and energy consumption is primarily driven by air conditioner electricity consumption during hotter months. Temperatures above 26{\deg}C correlate with increased electricity consumption. Energy consumption is sensitive to: (1) climatic seasonal fluctuations, (2) cyclonic activity, (3) temperature warming over the last 20 years. On small tropical islands, demographic and wealth variations also have a significant impact on energy consumption. The relationship between climate and energy consumption suggests reconsidering the production and consumption of carbon-based energy.",2024-08-22,http://arxiv.org/abs/2408.12438v1,Julien Gargani,arxiv.org,physics.soc-ph
Modeling chaotic Lorenz ODE System using Scientific Machine Learning,"In climate science, models for global warming and weather prediction face significant challenges due to the limited availability of high-quality data and the difficulty in obtaining it, making data efficiency crucial. In the past few years, Scientific Machine Learning (SciML) models have gained tremendous traction as they can be trained in a data-efficient manner, making them highly suitable for real-world climate applications. Despite this, very little attention has been paid to chaotic climate system modeling utilizing SciML methods. In this paper, we have integrated SciML methods into foundational weather models, where we have enhanced large-scale climate predictions with a physics-informed approach that achieves high accuracy with reduced data. We successfully demonstrate that by combining the interpretability of physical climate models with the computational power of neural networks, SciML models can prove to be a reliable tool for modeling climate. This indicates a shift from the traditional black box-based machine learning modeling of climate systems to physics-informed decision-making, leading to effective climate policy implementation.",2024-10-09,http://arxiv.org/abs/2410.06452v1,"Sameera S Kashyap, Raj Abhijit Dandekar, Rajat Dandekar, Sreedath Panat",arxiv.org,"cs.LG, cs.AI"
Dhoroni: Exploring Bengali Climate Change and Environmental Views with a   Multi-Perspective News Dataset and Natural Language Processing,"Climate change poses critical challenges globally, disproportionately affecting low-income countries that often lack resources and linguistic representation on the international stage. Despite Bangladesh's status as one of the most vulnerable nations to climate impacts, research gaps persist in Bengali-language studies related to climate change and NLP. To address this disparity, we introduce Dhoroni, a novel Bengali (Bangla) climate change and environmental news dataset, comprising a 2300 annotated Bangla news articles, offering multiple perspectives such as political influence, scientific/statistical data, authenticity, stance detection, and stakeholder involvement. Furthermore, we present an in-depth exploratory analysis of Dhoroni and introduce BanglaBERT-Dhoroni family, a novel baseline model family for climate and environmental opinion detection in Bangla, fine-tuned on our dataset. This research contributes significantly to enhancing accessibility and analysis of climate discourse in Bengali (Bangla), addressing crucial communication and research gaps in climate-impacted regions like Bangladesh with 180 million people.",2024-10-22,http://arxiv.org/abs/2410.17225v2,"Azmine Toushik Wasi, Wahid Faisal, Taj Ahmad, Abdur Rahman, Mst Rafia Islam",arxiv.org,"cs.CL, cs.CY, cs.LG, stat.AP"
Modelling the Climate Change Debate in Italy through Information Supply   and Demand,"Climate change is one of the most critical challenges of the twenty-first century. Public understanding of climate issues and of the goals regarding the climate transition is essential to translate awareness into concrete actions. Social media platforms play a crucial role in disseminating information about climate change and climate policy. In this context, we propose a model that analyses the Supply and Demand of information to better understand information circulation and information voids within the Italian climate-transition discourse. We conceptualise information supply as the production of content on Facebook and Instagram while leveraging Google searches to capture information demand. Our findings highlight the persistence of information voids, which can hinder informed decision-making and collective action. Furthermore, we observe that the dynamics of information supply and demand on climate-related topics tend to intensify in response to significant external events, shaping public attention and social media discourse.",2025-03-21,http://arxiv.org/abs/2503.17026v1,"Irene Scalco, Giulia Colafrancesco, Matteo Cinelli",arxiv.org,cs.SI
Net-Zero: A Comparative Study on Neural Network Design for   Climate-Economic PDEs Under Uncertainty,"Climate-economic modeling under uncertainty presents significant computational challenges that may limit policymakers' ability to address climate change effectively. This paper explores neural network-based approaches for solving high-dimensional optimal control problems arising from models that incorporate ambiguity aversion in climate mitigation decisions. We develop a continuous-time endogenous-growth economic model that accounts for multiple mitigation pathways, including emission-free capital and carbon intensity reductions. Given the inherent complexity and high dimensionality of these models, traditional numerical methods become computationally intractable. We benchmark several neural network architectures against finite-difference generated solutions, evaluating their ability to capture the dynamic interactions between uncertainty, technology transitions, and optimal climate policy. Our findings demonstrate that appropriate neural architecture selection significantly impacts both solution accuracy and computational efficiency when modeling climate-economic systems under uncertainty. These methodological advances enable more sophisticated modeling of climate policy decisions, allowing for better representation of technology transitions and uncertainty-critical elements for developing effective mitigation strategies in the face of climate change.",2025-05-19,http://arxiv.org/abs/2505.13264v1,"Carlos Rodriguez-Pardo, Louis Daumas, Leonardo Chiani, Massimo Tavoni",arxiv.org,"cs.LG, cs.AI, cs.NE, cs.PF, math.AP, 68T07 (Primary) 35Q91, 91B76 (Secondary), I.2.1; I.5.1; J.4"
"Variable Effects of Climate on Forest Growth in Relation to Climate   Extremes, Disturbance, and Forest Stand Dynamics","Changes in the frequency, duration, and severity of climate extremes are forecast to occur under global climate change. The impacts of climate extremes on forest productivity and health are complicated by potential interactions with disturbance events and stand dynamics. The effects of stand dynamics on forest responses to climate and disturbance are particularly important given forest characteristics driven by stand dynamics can be modified through forest management with the goal of increasing forest resistance and resilience to climate change. We develop a hierarchical Bayesian state-space model allowing climate effects on tree growth to vary over time and in relation to climate extremes, disturbance events, and stand dynamics. We apply the model to a dendrochronology dataset comprising measurements from forest stands of varying composition, structure, and development stage in northeastern Minnesota. Results indicate average forest growth was most sensitive to variables describing climatic water deficit. Forest growth responses to water deficit were partitioned into responses driven by climatic threshold exceedances and interactions with forest tent caterpillar defoliation. Forest growth was both resistant and resilient to climate extremes with the majority of forest growth responses occurring after multiple climatic threshold exceedances or insect defoliation events. Forest growth was most sensitive to water deficit during periods of high stem density following major regeneration events when average inter-tree competition was high. Results suggest that forest growth resistance and resilience to interactions between climate extremes and insect defoliation can be increased through management steps such as thinning to reduce competition during early stages of stand development and small-group selection harvests to maintain forest structures characteristic of older, mature stands.",2016-02-23,http://arxiv.org/abs/1602.07228v2,"Malcolm S. Itter, Andrew O. Finley, Anthony W. D'Amato, Jane R. Foster, John B. Bradford",arxiv.org,"stat.AP, q-bio.PE"
Climate land use and other drivers impacts on island ecosystem services:   a global review,"Islands are diversity hotspots and vulnerable to environmental degradation, climate variations, land use changes and societal crises. These factors can exhibit interactive impacts on ecosystem services. The study reviewed a large number of papers on the climate change-islands-ecosystem services topic worldwide. Potential inclusion of land use changes and other drivers of impacts on ecosystem services were sequentially also recorded. The study sought to investigate the impacts of climate change, land use change, and other non-climatic driver changes on island ecosystem services. Explanatory variables examined were divided into two categories: environmental variables and methodological ones. Environmental variables include sea zone geographic location, ecosystem, ecosystem services, climate, land use, other driver variables, Methodological variables include consideration of policy interventions, uncertainty assessment, cumulative effects of climate change, synergistic effects of climate change with land use change and other anthropogenic and environmental drivers, and the diversity of variables used in the analysis. Machine learning and statistical methods were used to analyze their effects on island ecosystem services. Negative climate change impacts on ecosystem services are better quantified by land use change or other non-climatic driver variables than by climate variables. The synergy of land use together with climate changes is modulating the impact outcome and critical for a better impact assessment. Analyzed together, there is little evidence of more pronounced for a specific sea zone, ecosystem, or ecosystem service. Climate change impacts may be underestimated due to the use of a single climate variable deployed in most studies. Policy interventions exhibit low classification accuracy in quantifying impacts indicating insufficient efficacy or integration in the studies.",2025-03-13,http://arxiv.org/abs/2503.10278v1,"Aristides Moustakas, Shiri Zemah-Shamir, Mirela Tase, Savvas Zotos, Nazli Demirel, Christos Zoumides, Irene Christoforidi, Turgay Dindaroglu, Tamer Albayrak, Cigdem Kaptan Ayhan, Mauro Fois, Paraskevi Manolaki, Attila D. Sandor, Ina Sieber, Valentini Stamatiadou, Elli Tzirkalli, Ioannis N. Vogiatzakis, Ziv Zemah-Shamir, George Zittis",arxiv.org,"q-bio.PE, cs.LG, q-bio.QM"
Reinforcement Learning (RL) Meets Urban Climate Modeling: Investigating   the Efficacy and Impacts of RL-Based HVAC Control,"Reinforcement learning (RL)-based heating, ventilation, and air conditioning (HVAC) control has emerged as a promising technology for reducing building energy consumption while maintaining indoor thermal comfort. However, the efficacy of such strategies is influenced by the background climate and their implementation may potentially alter both the indoor climate and local urban climate. This study proposes an integrated framework combining RL with an urban climate model that incorporates a building energy model, aiming to evaluate the efficacy of RL-based HVAC control across different background climates, impacts of RL strategies on indoor climate and local urban climate, and the transferability of RL strategies across cities. Our findings reveal that the reward (defined as a weighted combination of energy consumption and thermal comfort) and the impacts of RL strategies on indoor climate and local urban climate exhibit marked variability across cities with different background climates. The sensitivity of reward weights and the transferability of RL strategies are also strongly influenced by the background climate. Cities in hot climates tend to achieve higher rewards across most reward weight configurations that balance energy consumption and thermal comfort, and those cities with more varying atmospheric temperatures demonstrate greater RL strategy transferability. These findings underscore the importance of thoroughly evaluating RL-based HVAC control strategies in diverse climatic contexts. This study also provides a new insight that city-to-city learning will potentially aid the deployment of RL-based HVAC control.",2025-05-11,http://arxiv.org/abs/2505.07045v1,"Junjie Yu, John S. Schreck, David John Gagne, Keith W. Oleson, Jie Li, Yongtu Liang, Qi Liao, Mingfei Sun, David O. Topping, Zhonghua Zheng",arxiv.org,"cs.LG, cs.AI, physics.ao-ph"
Influence of autocorrelation on the topology of the climate network,"Different definitions of links in climate networks may lead to considerably different network topologies. We construct a network from climate records of surface level atmospheric temperature in different geographical sites around the globe using two commonly used definitions of links. Utilizing detrended fluctuation analysis, shuffled surrogates and separation analysis of maritime and continental records, we find that one of the major influences on the structure of climate networks is due to the auto-correlation in the records, that may introduce spurious links. This may explain why different methods could lead to different climate network topologies.",2014-07-23,http://arxiv.org/abs/1407.6243v1,"Oded C. Guez, Avi Gozolchiani, Shlomo Havlin",arxiv.org,"physics.ao-ph, physics.data-an, physics.soc-ph"
Limits on CO2 Climate Forcing from Recent Temperature Data of Earth,"The global atmospheric temperature anomalies of Earth reached a maximum in 1998 which has not been exceeded during the subsequent 10 years. The global anomalies are calculated from the average of climate effects occurring in the tropical and the extratropical latitude bands. El Nino/La Nina effects in the tropical band are shown to explain the 1998 maximum while variations in the background of the global anomalies largely come from climate effects in the northern extratropics. These effects do not have the signature associated with CO2 climate forcing. However, the data show a small underlying positive trend that is consistent with CO2 climate forcing with no-feedback.",2008-09-03,http://arxiv.org/abs/0809.0581v1,"David H. Douglass, John R. Christy",arxiv.org,physics.geo-ph
Progress and Prospects in Weather and Climate Modelling,"This popular article provides a short summary of the progress and prospects in Weather and Climate Modelling for the benefit of high school and undergraduate college students and early career researchers. Although this is not a comprehensive scientific article, the basic information provided here is intended to introduce students and researchers to the topic of Weather and Climate Modelling - which comes under the broad discipline of Atmospheric / Oceanic / Climate / Earth Sciences. This article briefly summarizes the historical developments, progress, scientific challenges in weather and climate modelling and career opportunities.",2020-11-23,http://arxiv.org/abs/2011.11353v1,"R Krishnan, Manmeet Singh, Ramesh Vellore, Milind Mujumdar",arxiv.org,"physics.ao-ph, physics.data-an"
Perspectives on the importance of complex systems in understanding   ourclimate and climate change -- The Nobel Prize in Physics 2021,"The Nobel Prize in Physics 2021 was awarded to Syukuro Manabe, Klaus Hasselmann, and Giorgio Parisi for their 'groundbreaking contributions to our understanding of complex systems' including major advances in the understanding of our climate and climate change. In this perspective article, we review their key contributions and discuss their relevance in relation to the present understanding of our climate. We conclude by outlining some promising research directions and open questions in climate science.",2022-03-07,http://arxiv.org/abs/2203.03331v2,"Shraddha Gupta, Nikolaos Mastrantonas, Cristina Masoller, Jürgen Kurths",arxiv.org,physics.ao-ph
"Climate, Agriculture and Food",Agriculture is arguably the most climate-sensitive sector of the economy. Growing concerns about anthropogenic climate change have increased research interest in assessing its potential impact on the sector and in identifying policies and adaptation strategies to help the sector cope with a changing climate. This chapter provides an overview of recent advancements in the analysis of climate change impacts and adaptation in agriculture with an emphasis on methods. The chapter provides an overview of recent research efforts addressing key conceptual and empirical challenges. The chapter also discusses practical matters about conducting research in this area and provides reproducible R code to perform common tasks of data preparation and model estimation in this literature. The chapter provides a hands-on introduction to new researchers in this area.,2021-05-25,http://arxiv.org/abs/2105.12044v1,Ariel Ortiz-Bobea,arxiv.org,"econ.GN, q-fin.EC"
Explore the possibility of advancing climate negotiations on the basis   of regional trade organizations: A study based on RICE-N,"Climate issues have become more and more important now. Although global governments have made some progress, we are still facing the truth that the prospect of international cooperation is not clear at present. Due to the limitations of the Integrated assessment models (IAMs) model, it is difficult to simulate the dynamic negotiation process. Therefore, using deep learning to build a new agents based model (ABM) might can provide new theoretical support for climate negotiations. Building on the RICE-N model, this work proposed an approach to climate negotiations based on existing trade groups. Simulation results show that the scheme has a good prospect.",2023-07-26,http://arxiv.org/abs/2307.14226v1,Wubo Dai,arxiv.org,"cs.CY, cs.AI"
AI for Anticipatory Action: Moving Beyond Climate Forecasting,"Disaster response agencies have been shifting from a paradigm of climate forecasting towards one of anticipatory action: assessing not just what the climate will be, but how it will impact specific populations, thereby enabling proactive response and resource allocation. Machine learning models are becoming exceptionally powerful at climate forecasting, but methodological gaps remain in terms of facilitating anticipatory action. Here we provide an overview of anticipatory action, review relevant applications of machine learning, identify common challenges, and highlight areas where machine learning can uniquely contribute to advancing disaster response for populations most vulnerable to climate change.",2023-07-28,http://arxiv.org/abs/2307.15727v1,"Benjamin Q. Huynh, Mathew V. Kiang",arxiv.org,"cs.LG, cs.AI, stat.AP"
Climate Variable Downscaling with Conditional Normalizing Flows,"Predictions of global climate models typically operate on coarse spatial scales due to the large computational costs of climate simulations. This has led to a considerable interest in methods for statistical downscaling, a similar process to super-resolution in the computer vision context, to provide more local and regional climate information. In this work, we apply conditional normalizing flows to the task of climate variable downscaling. We showcase its successful performance on an ERA5 water content dataset for different upsampling factors. Additionally, we show that the method allows us to assess the predictive uncertainty in terms of standard deviation from the fitted conditional distribution mean.",2024-05-31,http://arxiv.org/abs/2405.20719v1,"Christina Winkler, Paula Harder, David Rolnick",arxiv.org,"cs.AI, cs.CV, physics.ao-ph"
A Run on Fossil Fuel? Climate Change and Transition Risk,"I study the dynamic, general equilibrium implications of climate-change-linked transition risk on macroeconomic outcomes and asset prices. Climate-change-linked expectations of fossil fuel restrictions can produce a ``run on fossil fuels'' with accelerated production and decreasing spot prices, or a ``reverse run'' with restrained production and increased spot prices. The response depends on the expected economic consequences of the anticipated transition shock, and existing climate policies. Fossil fuel firm prices decrease in each case. I use a novel empirical measure of innovations in climate-related transition risk likelihood to show that dynamic empirical responses are consistent with a ``run on fossil fuel.''",2024-10-01,http://arxiv.org/abs/2410.00902v1,Michael Barnett,arxiv.org,"q-fin.GN, econ.GN, q-fin.EC"
"The Reality of Climate Change: Evidence, Impacts and Engineering   Solutions","Climate change is one of the most significant global challenges, yet misconceptions persist regarding its causes and impact. This report addresses common myths surrounding climate change and presents scientific evidence to clarify its reality. Utilising data from NASA, NOAA, and the NSW government, this study provides evidence of rising global temperatures, melting ice sheets, rising sea levels, and extreme weather patterns in regions like New South Wales. The analysis demonstrates the human-driven nature of climate change, primarily caused by increased carbon emissions. Engineering solutions, including renewable energy technologies, green buildings, and carbon capture methods, are essential to mitigating the effects of climate change. Future research should focus on improving the scalability of these technologies and addressing the broader impact on ecosystems and human societies.",2024-10-16,http://arxiv.org/abs/2410.12412v1,Sihua Lu,arxiv.org,physics.ao-ph
From Correlation to Causation: Understanding Climate Change through   Causal Analysis and LLM Interpretations,"This research presents a three-step causal inference framework that integrates correlation analysis, machine learning-based causality discovery, and LLM-driven interpretations to identify socioeconomic factors influencing carbon emissions and contributing to climate change. The approach begins with identifying correlations, progresses to causal analysis, and enhances decision making through LLM-generated inquiries about the context of climate change. The proposed framework offers adaptable solutions that support data-driven policy-making and strategic decision-making in climate-related contexts, uncovering causal relationships within the climate change domain.",2024-12-21,http://arxiv.org/abs/2412.16691v1,Shan Shan,arxiv.org,"cs.LG, cs.CY, stat.ME, stat.ML"
Impact of Climate Change on Forests in India,"Global assessments have shown that future climate change is likely to significantly impact forest ecosystems. The present study makes an assessment of the impact of projected climate change on forest ecosystems in India. This assessment is based on climate projections of Regional Climate Model of the Hadley Centre (HadRM3) using the A2 (740 ppm CO2) and B2 (575 ppm CO2) scenarios of Special Report on Emissions Scenarios and the BIOME4 vegetation response model. The main conclusion is that under the climate projection for the year 2085, 77% and 68% of the forested grids in India are likely to experience shift in forest types under A2 and B2 scenario, respectively. Indications are a shift towards wetter forest types in the northeastern region and drier forest types in the northwestern region in the absence of human influence. Increasing atmospheric CO2 concentration and climate warming could also result in a doubling of net primary productivity under the A2 scenario and nearly 70% increase under the B2 scenario. The trends of impacts could be considered as robust but the magnitudes should be viewed with caution, due to the uncertainty in climate projections. Given the projected trends of likely impacts of climate change on forest ecosystems, it is important to incorporate climate change consideration in forest sector long-term planning process.",2005-11-02,http://arxiv.org/abs/q-bio/0511001v1,"N. H. Ravindranath, N. V. Joshi, R. Sukumar, A. Saxena",arxiv.org,q-bio.OT
Cosmic Rays and Climate,"Among the most puzzling questions in climate change is that of solar-climate variability, which has attracted the attention of scientists for more than two centuries. Until recently, even the existence of solar-climate variability has been controversial - perhaps because the observations had largely involved temporary correlations between climate and the sunspot cycle. Over the last few years, however, diverse reconstructions of past climate change have revealed clear associations with cosmic ray variations recorded in cosmogenic isotope archives, providing persuasive evidence for solar or cosmic ray forcing of the climate. However, despite the increasing evidence of its importance, solar climate variability is likely to remain controversial until a physical mechanism is established. Although this remains a mystery, observations suggest that cloud cover may be influenced by cosmic rays, which are modulated by the solar wind and, on longer time scales, by the geomagnetic field and by the galactic environment of Earth. Two different classes of microphysical mechanisms have been proposed to connect cosmic rays with clouds: firstly, an influence of cosmic rays on the production of cloud condensation nuclei and, secondly, an influence of cosmic rays on the global electrical circuit in the atmosphere and, in turn, on ice nucleation and other cloud microphysical processes. Considerable progress on understanding ion-aerosol-cloud processes has been made in recent years, and the results are suggestive of a physically- plausible link between cosmic rays, clouds and climate. However, a concerted effort is now required to carry out definitive laboratory measurements of the fundamental physical and chemical processes involved, and to evaluate their climatic significance with dedicated field observations and modelling studies.",2008-04-11,http://arxiv.org/abs/0804.1938v1,Jasper Kirkby,arxiv.org,physics.ao-ph
Another Look at Climate Sensitivity,"We revisit a recent claim that the Earth's climate system is characterized by sensitive dependence to parameters; in particular, that the system exhibits an asymmetric, large-amplitude response to normally distributed feedback forcing. Such a response would imply irreducible uncertainty in climate change predictions and thus have notable implications for climate science and climate-related policy making. We show that equilibrium climate sensitivity in all generality does not support such an intrinsic indeterminacy; the latter appears only in essentially linear systems. The main flaw in the analysis that led to this claim is inappropriate linearization of an intrinsically nonlinear model; there is no room for physical interpretations or policy conclusions based on this mathematical error. Sensitive dependence nonetheless does exist in the climate system, as well as in climate models -- albeit in a very different sense from the one claimed in the linear work under scrutiny -- and we illustrate it using a classical energy balance model (EBM) with nonlinear feedbacks. EBMs exhibit two saddle-node bifurcations, more recently called ""tipping points"", which give rise to three distinct steady-state climates, two of which are stable. Such bistable behavior is, furthermore, supported by results from more realistic, nonequilibrium climate models. In a truly nonlinear setting, indeterminacy in the size of the response is observed only in the vicinity of tipping points. We show, in fact, that small disturbances cannot result in a large-amplitude response, unless the system is at or near such a point. We discuss briefly how the distance to the bifurcation may be related to the strength of Earth's ice-albedo feedback.",2010-03-01,http://arxiv.org/abs/1003.0253v1,"Ilya Zaliapin, Michael Ghil",arxiv.org,"physics.ao-ph, physics.geo-ph"
Stochastic Climate Theory and Modelling,"Stochastic methods are a crucial area in contemporary climate research and are increasingly being used in comprehensive weather and climate prediction models as well as reduced order climate models. Stochastic methods are used as subgrid-scale parameterizations as well as for model error representation, uncertainty quantification, data assimilation and ensemble prediction. The need to use stochastic approaches in weather and climate models arises because we still cannot resolve all necessary processes and scales in comprehensive numerical weather and climate prediction models. In many practical applications one is mainly interested in the largest and potentially predictable scales and not necessarily in the small and fast scales. For instance, reduced order models can simulate and predict large scale modes. Statistical mechanics and dynamical systems theory suggest that in reduced order models the impact of unresolved degrees of freedom can be represented by suitable combinations of deterministic and stochastic components and non-Markovian (memory) terms. Stochastic approaches in numerical weather and climate prediction models also lead to the reduction of model biases. Hence, there is a clear need for systematic stochastic approaches in weather and climate modelling. In this review we present evidence for stochastic effects in laboratory experiments. Then we provide an overview of stochastic climate theory from an applied mathematics perspectives. We also survey the current use of stochastic methods in comprehensive weather and climate prediction models and show that stochastic parameterizations have the potential to remedy many of the current biases in these comprehensive models.",2014-09-01,http://arxiv.org/abs/1409.0423v1,"Christian L. E. Franzke, Terence J. O'Kane, Judith Berner, Paul D. Williams, Valerio Lucarini",arxiv.org,"physics.ao-ph, physics.data-an"
On constraining projections of future climate using observations and   simulations from multiple climate models,"Numerical climate models are used to project future climate change due to both anthropogenic and natural causes. Differences between projections from different climate models are a major source of uncertainty about future climate. Emergent relationships shared by multiple climate models have the potential to constrain our uncertainty when combined with historical observations. We combine projections from 13 climate models with observational data to quantify the impact of emergent relationships on projections of future warming in the Arctic at the end of the 21st century. We propose a hierarchical Bayesian framework based on a coexchangeable representation of the relationship between climate models and the Earth system. We show how emergent constraints fit into the coexchangeable representation, and extend it to account for internal variability simulated by the models and natural variability in the Earth system. Our analysis shows that projected warming in some regions of the Arctic may be more than 2C lower and our uncertainty reduced by up to 30% when constrained by historical observations. A detailed theoretical comparison with existing multi-model projection frameworks is also provided. In particular, we show that projections may be biased if we do not account for internal variability in climate model predictions.",2017-11-11,http://arxiv.org/abs/1711.04139v4,"Philip G. Sansom, David B. Stephenson, Thomas J. Bracegirdle",arxiv.org,"stat.AP, physics.ao-ph"
Weathering Adaptation: Grid Infrastructure Planning in a Changing   Climate,"Decisions related to electric power systems planning and operations rely on assumptions and insights informed by historic weather data and records of past performance. Evolving climate trends are, however, changing the energy use patterns and operating conditions of grid assets, thus altering the nature and severity of risks the system faces. Because grid assets remain in operation for decades, planning for evolving risks will require incorporating climate projections into grid infrastructure planning processes. The current work traces a pathway for climate-aware decision-making in the electricity sector. We evaluate the suitability of using existing climate models and data for electricity planning and discuss their limitations. We review the interactions between grid infrastructure and climate by synthesizing what is known about how changing environmental operating conditions would impact infrastructure utilization, constraints, and performance. We contextualize our findings by presenting a case study of California, examining if and where climate data can be integrated into infrastructure planning processes. The core contribution of the work is a series of nine recommendations detailing advancements in climate projections, grid modeling architecture, and disaster preparedness that would be needed to ensure that infrastructure planning decisions are robust to uncertainty and risks associated with evolving climate conditions.",2019-12-05,http://arxiv.org/abs/1912.02920v1,"Anna M. Brockway, Laurel N. Dunn",arxiv.org,"physics.soc-ph, cs.SY, eess.SY, physics.ao-ph"
Spatial Projection of Multiple Climate Variables using Hierarchical   Multitask Learning,"Future projection of climate is typically obtained by combining outputs from multiple Earth System Models (ESMs) for several climate variables such as temperature and precipitation. While IPCC has traditionally used a simple model output average, recent work has illustrated potential advantages of using a multitask learning (MTL) framework for projections of individual climate variables. In this paper we introduce a framework for hierarchical multitask learning (HMTL) with two levels of tasks such that each super-task, i.e., task at the top level, is itself a multitask learning problem over sub-tasks. For climate projections, each super-task focuses on projections of specific climate variables spatially using an MTL formulation. For the proposed HMTL approach, a group lasso regularization is added to couple parameters across the super-tasks, which in the climate context helps exploit relationships among the behavior of different climate variables at a given spatial location. We show that some recent works on MTL based on learning task dependency structures can be viewed as special cases of HMTL. Experiments on synthetic and real climate data show that HMTL produces better results than decoupled MTL methods applied separately on the super-tasks and HMTL significantly outperforms baselines for climate projection.",2017-01-30,http://arxiv.org/abs/1701.08840v1,"André R. Gonçalves, Arindam Banerjee, Fernando J. Von Zuben",arxiv.org,"cs.LG, stat.ML"
Climate change sentiment on Twitter: An unsolicited public opinion poll,"The consequences of anthropogenic climate change are extensively debated through scientific papers, newspaper articles, and blogs. Newspaper articles may lack accuracy, while the severity of findings in scientific papers may be too opaque for the public to understand. Social media, however, is a forum where individuals of diverse backgrounds can share their thoughts and opinions. As consumption shifts from old media to new, Twitter has become a valuable resource for analyzing current events and headline news. In this research, we analyze tweets containing the word ""climate"" collected between September 2008 and July 2014. Through use of a previously developed sentiment measurement tool called the Hedonometer, we determine how collective sentiment varies in response to climate change news, events, and natural disasters. We find that natural disasters, climate bills, and oil-drilling can contribute to a decrease in happiness while climate rallies, a book release, and a green ideas contest can contribute to an increase in happiness. Words uncovered by our analysis suggest that responses to climate change news are predominately from climate change activists rather than climate change deniers, indicating that Twitter is a valuable resource for the spread of climate change awareness.",2015-05-14,http://arxiv.org/abs/1505.03804v2,"Emily M. Cody, Andrew J. Reagan, Lewis Mitchell, Peter Sheridan Dodds, Christopher M. Danforth",arxiv.org,"physics.soc-ph, cs.CY, cs.SI"
The Role of Plate Tectonic-Climate Coupling and Exposed Land Area in the   Development of Habitable Climates on Rocky Planets,"The long-term carbon cycle is vital for maintaining liquid water oceans on rocky planets due to the negative climate feedbacks involved in silicate weathering. Plate tectonics plays a crucial role in driving the long-term carbon cycle because it is responsible for CO$_2$ degassing at ridges and arcs, the return of CO$_2$ to the mantle through subduction, and supplying fresh, weatherable rock to the surface via uplift and orogeny. However, the presence of plate tectonics itself may depend on climate according to recent geodynamical studies showing that cool surface temperatures are important for maintaining vigorous plate tectonics. Using a simple carbon cycle model, I show that the negative climate feedbacks inherent in the long-term carbon cycle are uninhibited by climate's effect on plate tectonics. Furthermore, initial atmospheric CO$_2$ conditions do not impact the final climate state reached when the carbon cycle comes to equilibrium, as long as liquid water is present and silicate weathering can occur. Thus an initially hot, CO$_2$ rich atmosphere does not prevent the development of a temperate climate and plate tectonics on a planet. However, globally supply-limited weathering does prevent the development of temperate climates on planets with small subaerial land areas and large total CO$_2$ budgets because supply-limited weathering lacks stabilizing climate feedbacks. Planets in the supply-limited regime may become inhospitable for life and could experience significant water loss. Supply-limited weathering is less likely on plate tectonic planets, because plate tectonics promotes high erosion rates and thus a greater supply of bedrock to the surface.",2015-09-01,http://arxiv.org/abs/1509.00427v1,Bradford J. Foley,arxiv.org,astro-ph.EP
Geologic Constraints on Early Mars Climate,"Early Mars climate research has well-defined goals (Mars Exploration Program Analysis Group 2018). Achieving these goals requires geologists and climate modelers to coordinate. Coordination is easier if results are expressed in terms of well-defined parameters. Key parameters include the following quantitative geologic constraints. (1) Cumulative post-3.4 Ga precipitation-sourced water runoff in some places exceeded 1 km column. (2) There is no single Early Mars climate problem: the traces of $\ge$2 river-forming periods are seen. Relative to rivers that formed earlier in Mars history, rivers that formed later in Mars history are found preferentially at lower elevations, and show a stronger dependence on latitude. (3) The duration of the longest individual river-forming climate was >(10$^2$-10$^3$) yr, based on paleolake hydrology. (4) Peak runoff production was >0.1 mm/hr. However, (5) peak runoff production was intermittent, sustained (in a given catchment) for only <10% of the duration of river-forming climates. (6) The cumulative number of wet years during the valley-network-forming period was >10$^5$ yr. (7) Post-Noachian light-toned, layered sedimentary rocks took >10$^7$ yr to accumulate. However, (8) an ""average"" place on Mars saw water for <10$^7$ yr after the Noachian, suggesting that the river-forming climates were interspersed with long globally-dry intervals. (9) Geologic proxies for Early Mars atmospheric pressure indicate pressure was not less than 0.012 bar but not much more than 1 bar. A truth table of these geologic constraints versus currently published climate models shows that the late persistence of river-forming climates, combined with the long duration of individual lake forming climates, is a challenge for most models.",2018-12-31,http://arxiv.org/abs/1812.11722v1,Edwin S. Kite,arxiv.org,astro-ph.EP
Climate Policy Tracker: Pipeline for automated analysis of public   climate policies,"The number of standardized policy documents regarding climate policy and their publication frequency is significantly increasing. The documents are long and tedious for manual analysis, especially for policy experts, lawmakers, and citizens who lack access or domain expertise to utilize data analytics tools. Potential consequences of such a situation include reduced citizen governance and involvement in climate policies and an overall surge in analytics costs, rendering less accessibility for the public. In this work, we use a Latent Dirichlet Allocation-based pipeline for the automatic summarization and analysis of 10-years of national energy and climate plans (NECPs) for the period from 2021 to 2030, established by 27 Member States of the European Union. We focus on analyzing policy framing, the language used to describe specific issues, to detect essential nuances in the way governments frame their climate policies and achieve climate goals. The methods leverage topic modeling and clustering for the comparative analysis of policy documents across different countries. It allows for easier integration in potential user-friendly applications for the development of theories and processes of climate policy. This would further lead to better citizen governance and engagement over climate policies and public policy research.",2022-11-10,http://arxiv.org/abs/2211.05852v1,"Artur Żółkowski, Mateusz Krzyziński, Piotr Wilczyński, Stanisław Giziński, Emilia Wiśnios, Bartosz Pieliński, Julian Sienkiewicz, Przemysław Biecek",arxiv.org,cs.CL
Multi-scale Digital Twin: Developing a fast and physics-informed   surrogate model for groundwater contamination with uncertain climate models,"Soil and groundwater contamination is a pervasive problem at thousands of locations across the world. Contaminated sites often require decades to remediate or to monitor natural attenuation. Climate change exacerbates the long-term site management problem because extreme precipitation and/or shifts in precipitation/evapotranspiration regimes could re-mobilize contaminants and proliferate affected groundwater. To quickly assess the spatiotemporal variations of groundwater contamination under uncertain climate disturbances, we developed a physics-informed machine learning surrogate model using U-Net enhanced Fourier Neural Operator (U-FNO) to solve Partial Differential Equations (PDEs) of groundwater flow and transport simulations at the site scale.We develop a combined loss function that includes both data-driven factors and physical boundary constraints at multiple spatiotemporal scales. Our U-FNOs can reliably predict the spatiotemporal variations of groundwater flow and contaminant transport properties from 1954 to 2100 with realistic climate projections. In parallel, we develop a convolutional autoencoder combined with online clustering to reduce the dimensionality of the vast historical and projected climate data by quantifying climatic region similarities across the United States. The ML-based unique climate clusters provide climate projections for the surrogate modeling and help return reliable future recharge rate projections immediately without querying large climate datasets. In all, this Multi-scale Digital Twin work can advance the field of environmental remediation under climate change.",2022-11-20,http://arxiv.org/abs/2211.10884v1,"Lijing Wang, Takuya Kurihana, Aurelien Meray, Ilijana Mastilovic, Satyarth Praveen, Zexuan Xu, Milad Memarzadeh, Alexander Lavin, Haruko Wainwright",arxiv.org,"physics.geo-ph, cs.AI"
A population facing climate change: joint influences of Allee effects   and environmental boundary geometry,"As a result of climate change, many populations have to modify their range to follow the suitable areas - their ""climate envelope"" - often risking extinction. During this migration process, they may face absolute boundaries to dispersal, because of external environmental factors. Consequently, not only the position, but also the shape of the climate envelope can be modified. We use a reaction-diffusion model to analyse the effects on population persistence of simultaneous changes in the climate envelope position and shape. When the growth term is of logistic type, we show that extinction and persistence are principally conditioned by the species mobility and the speed of climate change, but not by the shape of the climate envelope. However, with a growth term taking an Allee effect into account, we find a high sensitivity to the variations of the shape of the climate envelope. In this case, the species which have a high mobility, although they could more easily follow the migration of the climate envelope, would be at risk of extinction when encountering a local narrowing of the boundary geometry. This effect can be attenuated by a progressive opening of the available space at the exit of the narrowing, even though it transiently leads to a diminished area of the climate envelope.",2009-07-06,http://arxiv.org/abs/0907.0989v1,"Lionel Roques, Alain Roques, Henri Berestycki, André Kretzschmar",arxiv.org,"math.AP, q-bio.PE"
Toward an Effective Pedagogy of Climate Change: Lessons From a Physics   Classroom,"A major roadblock to effective climate education is the lack of radical visions (Kwauk, 2020) for engaging with climate change and related social-environmental crises in the classroom. Key aspects of the climate system: its inherent complexity and transdisciplinarity, the entanglement of social and natural systems, and the fact that it spans large scales of space and time - all present serious challenges to our modern, compartmentalized formal education systems. I present an argument for a justice-centered, inter-to-transdisciplinary conceptualization of climate change at the nexus of science, society and justice, which is designed to engage with the very features of the climate system that make it challenging. Based on pedagogical experiments in an undergraduate physics classroom for non-science majors, I identify five general barriers to teaching climate change, and formulate four dimensions of an effective climate pedagogy: the scientific-technological, the transdisciplinary, the onto-epistemological, and the psychosocial action dimensions. Within the context of a collaborative classroom culture informed by transformational learning, I describe the use of a meta-conceptual framework that, along with the issue of climate justice, seeks to realize all four dimensions of an effective climate pedagogy. This broad holistic framework is potentially adaptable to contexts and disciplines beyond undergraduate physics.",2020-08-01,http://arxiv.org/abs/2008.00281v2,Vandana Singh,arxiv.org,physics.ed-ph
How are cities pledging net zero? A computational approach to analyzing   subnational climate strategies,"Cities have become primary actors on climate change and are increasingly setting goals aimed at net-zero emissions. The rapid proliferation of subnational governments ""racing to zero"" emissions and articulating their own climate mitigation plans warrants closer examination to understand how these actors intend to meet these goals. The scattered, incomplete and heterogeneous nature of city climate policy documents, however, has made their systemic analysis challenging. We analyze 318 climate action documents from cities that have pledged net-zero targets or joined a transnational climate initiative with this goal using machine learning-based natural language processing (NLP) techniques. We use these approaches to accomplish two primary goals: 1) determine text patterns that predict ""ambitious"" net-zero targets, where we define an ambitious target as one that encompasses a subnational government's economy-wide emissions; and 2) perform a sectoral analysis to identify patterns and trade-offs in climate action themes (i.e., land-use, industry, buildings, etc.). We find that cities that have defined ambitious climate actions tend to emphasize quantitative metrics and specific high-emitting sectors in their plans, supported by mentions of governance and citizen participation. Cities predominantly emphasize energy-related actions in their plans, particularly in the buildings, transport and heating sectors, but often at the expense of other sectors, including land-use and climate impacts. The method presented in this paper provides a replicable, scalable approach to analyzing climate action plans and a first step towards facilitating cross-city learning.",2021-12-14,http://arxiv.org/abs/2112.11207v1,"Siddharth Sachdeva, Angel Hsu, Ian French, Elwin Lim",arxiv.org,"cs.CY, cs.CL, cs.LG, stat.AP"
Minimax-Regret Climate Policy with Deep Uncertainty in Climate Modeling   and Intergenerational Discounting,"Integrated assessment models have become the primary tools for comparing climate policies that seek to reduce greenhouse gas emissions. Policy comparisons have often been performed by considering a planner who seeks to make optimal trade-offs between the costs of carbon abatement and the economic damages from climate change. The planning problem has been formalized as one of optimal control, the objective being to minimize the total costs of abatement and damages over a time horizon. Studying climate policy as a control problem presumes that a planner knows enough to make optimization feasible, but physical and economic uncertainties abound. Earlier, Manski, Sanstad, and DeCanio proposed and studied use of the minimax-regret (MMR) decision criterion to account for deep uncertainty in climate modeling. Here we study choice of climate policy that minimizes maximum regret with deep uncertainty regarding both the correct climate model and the appropriate time discount rate to use in intergenerational assessment of policy consequences. The analysis specifies a range of discount rates to express both empirical and normative uncertainty about the appropriate rate. The findings regarding climate policy are novel and informative. The MMR analysis points to use of a relatively low discount rate of 0.02 for climate policy. The MMR decision rule keeps the maximum future temperature increase below 2C above the 1900-10 level for most of the parameter values used to weight costs and damages.",2022-01-21,http://arxiv.org/abs/2201.08826v1,"Stephen J. DeCanio, Charles F. Manski, Alan H. Sanstad",arxiv.org,"econ.EM, physics.soc-ph"
AI For Global Climate Cooperation 2023 Competition Proceedings,"The international community must collaborate to mitigate climate change and sustain economic growth. However, collaboration is hard to achieve, partly because no global authority can ensure compliance with international climate agreements. Combining AI with climate-economic simulations offers a promising solution to design international frameworks, including negotiation protocols and climate agreements, that promote and incentivize collaboration. In addition, these frameworks should also have policy goals fulfillment, and sustained commitment, taking into account climate-economic dynamics and strategic behaviors. These challenges require an interdisciplinary approach across machine learning, economics, climate science, law, policy, ethics, and other fields.   Towards this objective, we organized AI for Global Climate Cooperation, a Mila competition in which teams submitted proposals and analyses of international frameworks, based on (modifications of) RICE-N, an AI-driven integrated assessment model (IAM). In particular, RICE-N supports modeling regional decision-making using AI agents. Furthermore, the IAM then models the climate-economic impact of those decisions into the future.   Whereas the first track focused only on performance metrics, the proposals submitted to the second track were evaluated both quantitatively and qualitatively. The quantitative evaluation focused on a combination of (i) the degree of mitigation of global temperature rise and (ii) the increase in economic productivity. On the other hand, an interdisciplinary panel of human experts in law, policy, sociology, economics and environmental science, evaluated the solutions qualitatively. In particular, the panel considered the effectiveness, simplicity, feasibility, ethics, and notions of climate justice of the protocols. In the third track, the participants were asked to critique and improve RICE-N.",2023-07-10,http://arxiv.org/abs/2307.06951v1,"Yoshua Bengio, Prateek Gupta, Lu Li, Soham Phade, Sunil Srinivasa, Andrew Williams, Tianyu Zhang, Yang Zhang, Stephan Zheng",arxiv.org,"cs.AI, cs.LG"
ClimateNLP: Analyzing Public Sentiment Towards Climate Change Using   Natural Language Processing,"Climate change's impact on human health poses unprecedented and diverse challenges. Unless proactive measures based on solid evidence are implemented, these threats will likely escalate and continue to endanger human well-being. The escalating advancements in information and communication technologies have facilitated the widespread availability and utilization of social media platforms. Individuals utilize platforms such as Twitter and Facebook to express their opinions, thoughts, and critiques on diverse subjects, encompassing the pressing issue of climate change. The proliferation of climate change-related content on social media necessitates comprehensive analysis to glean meaningful insights. This paper employs natural language processing (NLP) techniques to analyze climate change discourse and quantify the sentiment of climate change-related tweets. We use ClimateBERT, a pretrained model fine-tuned specifically for the climate change domain. The objective is to discern the sentiment individuals express and uncover patterns in public opinion concerning climate change. Analyzing tweet sentiments allows a deeper comprehension of public perceptions, concerns, and emotions about this critical global challenge. The findings from this experiment unearth valuable insights into public sentiment and the entities associated with climate change discourse. Policymakers, researchers, and organizations can leverage such analyses to understand public perceptions, identify influential actors, and devise informed strategies to address climate change challenges.",2023-10-12,http://arxiv.org/abs/2310.08099v2,"Ajay Krishnan, V. S. Anoop",arxiv.org,cs.CL
Regional impacts poorly constrained by climate sensitivity,"Climate risk assessments must account for a wide range of possible futures, so scientists often use simulations made by numerous global climate models to explore potential changes in regional climates and their impacts. Some of the latest-generation models have high effective climate sensitivities or EffCS. It has been argued these so-called hot models are unrealistic and should therefore be excluded from analyses of climate change impacts. Whether this would improve regional impact assessments, or make them worse, is unclear. Here we show there is no universal relationship between EffCS and projected changes in a number of important climatic drivers of regional impacts. Analysing heavy rainfall events, meteorological drought, and fire weather in different regions, we find little or no significant correlation with EffCS for most regions and climatic drivers. Even when a correlation is found, internal variability and processes unrelated to EffCS have similar effects on projected changes in the climatic drivers as EffCS. Model selection based solely on EffCS appears to be unjustified and may neglect realistic impacts, leading to an underestimation of climate risks.",2024-04-18,http://arxiv.org/abs/2404.11939v1,"Ranjini Swaminathan, Jacob Schewe, Jeremy Walton, Klaus Zimmermann, Colin Jones, Richard A. Betts, Chantelle Burton, Chris D. Jones, Matthias Mengel, Christopher P. O. Reyer, Andrew G. Turner, Katja Weigel",arxiv.org,physics.ao-ph
ClimDetect: A Benchmark Dataset for Climate Change Detection and   Attribution,"Detecting and attributing temperature increases driven by climate change is crucial for understanding global warming and informing adaptation strategies. However, distinguishing human-induced climate signals from natural variability remains challenging for traditional detection and attribution (D&A) methods, which rely on identifying specific ""fingerprints"" -- spatial patterns expected to emerge from external forcings such as greenhouse gas emissions. Deep learning offers promise in discerning these complex patterns within expansive spatial datasets, yet the lack of standardized protocols has hindered consistent comparisons across studies.   To address this gap, we introduce ClimDetect, a standardized dataset comprising 1.17M daily climate snapshots paired with target climate change indicator variables. The dataset is curated from both CMIP6 climate model simulations and real-world observation-assimilated reanalysis datasets (ERA5, JRA-3Q, and MERRA-2), and is designed to enhance model accuracy in detecting climate change signals. ClimDetect integrates various input and target variables used in previous research, ensuring comparability and consistency across studies. We also explore the application of vision transformers (ViT) to climate data -- a novel approach that, to our knowledge, has not been attempted before for climate change detection tasks. Our open-access data serve as a benchmark for advancing climate science by enabling end-to-end model development and evaluation. ClimDetect is publicly accessible via Hugging Face dataset repository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.",2024-08-28,http://arxiv.org/abs/2408.15993v2,"Sungduk Yu, Brian L. White, Anahita Bhiwandiwalla, Musashi Hinck, Matthew Lyle Olson, Yaniv Gurwicz, Raanan Y. Rohekar, Tung Nguyen, Vasudev Lal",arxiv.org,"cs.CV, cs.LG, physics.ao-ph"
RAIN: Reinforcement Algorithms for Improving Numerical Weather and   Climate Models,"This study explores integrating reinforcement learning (RL) with idealised climate models to address key parameterisation challenges in climate science. Current climate models rely on complex mathematical parameterisations to represent sub-grid scale processes, which can introduce substantial uncertainties. RL offers capabilities to enhance these parameterisation schemes, including direct interaction, handling sparse or delayed feedback, continuous online learning, and long-term optimisation. We evaluate the performance of eight RL algorithms on two idealised environments: one for temperature bias correction, another for radiative-convective equilibrium (RCE) imitating real-world computational constraints. Results show different RL approaches excel in different climate scenarios with exploration algorithms performing better in bias correction, while exploitation algorithms proving more effective for RCE. These findings support the potential of RL-based parameterisation schemes to be integrated into global climate models, improving accuracy and efficiency in capturing complex climate dynamics. Overall, this work represents an important first step towards leveraging RL to enhance climate model accuracy, critical for improving climate understanding and predictions. Code accessible at https://github.com/p3jitnath/climate-rl.",2024-08-28,http://arxiv.org/abs/2408.16118v3,"Pritthijit Nath, Henry Moss, Emily Shuckburgh, Mark Webb",arxiv.org,"cs.LG, physics.ao-ph"
Behaviors of Martian CO2-driven dry climate system and conditions for   atmospheric collapses,"The present Martian climate is characterized by a cold and dry environment with a thin atmosphere of carbon dioxides (CO2). In such conditions, the planetary climate and habitability are determined by the distribution of CO2 between exchangeable reservoirs, that is the atmosphere, ice caps, and regolith. This produces unique responses of the Martian CO2-driven climate system to variations of astronomical forcings. Specifically, it has been shown that the phenomenon called an atmospheric collapse occurs when the axial obliquity is low, affecting the Martian climatic evolution. However, the behavior of the Martian climate system and the accompanying changes in climate and habitability of such planets remain ambiguous. Here we employed a latitudinally-resolved Martian energy balance model and assessed the possible climate on Mars for wider ranges of orbital parameters, solar irradiance, and total exchangeable CO2 mass. We show that the atmospheric collapse occurs when the obliquity is below ~10 degrees when other parameters are kept at the present Mars condition. We also show that the climate solutions on Mars depend on orbital parameters, solar luminosity, and the total exchangeable CO2 mass. We found that the atmospheric collapse would have occurred repeatedly in the history of Mars following the variation of the axial obliquity, while the long-term evolution of atmospheric pCO2 is also affected by the changes in the total exchangeable CO2 mass in Martian history. Even considering the broad ranges of these parameters, the habitable conditions in the Martian CO2-driven dry climate system would be limited to high-latitude summers.",2024-11-11,http://arxiv.org/abs/2411.06871v1,"Yasuto Watanabe, Eiichi Tajika, Arihiro Kamada",arxiv.org,astro-ph.EP
Quantifying Climate Change Impacts on Renewable Energy Generation: A   Super-Resolution Recurrent Diffusion Model,"Driven by global climate change and the ongoing energy transition, the coupling between power supply capabilities and meteorological factors has become increasingly significant. Over the long term, accurately quantifying the power generation of renewable energy under the influence of climate change is essential for the development of sustainable power systems. However, due to interdisciplinary differences in data requirements, climate data often lacks the necessary hourly resolution to capture the short-term variability and uncertainties of renewable energy resources. To address this limitation, a super-resolution recurrent diffusion model (SRDM) has been developed to enhance the temporal resolution of climate data and model the short-term uncertainty. The SRDM incorporates a pre-trained decoder and a denoising network, that generates long-term, high-resolution climate data through a recurrent coupling mechanism. The high-resolution climate data is then converted into power value using the mechanism model, enabling the simulation of wind and photovoltaic (PV) power generation on future long-term scales. Case studies were conducted in the Ejina region of Inner Mongolia, China, using fifth-generation reanalysis (ERA5) and coupled model intercomparison project (CMIP6) data under two climate pathways: SSP126 and SSP585. The results demonstrate that the SRDM outperforms existing generative models in generating super-resolution climate data. Furthermore, the research highlights the estimation biases introduced when low-resolution climate data is used for power conversion.",2024-12-16,http://arxiv.org/abs/2412.11399v2,"Xiaochong Dong, Jun Dan, Yingyun Sun, Yang Liu, Xuemin Zhang, Shengwei Mei",arxiv.org,"cs.LG, eess.SP"
Developing a Climate Litigation Framework: China's Contribution to   International Environmental Law,"Although ""climate litigation"" is not an indigenous term in China, localizing it is essential to support the development of an independent environmental legal knowledge system in China. Rooted in China's judicial tradition, which emphasizes substantive rationality, traditional legal theories have primarily focused on environmental law. However, the contemporary practices in the rule of law have created an unclear trajectory for climate litigation. Research in this area has long been trapped in a paradigm that relies on lawsuits for ecological environmental damage compensation and environmental public interest litigation, leading to a significant disconnect between theoretical frameworks and practical application. With the advancement of the ""dual carbon"" strategic goals-carbon peaking and carbon neutrality-it has become imperative to redefine the concept of climate litigation within the Chinese context. We need to establish a theoretical framework that aligns with the ""dual carbon"" objectives while providing theoretical and institutional support for climate litigation, ultimately contributing to the international discourse on climate justice. Additionally, Hong Kong's proactive climate governance and robust ESG (Environmental, Social, and Governance) practices provide valuable insights for developing comprehensive climate litigation mechanisms. Based on this analysis, we propose concrete plans for building a climate litigation system in China, establishing a preventive relief system and a multi-source legal framework at the substantive level and developing climate judicial mechanisms for mitigation and adaptation at the procedural level.",2025-02-06,http://arxiv.org/abs/2502.03906v1,Yedong Zhang,arxiv.org,econ.TH
ClimateBench-M: A Multi-Modal Climate Data Benchmark with a Simple   Generative Method,"Climate science studies the structure and dynamics of Earth's climate system and seeks to understand how climate changes over time, where the data is usually stored in the format of time series, recording the climate features, geolocation, time attributes, etc. Recently, much research attention has been paid to the climate benchmarks. In addition to the most common task of weather forecasting, several pioneering benchmark works are proposed for extending the modality, such as domain-specific applications like tropical cyclone intensity prediction and flash flood damage estimation, or climate statement and confidence level in the format of natural language. To further motivate the artificial general intelligence development for climate science, in this paper, we first contribute a multi-modal climate benchmark, i.e., ClimateBench-M, which aligns (1) the time series climate data from ERA5, (2) extreme weather events data from NOAA, and (3) satellite image data from NASA HLS based on a unified spatial-temporal granularity. Second, under each data modality, we also propose a simple but strong generative method that could produce competitive performance in weather forecasting, thunderstorm alerts, and crop segmentation tasks in the proposed ClimateBench-M. The data and code of ClimateBench-M are publicly available at https://github.com/iDEA-iSAIL-Lab-UIUC/ClimateBench-M.",2025-04-10,http://arxiv.org/abs/2504.07394v1,"Dongqi Fu, Yada Zhu, Zhining Liu, Lecheng Zheng, Xiao Lin, Zihao Li, Liri Fang, Katherine Tieu, Onkar Bhardwaj, Kommy Weldemariam, Hanghang Tong, Hendrik Hamann, Jingrui He",arxiv.org,"cs.LG, cs.AI"
A spatial analysis of multivariate output from regional climate models,"Climate models have become an important tool in the study of climate and climate change, and ensemble experiments consisting of multiple climate-model runs are used in studying and quantifying the uncertainty in climate-model output. However, there are often only a limited number of model runs available for a particular experiment, and one of the statistical challenges is to characterize the distribution of the model output. To that end, we have developed a multivariate hierarchical approach, at the heart of which is a new representation of a multivariate Markov random field. This approach allows for flexible modeling of the multivariate spatial dependencies, including the cross-dependencies between variables. We demonstrate this statistical model on an ensemble arising from a regional-climate-model experiment over the western United States, and we focus on the projected change in seasonal temperature and precipitation over the next 50 years.",2011-04-14,http://arxiv.org/abs/1104.2703v1,"Stephan R. Sain, Reinhard Furrer, Noel Cressie",arxiv.org,stat.AP
Assessing the consistency between short-term global temperature trends   in observations and climate model projections,"Assessing the consistency between short-term global temperature trends in observations and climate model projections is a challenging problem. While climate models capture many processes governing short-term climate fluctuations, they are not expected to simulate the specific timing of these somewhat random phenomena - the occurrence of which may impact the realized trend. Therefore, to assess model performance, we develop distributions of projected temperature trends from a collection of climate models running the IPCC A1B emissions scenario. We evaluate where observed trends of length 5 to 15 years fall within the distribution of model trends of the same length. We find that current trends lie near the lower limits of the model distributions, with cumulative probability-of-occurrence values typically between 5 percent and 20 percent, and probabilities below 5 percent not uncommon. Our results indicate cause for concern regarding the consistency between climate model projections and observed climate behavior under conditions of increasing anthropogenic greenhouse-gas emissions.",2013-09-20,http://arxiv.org/abs/1309.5164v1,"Patrick J. Michaels, Paul C. Knappenberger, John R. Christy, Chad S. Herman, Lucia M. Liljegren, James D. Annan",arxiv.org,physics.ao-ph
Disentangling different types of El Niño episodes by evolving climate   network analysis,"Complex network theory provides a powerful toolbox for studying the structure of statistical interrelationships between multiple time series in various scientific disciplines. In this work, we apply the recently proposed climate network approach for characterizing the evolving correlation structure of the Earth's climate system based on reanalysis data of surface air temperatures. We provide a detailed study on the temporal variability of several global climate network characteristics. Based on a simple conceptual view on red climate networks (i.e., networks with a comparably low number of edges), we give a thorough interpretation of our evolving climate network characteristics, which allows a functional discrimination between recently recognized different types of El Ni\~no episodes. Our analysis provides deep insights into the Earth's climate system, particularly its global response to strong volcanic eruptions and large-scale impacts of different phases of the El Ni\~no Southern Oscillation (ENSO).",2013-10-21,http://arxiv.org/abs/1310.5494v1,"Alexander Radebach, Reik V. Donner, Jakob Runge, Jonathan F. Donges, Jürgen Kurths",arxiv.org,"physics.data-an, physics.ao-ph"
Assessing the direction of climate interactions by means of complex   networks and information theoretic tools,"An estimate of the net direction of climate interactions in different geographical regions is made by constructing a directed climate network from a regular latitude-longitude grid of nodes, using a directionality index (DI) based on conditional mutual information. Two datasets of surface air temperature anomalies - one monthly-averaged and another daily-averaged - are analyzed and compared. The network links are interpreted in terms of known atmospheric tropical and extratropical variability patterns. Specific and relevant geographical regions are selected, the net direction of propagation of the atmospheric patterns is analyzed and the direction of the inferred links is validated by recovering some well-known climate variability structures. These patterns are found to be acting at various time-scales, such as atmospheric waves in the extra-tropics or longer range events in the tropics. This analysis demonstrates the capability of the DI measure to infer the net direction of climate interactions and may contribute to improve the present understanding of climate phenomena and climate predictability. The work presented here also stands out as an application of advanced tools to the analysis of empirical, real-world data.",2015-02-04,http://arxiv.org/abs/1502.01298v1,"J. Ignacio Deza, Cristina Masoller, Marcelo Barreiro",arxiv.org,nlin.CD
Offshore wind energy climate projection using UPSCALE climate data under   the RCP8.5 emission scenario,"Recently it was demonstrated how climate data can be utilized to estimate regional wind power densities. In particular it was shown that the quality of the global scale estimate compared well with regional high resolution studies and a link between surface temperature and moist density in the estimate was presented. In the present paper the methodology is tested further, to ensure that the results using one climate data set are reliable. This is achieved by extending the study to include four ensemble members. With the confidence that one instantiation is sufficient a climate change data set, which was also a result of the UPSCALE experiment, is analyzed. This, for the first time, provides a projection of future changes in wind power resources using this data set. This climate change data set is based on the Representative Concentration Pathways (RCP) 8.5 climate change scenario. This provides guidance for developers and policy makers to mitigate and adapt.",2015-10-07,http://arxiv.org/abs/1510.02043v1,"Markus Gross, Vanesa Magar",arxiv.org,"physics.ao-ph, physics.geo-ph"
Betting and Belief: Prediction Markets and Attribution of Climate Change,"Despite much scientific evidence, a large fraction of the American public doubts that greenhouse gases are causing global warming. We present a simulation model as a computational test-bed for climate prediction markets. Traders adapt their beliefs about future temperatures based on the profits of other traders in their social network. We simulate two alternative climate futures, in which global temperatures are primarily driven either by carbon dioxide or by solar irradiance. These represent, respectively, the scientific consensus and a hypothesis advanced by prominent skeptics. We conduct sensitivity analyses to determine how a variety of factors describing both the market and the physical climate may affect traders' beliefs about the cause of global climate change. Market participation causes most traders to converge quickly toward believing the ""true"" climate model, suggesting that a climate market could be useful for building public consensus.",2016-03-29,http://arxiv.org/abs/1603.08961v3,"John J. Nay, Martin Van der Linden, Jonathan M. Gilligan",arxiv.org,"cs.MA, physics.soc-ph, q-fin.EC"
Expansion under climate change: the genetic consequences,"Range expansion and range shifts are crucial population responses to climate change. Genetic consequences are not well understood but are clearly coupled to ecological dynamics that, in turn, are driven by shifting climate conditions. We model a population with a deterministic reaction-- diffusion model coupled to a heterogeneous environment that develops in time due to climate change. We decompose the resulting travelling wave solution into neutral genetic components to analyse the spatio-temporal dynamics of its genetic structure. Our analysis shows that range expansions and range shifts under slow climate change preserve genetic diversity. This is because slow climate change creates range boundaries that promote spatial mixing of genetic components. Mathematically , the mixing leads to so-called pushed travelling wave solutions. This mixing phenomenon is not seen in spatially homogeneous environments, where range expansion reduces genetic diversity through gene surfing arising from pulled travelling wave solutions. However, the preservation of diversity is diminished when climate change occurs too quickly. Using diversity indices, we show that fast expansions and range shifts erode genetic diversity more than slow range expansions and range shifts. Our study provides analytical insight into the dynamics of travelling wave solutions in heterogeneous environments.",2016-09-27,http://arxiv.org/abs/1609.08611v1,"Jimmy Garnier, Mark Lewis",arxiv.org,"q-bio.PE, math.AP"
"A Data Driven, Zero-Dimensional Time Delay Model with Radiative Forcing   for Simulating Global Climate","Several complicated non-linear models exist which simulate the physical processes leading to fluctuations in global climate. Some of these more advanced models use observations to constrain various parameters involved. However, they tend to be very computationally expensive. Also, the exact physical processes that affect the climate variations have not been completely comprehended. Therefore, to obtain an insight into global climate, we have developed a physically motivated reduced climate model. The model utilizes a novel mathematical formulation involving a non-linear delay differential equation to study temperature fluctuations when subjected to imposed radiative forcing. We have further incorporated simplified equations to test the effect of speculated mechanisms of climate forcing and evaluated the extent of their influence. The findings are significant in our efforts to predict climate change and help in policy framing necessary to tackle it.",2017-09-26,http://arxiv.org/abs/1709.08860v1,"Rajashik Tarafder, Dibyendu Nandy",arxiv.org,"physics.ao-ph, astro-ph.EP"
Domino Effects in the Earth System -- The potential role of wanted   tipping points,"Vital parts of the climate system, such as the West Antarctic Ice Sheet, are at risk even within the aspired aims of the Paris Agreement to limit global temperature rise to 1.5 -- 2{\deg}C. These so-called natural tipping elements are characterized by rapid qualitative shifts in their states once a critical threshold, e.g., of global mean temperature, is transgressed. We argue that the prevention of such unwanted tipping through effective climate policies may critically depend on cascading Domino effects from (anticipated) climate impacts to emission reductions via wanted social tipping in attitudes, behaviors and policies. Specifically, the latter has recently been noted as a key aspect in addressing contemporary global challenges, such as climate change, via self-amplifying positive feedbacks similar to the processes observable in climate tipping elements. Our discussion is supported with data on commonly observed linkages between (subjective) climate change knowledge, concern and consequential potential action and we argue that the presence of such interrelations serves as a necessary condition for the possibility of rapid climate action.",2019-11-12,http://arxiv.org/abs/1911.10063v2,"Marc Wiedermann, Ricarda Winkelmann, Jonathan F. Donges, Christina Eder, Jobst Heitzig, Alexia Katsanidou, E. Keith Smith",arxiv.org,physics.soc-ph
A continuous spatio-temporal approach to estimate climate change,"We introduce a method for decomposition of trend, cycle and seasonal components in spatio-temporal models and apply it to investigate the existence of climate changes in temperature and rainfall series. The method incorporates critical features in the analysis of climatic problems - the importance of spatial heterogeneity, information from a large number of weather stations, and the presence of missing data. The spatial component is based on continuous projections of spatial covariance functions, allowing modeling the complex patterns of dependence observed in climatic data.   We apply this method to study climate changes in the Northeast region of Brazil, characterized by a great wealth of climates and large amplitudes of temperatures and rainfall. The results show the presence of a tendency for temperature increases, indicating changes in the climatic patterns in this region.",2017-03-20,http://arxiv.org/abs/1703.06804v1,Marcio Poletti Laurini,arxiv.org,"stat.AP, 62"
Wavelet regression: An approach for undertaking multi-time scale   analyses of hydro-climate relationships,"Previous studies showed that hydro-climate processes are stochastic and complex systems, and it is difficult to discover the hidden patterns in the all non-stationary data and thoroughly understand the hydro-climate relationships. For the purpose to show multi-time scale responses of a hydrological variable to climate change, we developed an integrated approach by combining wavelet analysis and regression method, which is called wavelet regression (WR). The customization and the advantage of this approach over the existing methods are presented below: (1) The patterns in the data series of a hydrological variable and its related climatic factors are revealed by the wavelet analysis at different time scales. (2) The hydro-climate relationship of each pattern is revealed by the regression method based on the results of wavelet analysis. (3) The advantage of this approach over the existing methods is that the approach provides a routing to discover the hidden patterns in the stochastic and non-stationary data and quantitatively describe the hydro-climate relationships at different time scales.",2018-06-16,http://arxiv.org/abs/1806.06194v1,Jianhua Xu,arxiv.org,"stat.AP, nlin.PS, physics.ao-ph, physics.geo-ph"
Effect of Climate and Geography on worldwide fine resolution economic   activity,"Geography, including climatic factors, have long been considered potentially important elements in shaping socio-economic activities, alongside other determinants, such as institutions. Here we demonstrate that geography and climate satisfactorily explain worldwide economic activity as measured by the per capita Gross Cell Product (GCP-PC) at a fine geographical resolution, typically much higher than country average. A 1{\deg} by 1{\deg} GCP-PC dataset has been key for establishing and testing a direct relationship between 'local' geography/climate and GCP-PC. Not only have we tested the geography/climate hypothesis using many possible explanatory variables, importantly we have also predicted and reconstructed GCP-PC worldwide by retaining the most significant predictors. While this study confirms that latitude is the most important predictor for GCP-PC when taken in isolation, the accuracy of the GCP-PC prediction is greatly improved when other factors mainly related to variations in climatic variables, such as the variability in air pressure, rather than average climatic conditions as typically used, are considered. Implications of these findings include an improved understanding of why economically better-off societies are geographically placed where they are",2018-06-17,http://arxiv.org/abs/1806.06358v2,Alberto Troccoli,arxiv.org,"econ.EM, physics.ao-ph"
Climate-Related Disasters and the Death Toll,"With climate change accelerating, the frequency of climate disasters is expected to increase in the decades to come. There is ongoing debate as to how different climatic regions will be affected by such an acceleration. In this paper, we describe a model for predicting the frequency of climate disasters and the severity of the resulting number of deaths. The frequency of disasters is described as a Poisson process driven by aggregate CO2 emissions. The severity of disasters is described using a generalized Pareto distribution driven by the trend in regional real gross domestic product (GDP) per capita. We predict the death toll for different types of climate disasters based on the projections made by the Intergovernmental Panel on Climate Change for the population, the regional real GDP per capita, and aggregate CO2 emissions in the ""sustainable"" and ""business-as-usual"" baseline scenarios.",2021-09-05,http://arxiv.org/abs/2109.02111v1,"Valerie Chavez-Demoulin, Eric Jondeau, Linda Mhalla",arxiv.org,stat.AP
Computationally-Efficient Climate Predictions using Multi-Fidelity   Surrogate Modelling,"Accurately modelling the Earth's climate has widespread applications ranging from forecasting local weather to understanding global climate change. Low-fidelity simulations of climate phenomena are readily available, but high-fidelity simulations are expensive to obtain. We therefore investigate the potential of Gaussian process-based multi-fidelity surrogate modelling as a way to produce high-fidelity climate predictions at low cost. Specifically, our model combines the predictions of a low-fidelity Global Climate Model (GCM) and those of a high-fidelity Regional Climate Model (RCM) to produce high-fidelity temperature predictions for a mountainous region on the coastline of Peru. We are able to produce high-fidelity temperature predictions at significantly lower computational cost compared to the high-fidelity model alone: our predictions have an average error of $15.62^\circ\text{C}^2$ yet our approach only evaluates the high-fidelity model on 6% of the region of interest.",2021-08-03,http://arxiv.org/abs/2109.07468v1,"Ben Hudson, Frederik Nijweide, Isaac Sebenius",arxiv.org,"physics.ao-ph, cs.LG, I.2.6; J.2"
Economic activity and climate change,"In this paper, we survey recent econometric contributions to measure the relationship between economic activity and climate change. Due to the critical relevance of these effects for the well-being of future generations, there is an explosion of publications devoted to measuring this relationship and its main channels. The relation between economic activity and climate change is complex with the possibility of causality running in both directions. Starting from economic activity, the channels that relate economic activity and climate change are energy consumption and the consequent pollution. Hence, we first describe the main econometric contributions about the interactions between economic activity and energy consumption, moving then to describing the contributions on the interactions between economic activity and pollution. Finally, we look at the main results on the relationship between climate change and economic activity. An important consequence of climate change is the increasing occurrence of extreme weather phenomena. Therefore, we also survey contributions on the economic effects of catastrophic climate phenomena.",2022-06-07,http://arxiv.org/abs/2206.03187v2,"Aránzazu de Juan, Pilar Poncela, Vladimir Rodríguez-Caballero, Esther Ruiz",arxiv.org,"econ.EM, stat.AP, 91B84 91B76 91B82 62P12 62P20"
On the sensitivity of the simulated European Neolithic transition to   climate extremes,"Was the spread of agropastoralism from the Fertile Crescent throughout Europe influenced by extreme climate events, or was it independent of climate? We here generate idealized climate events using palaeoclimate records. In a mathematical model of regional sociocultural development, these events disturb the subsistence base of simulated forager and farmer societies. We evaluate the regional simulated transition timings and durations against a published large set of radiocarbon dates for western Eurasia; the model is able to realistically hindcast much of the inhomogeneous space-time evolution of regional Neolithic transitions. Our study shows that the consideration of climate events improves the simulation of typical lags between cultural complexes, but that the overall difference to a model without climate events is not significant. Climate events may not have been as important for early sociocultural dynamics as endogenous factors.",2012-03-01,http://arxiv.org/abs/1203.0222v2,"Carsten Lemmen, Kai W. Wirtz",arxiv.org,"q-bio.PE, cs.MA, math.DS, physics.geo-ph"
Visualizing the Consequences of Climate Change Using Cycle-Consistent   Adversarial Networks,"We present a project that aims to generate images that depict accurate, vivid, and personalized outcomes of climate change using Cycle-Consistent Adversarial Networks (CycleGANs). By training our CycleGAN model on street-view images of houses before and after extreme weather events (e.g. floods, forest fires, etc.), we learn a mapping that can then be applied to images of locations that have not yet experienced these events. This visual transformation is paired with climate model predictions to assess likelihood and type of climate-related events in the long term (50 years) in order to bring the future closer in the viewers mind. The eventual goal of our project is to enable individuals to make more informed choices about their climate future by creating a more visceral understanding of the effects of climate change, while maintaining scientific credibility by drawing on climate model projections.",2019-05-02,http://arxiv.org/abs/1905.03709v1,"Victor Schmidt, Alexandra Luccioni, S. Karthik Mukkavilli, Narmada Balasooriya, Kris Sankaran, Jennifer Chayes, Yoshua Bengio",arxiv.org,"cs.CV, cs.AI"
Climate Change Valuation Adjustment (CCVA) using parameterized climate   change impacts,"We introduce Climate Change Valuation Adjustment (CCVA) to capture climate change impacts on CVA+FVA that are currently invisible assuming typical market practice. To discuss such impacts on CVA+FVA from changes to instantaneous hazard rates we introduce a flexible and expressive parameterization to capture the path of this impact to climate change endpoints, and transient transition effects. Finally we provide quantification of examples of typical interest where there is risk of economic stress from sea level change up to 2101, and from transformations of business models. We find that even with the slowest possible uniform approach to a climate change impact in 2101 there can still be significant CVA+FVA impacts on interest rate swaps of 20 years or more maturity. Transformation effects on CVA+FVA are strongly dependent on timing and duration of business model transformation. Using a parameterized approach enables discussion with stakeholders of economic impacts on CVA+FVA, whatever the details behind the climate impact.",2021-02-21,http://arxiv.org/abs/2102.10691v3,"Chris Kenyon, Mourad Berrahoui",arxiv.org,"q-fin.PR, q-fin.PM, q-fin.RM, 91G20, 91G30, 91G40, 91G80, 91G10, 86A08, J.1; F.2.1; G.1.6; G.3; H.4.2; I.1.2; I.6; J.4; J.2"
Improving the predictions of ML-corrected climate models with novelty   detection,"While previous works have shown that machine learning (ML) can improve the prediction accuracy of coarse-grid climate models, these ML-augmented methods are more vulnerable to irregular inputs than the traditional physics-based models they rely on. Because ML-predicted corrections feed back into the climate model's base physics, the ML-corrected model regularly produces out of sample data, which can cause model instability and frequent crashes. This work shows that adding semi-supervised novelty detection to identify out-of-sample data and disable the ML-correction accordingly stabilizes simulations and sharply improves the quality of predictions. We design an augmented climate model with a one-class support vector machine (OCSVM) novelty detector that provides better temperature and precipitation forecasts in a year-long simulation than either a baseline (no-ML) or a standard ML-corrected run. By improving the accuracy of coarse-grid climate models, this work helps make accurate climate models accessible to researchers without massive computational resources.",2022-11-23,http://arxiv.org/abs/2211.13354v1,"Clayton Sanford, Anna Kwa, Oliver Watt-Meyer, Spencer Clark, Noah Brenowitz, Jeremy McGibbon, Christopher Bretherton",arxiv.org,physics.ao-ph
Dynamical Landscape and Multistability of a Climate Model,"We apply two independent data analysis methodologies to locate stable climate states in an intermediate complexity climate model and analyze their interplay. First, drawing from the theory of quasipotentials, and viewing the state space as an energy landscape with valleys and mountain ridges, we infer the relative likelihood of the identified multistable climate states, and investigate the most likely transition trajectories as well as the expected transition times between them. Second, harnessing techniques from data science, specifically manifold learning, we characterize the data landscape of the simulation output to find climate states and basin boundaries within a fully agnostic and unsupervised framework. Both approaches show remarkable agreement, and reveal, apart from the well known warm and snowball earth states, a third intermediate stable state in one of the two climate models we consider. The combination of our approaches allows to identify how the negative feedback of ocean heat transport and entropy production via the hydrological cycle drastically change the topography of the dynamical landscape of Earth's climate.",2020-10-20,http://arxiv.org/abs/2010.10374v2,"Georgios Margazoglou, Tobias Grafke, Alessandro Laio, Valerio Lucarini",arxiv.org,"physics.ao-ph, cond-mat.stat-mech, physics.comp-ph, stat.ML"
PCE-PINNs: Physics-Informed Neural Networks for Uncertainty Propagation   in Ocean Modeling,"Climate models project an uncertainty range of possible warming scenarios from 1.5 to 5 degree Celsius global temperature increase until 2100, according to the CMIP6 model ensemble. Climate risk management and infrastructure adaptation requires the accurate quantification of the uncertainties at the local level. Ensembles of high-resolution climate models could accurately quantify the uncertainties, but most physics-based climate models are computationally too expensive to run as ensemble. Recent works in physics-informed neural networks (PINNs) have combined deep learning and the physical sciences to learn up to 15k faster copies of climate submodels. However, the application of PINNs in climate modeling has so far been mostly limited to deterministic models. We leverage a novel method that combines polynomial chaos expansion (PCE), a classic technique for uncertainty propagation, with PINNs. The PCE-PINNs learn a fast surrogate model that is demonstrated for uncertainty propagation of known parameter uncertainties. We showcase the effectiveness in ocean modeling by using the local advection-diffusion equation.",2021-05-05,http://arxiv.org/abs/2105.02939v1,"Björn Lütjens, Catherine H. Crawford, Mark Veillette, Dava Newman",arxiv.org,cs.LG
An Ontology Model for Climatic Data Analysis,"Recently ontologies have been exploited in a wide range of research areas for data modeling and data management. They greatly assists in defining the semantic model of the underlying data combined with domain knowledge. In this paper, we propose the Climate Analysis (CA) Ontology to model climate datasets used by remote sensing analysts. We use the data published by National Oceanic and Atmospheric Administration (NOAA) to further explore how ontology modeling can be used to facilitate the field of climatic data processing. The idea of this work is to convert relational climate data to the Resource Description Framework (RDF) data model, so that it can be stored in a graph database and easily accessed through the Web as Linked Data. Typically, this provides climate researchers, who are interested in datasets such as NOAA, with the potential of enriching and interlinking with other databases. As a result, our approach facilitates data integration and analysis of diverse climatic data sources and allows researchers to interrogate these sources directly on the Web using the standard SPARQL query language.",2021-06-06,http://arxiv.org/abs/2106.03085v1,"Jiantao Wu, Fabrizio Orlandi, Declan O'Sullivan, Soumyabrata Dev",arxiv.org,cs.DB
Analysis of the evolution of agroclimatic risks in a context of climate   variability in the region of Segou in Mali,"In the Sahel region the population depends largely on rain-fed agriculture. In West Africa in particular, climate models turn to be unable to capture some basic features of present-day climate variability. This study proposes a contribution to the analysis of the evolution of agro-climatic risks in a context of climate variability. Some statistical tests are used on the main variables of the rainy season to determine the trends and the variabilities are described by the data series. Thus, the paper provides a statistical modeling of the different agro-climatic risks while the seasonal variability of agro-climatic parameters were analized as well as their inter annual variability. The study identifies the probability distributions of agroclimatic risks and the characterization of the rainy season was clarified.",2021-06-23,http://arxiv.org/abs/2106.12571v1,"Diop Amadou, Barro Diakarya",arxiv.org,stat.ME
Analysis of Climate Campaigns on Social Media using Bayesian Model   Averaging,"Climate change is the defining issue of our time, and we are at a defining moment. Various interest groups, social movement organizations, and individuals engage in collective action on this issue on social media. In addition, issue advocacy campaigns on social media often arise in response to ongoing societal concerns, especially those faced by energy industries. Our goal in this paper is to analyze how those industries, their advocacy group, and climate advocacy group use social media to influence the narrative on climate change. In this work, we propose a minimally supervised model soup [57] approach combined with messaging themes to identify the stances of climate ads on Facebook. Finally, we release our stance dataset, model, and set of themes related to climate campaigns for future work on opinion mining and the automatic detection of climate change stances.",2023-05-06,http://arxiv.org/abs/2305.06174v2,"Tunazzina Islam, Ruqi Zhang, Dan Goldwasser",arxiv.org,"cs.CL, cs.AI, cs.CY, cs.LG, cs.SI"
HAiVA: Hybrid AI-assisted Visual Analysis Framework to Study the Effects   of Cloud Properties on Climate Patterns,"Clouds have a significant impact on the Earth's climate system. They play a vital role in modulating Earth's radiation budget and driving regional changes in temperature and precipitation. This makes clouds ideal for climate intervention techniques like Marine Cloud Brightening (MCB) which refers to modification in cloud reflectivity, thereby cooling the surrounding region. However, to avoid unintended effects of MCB, we need a better understanding of the complex cloud to climate response function. Designing and testing such interventions scenarios with conventional Earth System Models is computationally expensive. Therefore, we propose a hybrid AI-assisted visual analysis framework to drive such scientific studies and facilitate interactive what-if investigation of different MCB intervention scenarios to assess their intended and unintended impacts on climate patterns. We work with a team of climate scientists to develop a suite of hybrid AI models emulating cloud-climate response function and design a tightly coupled frontend interactive visual analysis system to perform different MCB intervention experiments.",2023-05-13,http://arxiv.org/abs/2305.07859v1,"Subhashis Hazarika, Haruki Hirasawa, Sookyung Kim, Kalai Ramea, Salva R. Cachay, Peetak Mitra, Dipti Hingmire, Hansi Singh, Phil J. Rasch",arxiv.org,cs.LG
Contemporary climate analogs project north-south polarization of urban   water-energy nexus across US cities under warming climate,"Despite the coupled nature of water and electricity demand, the two utilities are often managed by different entities with minimal interaction. Neglecting the water-energy demand nexus leads to to suboptimal management decisions, particularly under climate change. Here, we leverage state-of-the-art machine learning and contemporary climate analogs to project the city-level coupled water and electricity demand of 46 major U.S. cities into the future. The results show that many U.S. cities may experience an increase in electricity (water) demand of up to 20% (15%) due to climate change under a high emissions scenario, with a clear north-south gradient. In the absence of appropriate mitigation strategies, these changes will likely stress current infrastructure, limiting the effectiveness of the ongoing grid decarbonization efforts. In the event that cities are unable to match the increasing demand, there may be increased occurrence of supply shortages, leading to blackouts with disproportionate impacts on vulnerable populations. As such, reliable projections of future water and electricity demand under climate change are critical not only for preventing further exacerbation of the existing environmental injustices but also for more effective design and execution of climate change mitigation and adaptation plans.",2023-06-29,http://arxiv.org/abs/2306.17050v1,"Renee Obringer, Roshanak Nateghi, Jessica Knee, Kaveh Madani, Rohini Kumar",arxiv.org,stat.AP
Multifractal and recurrence measures from meteorological data of climate   zones in India,"We present a study on the spatio-temporal pattern underlying the climate dynamics in various locations spread over India, including the Himalayan region, coastal region, central and northeastern parts of India. We try to capture the variations in the complexity of their dynamics derived from temperature and relative humidity data and classify them based on the multifractal features of their reconstructed phase space dynamics. We also report the variations in climate dynamics over time in these locations by estimating the recurrence-based measures using a sliding window analysis on the data sets. We could then detect significant shifts in climate variability in different spatial locations during the period 1970-2000. The dynamical systems approach presented thus helps to understand the complexity and identify the heterogeneity in climate dynamics. The study also provides relevant inputs on the nature of the shifts in climate that occur in the locations spread over different climate zones.",2023-07-03,http://arxiv.org/abs/2307.01165v3,"Joshin John Bejoy, Jayesh Dave, G. Ambika",arxiv.org,"physics.ao-ph, nlin.CD, physics.data-an"
Multi-variable Hard Physical Constraints for Climate Model Downscaling,"Global Climate Models (GCMs) are the primary tool to simulate climate evolution and assess the impacts of climate change. However, they often operate at a coarse spatial resolution that limits their accuracy in reproducing local-scale phenomena. Statistical downscaling methods leveraging deep learning offer a solution to this problem by approximating local-scale climate fields from coarse variables, thus enabling regional GCM projections. Typically, climate fields of different variables of interest are downscaled independently, resulting in violations of fundamental physical properties across interconnected variables. This study investigates the scope of this problem and, through an application on temperature, lays the foundation for a framework introducing multi-variable hard constraints that guarantees physical relationships between groups of downscaled climate variables.",2023-08-02,http://arxiv.org/abs/2308.01868v1,"Jose González-Abad, Álex Hernández-García, Paula Harder, David Rolnick, José Manuel Gutiérrez",arxiv.org,"physics.ao-ph, cs.LG"
Sea level Projections with Machine Learning using Altimetry and Climate   Model ensembles,"Satellite altimeter observations retrieved since 1993 show that the global mean sea level is rising at an unprecedented rate (3.4mm/year). With almost three decades of observations, we can now investigate the contributions of anthropogenic climate-change signals such as greenhouse gases, aerosols, and biomass burning in this rising sea level. We use machine learning (ML) to investigate future patterns of sea level change. To understand the extent of contributions from the climate-change signals, and to help in forecasting sea level change in the future, we turn to climate model simulations. This work presents a machine learning framework that exploits both satellite observations and climate model simulations to generate sea level rise projections at a 2-degree resolution spatial grid, 30 years into the future. We train fully connected neural networks (FCNNs) to predict altimeter values through a non-linear fusion of the climate model hindcasts (for 1993-2019). The learned FCNNs are then applied to future climate model projections to predict future sea level patterns. We propose segmenting our spatial dataset into meaningful clusters and show that clustering helps to improve predictions of our ML model.",2023-08-02,http://arxiv.org/abs/2308.02460v1,"Saumya Sinha, John Fasullo, R. Steven Nerem, Claire Monteleoni",arxiv.org,"physics.ao-ph, cs.LG"
Quantum Machine Learning in Climate Change and Sustainability: a Review,"Climate change and its impact on global sustainability are critical challenges, demanding innovative solutions that combine cutting-edge technologies and scientific insights. Quantum machine learning (QML) has emerged as a promising paradigm that harnesses the power of quantum computing to address complex problems in various domains including climate change and sustainability. In this work, we survey existing literature that applies quantum machine learning to solve climate change and sustainability-related problems. We review promising QML methodologies that have the potential to accelerate decarbonization including energy systems, climate data forecasting, climate monitoring, and hazardous events predictions. We discuss the challenges and current limitations of quantum machine learning approaches and provide an overview of potential opportunities and future work to leverage QML-based methods in the important area of climate change research.",2023-10-13,http://arxiv.org/abs/2310.09162v1,"Amal Nammouchi, Andreas Kassler, Andreas Theorachis",arxiv.org,"cs.LG, cs.AI"
Understanding Opinions Towards Climate Change on Social Media,"Social media platforms such as Twitter (now known as X) have revolutionized how the public engage with important societal and political topics. Recently, climate change discussions on social media became a catalyst for political polarization and the spreading of misinformation. In this work, we aim to understand how real world events influence the opinions of individuals towards climate change related topics on social media. To this end, we extracted and analyzed a dataset of 13.6 millions tweets sent by 3.6 million users from 2006 to 2019. Then, we construct a temporal graph from the user-user mentions network and utilize the Louvain community detection algorithm to analyze the changes in community structure around Conference of the Parties on Climate Change~(COP) events. Next, we also apply tools from the Natural Language Processing literature to perform sentiment analysis and topic modeling on the tweets. Our work acts as a first step towards understanding the evolution of pro-climate change communities around COP events. Answering these questions helps us understand how to raise people's awareness towards climate change thus hopefully calling on more individuals to join the collaborative effort in slowing down climate change.",2023-12-02,http://arxiv.org/abs/2312.01217v1,"Yashaswi Pupneja, Joseph Zou, Sacha Lévy, Shenyang Huang",arxiv.org,"cs.SI, cs.CL, cs.LG"
Towards Causal Representations of Climate Model Data,"Climate models, such as Earth system models (ESMs), are crucial for simulating future climate change based on projected Shared Socioeconomic Pathways (SSP) greenhouse gas emissions scenarios. While ESMs are sophisticated and invaluable, machine learning-based emulators trained on existing simulation data can project additional climate scenarios much faster and are computationally efficient. However, they often lack generalizability and interpretability. This work delves into the potential of causal representation learning, specifically the \emph{Causal Discovery with Single-parent Decoding} (CDSD) method, which could render climate model emulation efficient \textit{and} interpretable. We evaluate CDSD on multiple climate datasets, focusing on emissions, temperature, and precipitation. Our findings shed light on the challenges, limitations, and promise of using CDSD as a stepping stone towards more interpretable and robust climate model emulation.",2023-12-05,http://arxiv.org/abs/2312.02858v2,"Julien Boussard, Chandni Nagda, Julia Kaltenborn, Charlotte Emilie Elektra Lange, Philippe Brouillard, Yaniv Gurwicz, Peer Nowack, David Rolnick",arxiv.org,"cs.LG, cs.AI, physics.ao-ph, stat.ME"
The Need for Climate Data Stewardship: 10 Tensions and Reflections   regarding Climate Data Governance,"Datafication -- the increase in data generation and advancements in data analysis -- offers new possibilities for governing and tackling worldwide challenges such as climate change. However, employing new data sources in policymaking carries various risks, such as exacerbating inequalities, introducing biases, and creating gaps in access. This paper articulates ten core tensions related to climate data and its implications for climate data governance, ranging from the diversity of data sources and stakeholders to issues of quality, access, and the balancing act between local needs and global imperatives. Through examining these tensions, the article advocates for a paradigm shift towards multi-stakeholder governance, data stewardship, and equitable data practices to harness the potential of climate data for public good. It underscores the critical role of data stewards in navigating these challenges, fostering a responsible data ecology, and ultimately contributing to a more sustainable and just approach to climate action and broader social issues.",2024-03-26,http://arxiv.org/abs/2403.18107v1,Stefaan Verhulst,arxiv.org,cs.CY
Detecting Fallacies in Climate Misinformation: A Technocognitive   Approach to Identifying Misleading Argumentation,"Misinformation about climate change is a complex societal issue requiring holistic, interdisciplinary solutions at the intersection between technology and psychology. One proposed solution is a ""technocognitive"" approach, involving the synthesis of psychological and computer science research. Psychological research has identified that interventions in response to misinformation require both fact-based (e.g., factual explanations) and technique-based (e.g., explanations of misleading techniques) content. However, little progress has been made on documenting and detecting fallacies in climate misinformation. In this study, we apply a previously developed critical thinking methodology for deconstructing climate misinformation, in order to develop a dataset mapping different types of climate misinformation to reasoning fallacies. This dataset is used to train a model to detect fallacies in climate misinformation. Our study shows F1 scores that are 2.5 to 3.5 better than previous works. The fallacies that are easiest to detect include fake experts and anecdotal arguments, while fallacies that require background knowledge, such as oversimplification, misrepresentation, and slothful induction, are relatively more difficult to detect. This research lays the groundwork for development of solutions where automatically detected climate misinformation can be countered with generative technique-based corrections.",2024-05-14,http://arxiv.org/abs/2405.08254v1,"Francisco Zanartu, John Cook, Markus Wagner, Julian Garcia",arxiv.org,cs.CL
Unlearning Climate Misinformation in Large Language Models,"Misinformation regarding climate change is a key roadblock in addressing one of the most serious threats to humanity. This paper investigates factual accuracy in large language models (LLMs) regarding climate information. Using true/false labeled Q&A data for fine-tuning and evaluating LLMs on climate-related claims, we compare open-source models, assessing their ability to generate truthful responses to climate change questions. We investigate the detectability of models intentionally poisoned with false climate information, finding that such poisoning may not affect the accuracy of a model's responses in other domains. Furthermore, we compare the effectiveness of unlearning algorithms, fine-tuning, and Retrieval-Augmented Generation (RAG) for factually grounding LLMs on climate change topics. Our evaluation reveals that unlearning algorithms can be effective for nuanced conceptual claims, despite previous findings suggesting their inefficacy in privacy contexts. These insights aim to guide the development of more factually reliable LLMs and highlight the need for additional work to secure LLMs against misinformation attacks.",2024-05-29,http://arxiv.org/abs/2405.19563v1,"Michael Fore, Simranjit Singh, Chaehong Lee, Amritanshu Pandey, Antonios Anastasopoulos, Dimitrios Stamoulis",arxiv.org,cs.CL
CLIMATELI: Evaluating Entity Linking on Climate Change Data,"Climate Change (CC) is a pressing topic of global importance, attracting increasing attention across research fields, from social sciences to Natural Language Processing (NLP). CC is also discussed in various settings and communication platforms, from academic publications to social media forums. Understanding who and what is mentioned in such data is a first critical step to gaining new insights into CC. We present CLIMATELI (CLIMATe Entity LInking), the first manually annotated CC dataset that links 3,087 entity spans to Wikipedia. Using CLIMATELI (CLIMATe Entity LInking), we evaluate existing entity linking (EL) systems on the CC topic across various genres and propose automated filtering methods for CC entities. We find that the performance of EL models notably lags behind humans at both token and entity levels. Testing within the scope of retaining or excluding non-nominal and/or non-CC entities particularly impacts the models' performances.",2024-06-24,http://arxiv.org/abs/2406.16732v2,"Shijia Zhou, Siyao Peng, Barbara Plank",arxiv.org,cs.CL
Bayesian Inference for Stochastic Predictions of Non-Gaussian Systems   with Applications in Climate Change,"Climate change poses significant challenges for accurate climate modeling due to the complexity and variability of non-Gaussian climate systems. To address the complexities of non-Gaussian systems in climate modeling, this thesis proposes a Bayesian framework utilizing the Unscented Kalman Filter (UKF), Ensemble Kalman Filter (EnKF), and Unscented Particle Filter (UPF) for one-dimensional and two-dimensional stochastic climate models, evaluated with real-world temperature and sea level data. We study these methods under varying conditions, including measurement noise, sample sizes, and observed and hidden variables, to highlight their respective advantages and limitations. Our findings reveal that merely increasing data is insufficient for accurate predictions; instead, selecting appropriate methods is crucial. This research provides insights into issues related to information barrier, curse of dimensionality, prediction variability, and measurement noise quantification, thereby enhancing the application of these techniques in real-world climate scenarios.",2024-06-20,http://arxiv.org/abs/2406.18606v1,Yunjin Tong,arxiv.org,"stat.AP, physics.ao-ph"
"Assessing the climate benefits of afforestation: processes, methods, and   frameworks","Afforestation greatly influences several earth system processes, making it essential to understand these effects to accurately assess its potential for climate change mitigation. Although our understanding of forest-climate system interactions has improved, significant knowledge gaps remain, preventing definitive assessments of afforestation's net climate benefits. In this review, focusing on the Canadian northern boreal and southern arctic, we identify these gaps and synthesize existing knowledge. The review highlights regional realities, Earth's climatic history, uncertainties in biogeochemical (BGC) and biogeophysical (BGP) changes following afforestation, and limitations in current assessment methodologies, emphasizing the need to reconcile these uncertainties before drawing firm conclusions about the climate benefits of afforestation. Finally, we propose an assessment framework which considers multiple forcing components, temporal analysis, future climatic contexts, and implementation details. We hope that the research gaps and assessment framework discussed in this review inform afforestation policy in Canada and other circumpolar nations.",2024-07-19,http://arxiv.org/abs/2407.14617v3,"Kevin Bradley Dsouza, Enoch Ofosu, Jack Salkeld, Richard Boudreault, Juan Moreno-Cruz, Yuri Leonenko",arxiv.org,"physics.ao-ph, physics.geo-ph"
Quantifying uncertainty in climate projections with conformal ensembles,"Large climate model ensembles are the primary tool for robustly projecting future climate states and quantifying projection uncertainty. Despite significant advancements in climate modeling over the past few decades, overall projection certainty has not commensurately decreased with steadily improving model skill. We introduce conformal ensembling, a new approach to uncertainty quantification in climate projections based on conformal inference to reduce projection uncertainty. Unlike traditional methods, conformal ensembling seamlessly integrates climate model ensembles and observational data across a range of scales to generate statistically rigorous, easy-to-interpret uncertainty estimates. It can be applied to any climatic variable using any ensemble analysis method and outperforms existing inter-model variability methods in uncertainty quantification across all time horizons and most spatial locations under SSP2-4.5. Conformal ensembling is also computationally efficient, requires minimal assumptions, and is highly robust to the conformity measure. Experiments show that it is effective when conditioning future projections on historical reanalysis data compared with standard ensemble averaging approaches, yielding more physically consistent projections.",2024-08-13,http://arxiv.org/abs/2408.06642v2,"Trevor Harris, Ryan Sriver",arxiv.org,"stat.AP, stat.ML"
Cross-Country Comparative Analysis of Climate Resilience and Localized   Mapping in Data-Sparse Regions,"Climate resilience across sectors varies significantly in low-income countries (LICs), with agriculture being the most vulnerable to climate change. Existing studies typically focus on individual countries, offering limited insights into broader cross-country patterns of adaptation and vulnerability. This paper addresses these gaps by introducing a framework for cross-country comparative analysis of sectoral climate resilience using meta-analysis and cross-country panel data techniques. The study identifies shared vulnerabilities and adaptation strategies across LICs, enabling more effective policy design. Additionally, a novel localized climate-agriculture mapping technique is developed, integrating sparse agricultural data with high-resolution satellite imagery to generate fine-grained maps of agricultural productivity under climate stress. Spatial interpolation methods, such as kriging, are used to address data gaps, providing detailed insights into regional agricultural productivity and resilience. The findings offer policymakers tools to prioritize climate adaptation efforts and optimize resource allocation both regionally and nationally.",2024-09-13,http://arxiv.org/abs/2409.08765v1,Ronald Katende,arxiv.org,"cs.NE, stat.AP"
Rapid Climate Model Downscaling to Assess Risk of Extreme Rainfall in   Bangladesh in a Warming Climate,"As climate change drives an increase in global extremes, it is critical for Bangladesh, a nation highly vulnerable to these impacts, to assess future risks for effective adaptation and mitigation planning. Downscaling coarse-resolution climate models to finer scales is essential for accurately evaluating the risk of extremes. In this study, we apply our downscaling method, which integrates data, physics, and machine learning, to quantify the risks of extreme precipitation in Bangladesh. The proposed approach successfully captures the observed spatial patterns and risks of extreme rainfall in the current climate while generating risk and uncertainty estimates by efficiently downscaling multiple models under future climate scenarios. Our findings project that the risk of extreme rainfall rises across the country, with the most significant increases in the northeastern hilly and southeastern coastal areas. Projections show that the daily maximum rainfall for a 100-year return period could increase by approximately 50 mm/day by mid-century and around 100 mm/day by the end of the century. However, substantial uncertainties remain due to variations across multiple climate models and scenarios.",2024-12-21,http://arxiv.org/abs/2412.16407v1,"Anamitra Saha, Sai Ravela",arxiv.org,physics.ao-ph
Sovereign Debt Default and Climate Risk,"We explore the interplay between sovereign debt default/renegotiation and environmental factors (e.g., pollution from land use, natural resource exploitation). Pollution contributes to the likelihood of natural disasters and influences economic growth rates. The country can default on its debt at any time while also deciding whether to invest in pollution abatement. The framework provides insights into the credit spreads of sovereign bonds and explains the observed relationship between bond spread and a country's climate vulnerability. Through calibration for developing and low-income countries, we demonstrate that there is limited incentive for these countries to address climate risk, and the sensitivity of bond spreads to climate vulnerability remains modest. Climate risk does not play a relevant role on the decision to default on sovereign debt. Financial support for climate abatement expenditures can effectively foster climate adaptation actions, instead renegotiation conditional upon pollution abatement does not produce any effect.",2025-01-20,http://arxiv.org/abs/2501.11552v1,"Emilio Barucci, Daniele Marazzina, Aldo Nassigh",arxiv.org,"econ.GN, q-fin.EC, q-fin.GN, q-fin.PR, q-fin.RM"
Changes over time in the 100-year return value of climate model   variables,"We assess evidence for changes in tail characteristics of wind, solar irradiance and temperature variables output from CMIP6 global climate models (GCMs) due to climate forcing. We estimate global and climate zone annual maximum and annual means for period (2015, 2100) from daily output of seven GCMs for daily wind speed, maximum wind speed, solar irradiance and near-surface temperature. We calculate corresponding annualised data for individual locations within neighbourhoods of the North Atlantic and Celtic Sea region. We consider output for three climate scenarios and multiple climate ensembles. We estimate non-stationary extreme value models for annual extremes, and non-homogeneous Gaussian regressions for annual means, using Bayesian inference. We use estimated statistical models to quantify the distribution of (i) the change in 100-year return value for annual extremes, and (2) the change in annual mean, over the period (2025, 2125). To summarise results, we estimate linear mixed effects models for observed variation of (i) and (ii). Evidence for changes in the 100-year return value for annual maxima of solar irradiance and temperature is much stronger than for wind variables over time and with climate scenario.",2025-01-20,http://arxiv.org/abs/2501.11650v2,"Callum Leach, Kevin Ewans, Philip Jonathan",arxiv.org,"stat.AP, physics.ao-ph"
Exploring the Reform and Development Pathways of AIIB's Climate   Accountability Mechanism in the Context of Global Climate Governance,"After the Asian Infrastructure Investment Bank (AIIB) revised its Environmental and Social Framework, it has committed to certain climate-related objectives, yet an independent climate accountability mechanism has not been established. The absence of clear evaluation principles and procedural rules presents challenges in effectively addressing environmental investment disputes. This article reviews both domestic and international literature related to AIIB's climate accountability mechanism, identifying that the current Environmental and Social Framework's principled content and reliance on traditional approaches have resulted in issues of enforceability and the absence of an independent accountability institution. To enhance the effectiveness of AIIB's climate accountability mechanism, a reassessment of its development path is necessary. Future developments should transition from an additive path to a substitutive path, focusing on the application of international environmental and social standards, increasing stakeholder recognition and participation, promoting comprehensive reforms within AIIB, and establishing a coordinated independent accountability system. These measures aim to support the robust development of AIIB's climate accountability mechanism.",2025-02-06,http://arxiv.org/abs/2502.03893v1,Yedong Zhang,arxiv.org,econ.TH
Causal Climate Emulation with Bayesian Filtering,"Traditional models of climate change use complex systems of coupled equations to simulate physical processes across the Earth system. These simulations are highly computationally expensive, limiting our predictions of climate change and analyses of its causes and effects. Machine learning has the potential to quickly emulate data from climate models, but current approaches are not able to incorporate physics-informed causal relationships. Here, we develop an interpretable climate model emulator based on causal representation learning. We derive a physics-informed approach including a Bayesian filter for stable long-term autoregressive emulation. We demonstrate that our emulator learns accurate climate dynamics, and we show the importance of each one of its components on a realistic synthetic dataset and data from two widely deployed climate models.",2025-06-11,http://arxiv.org/abs/2506.09891v1,"Sebastian Hickman, Ilija Trajkovic, Julia Kaltenborn, Francis Pelletier, Alex Archibald, Yaniv Gurwicz, Peer Nowack, David Rolnick, Julien Boussard",arxiv.org,"cs.LG, cs.AI, cs.CE, physics.ao-ph"
Vision Transformers for Multi-Variable Climate Downscaling: Emulating   Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture,"Global Climate Models (GCMs) are critical for simulating large-scale climate dynamics, but their coarse spatial resolution limits their applicability in regional studies. Regional Climate Models (RCMs) refine this through dynamic downscaling, albeit at considerable computational cost and with limited flexibility. While deep learning has emerged as an efficient data-driven alternative, most existing studies have focused on single-variable models that downscale one variable at a time. This approach can lead to limited contextual awareness, redundant computation, and lack of cross-variable interaction. Our study addresses these limitations by proposing a multi-task, multi-variable Vision Transformer (ViT) architecture with a shared encoder and variable-specific decoders (1EMD). The proposed architecture jointly predicts three key climate variables: surface temperature (tas), wind speed (sfcWind), and 500 hPa geopotential height (zg500), directly from GCM-resolution inputs, emulating RCM-scale downscaling over Europe. We show that our multi-variable approach achieves positive cross-variable knowledge transfer and consistently outperforms single-variable baselines trained under identical conditions, while also improving computational efficiency. These results demonstrate the effectiveness of multi-variable modeling for high-resolution climate downscaling.",2025-06-12,http://arxiv.org/abs/2506.22447v1,"Fabio Merizzi, Harilaos Loukos",arxiv.org,"cs.LG, cs.AI, eess.IV"
The Green Premium Puzzle: Empirical Evidence from Climate-Friendly Food   Products,"This paper investigates whether climate-friendly food products command a price premium in consumer markets. Using product-level data from a supermarket in Sweden, we examine the relationship between front-of-package climate impact scores and retail prices, controlling for product size, nutritional content, and fixed effects. Contrary to the intuitive expectation of a positive green premium, we find no evidence that climate-friendly products are priced higher. In some product categories, products with better climate scores are in fact associated with lower prices, suggesting a negative premium, an outcome that gives rise to what we refer to as the green premium puzzle. We argue that market frictions such as competing consumer priorities, psychological distance from climate issues, and skepticism toward environmental labeling may suppress the price signals intended to reward sustainable consumption. These findings offer important insights for producers, retailers, and policymakers seeking to align climate goals with effective market incentives in the transition toward a more sustainable society.",2025-07-14,http://arxiv.org/abs/2507.10333v1,"Voraprapa Nakavachara, Chanon Thongtai, Thanarat Chalidabhongse, Chanathip Pharino",arxiv.org,"econ.GN, q-fin.EC"
Changes in zonal surface temperature gradients and Walker circulations   in a wide range of climates,"Variations in zonal surface temperature gradients and zonally asymmetric tropical overturning circulations (Walker circulations) are examined over a wide range of climates simulated with an idealized atmospheric general circulation model (GCM). The asymmetry in the tropical climate is generated by an imposed ocean energy flux, which does not vary with climate. The range of climates is simulated by modifying the optical thickness of an idealized longwave absorber (representing greenhouse gases).   The zonal surface temperature gradient in low latitudes generally decreases as the climate warms in the idealized GCM simulations. A scaling relationship based on a two-term balance in the surface energy budget accounts for the changes in the zonally asymmetric component of the GCM-simulated surface temperature gradients.   The Walker circulation weakens as the climate warms in the idealized simulations, as it does in comprehensive simulations of climate change. The wide range of climates allows a systematic test of energetic arguments that have been proposed to account for these changes in the tropical circulation. The analysis shows that a scaling estimate based on changes in the hydrological cycle (precipitation rate and saturation specific humidity) accounts for the simulated changes in the Walker circulation. However, it must be evaluated locally, with local precipitation rates. If global-mean quantities are used, the scaling estimate does not generally account for changes in the Walker circulation, and the extent to which it does is the result of compensating errors in changes in precipitation and saturation specific humidity that enter the scaling estimate.",2010-09-01,http://arxiv.org/abs/1009.0301v2,"Timothy M. Merlis, Tapio Schneider",arxiv.org,physics.ao-ph
Quantifying Age and Model Uncertainties in Paleoclimate Data and   Dynamical Climate Models with a Joint Inferential Analysis,"A major goal in paleoclimate science is to reconstruct historical climates using proxies for climate variables such as those observed in sediment cores, and in the process learn about climate dynamics. This is hampered by uncertainties in how sediment core depths relate to ages, how proxy quantities relate to climate variables, how climate models are specified, and the values of parameters in climate models. Quantifying these uncertainties is key in drawing well founded conclusions. Analyses are often performed in separate stages with, for example, a sediment core's depth-age relation being estimated as stage one, then fed as an input to calibrate climate models as stage two. Here, we show that such ""multi-stage"" approaches can lead to misleading conclusions. We develop a joint inferential approach for climate reconstruction, model calibration, and age model estimation. We focus on the glacial-interglacial cycle over the past 780 kyr, analysing two sediment cores that span this range. Our age estimates are largely in agreement with previous studies, but provides the full joint specification of all uncertainties, estimation of model parameters, and the model evidence. By sampling plausible chronologies from the posterior distribution, we demonstrate that downstream scientific conclusions can differ greatly both between different sampled chronologies, and in comparison with conclusions obtained in the complete joint inferential analysis. We conclude that multi-stage analyses are insufficient when dealing with uncertainty, and that to draw sound conclusions the full joint inferential analysis must be performed.",2018-03-22,http://arxiv.org/abs/1803.08482v2,"Jake Carson, Michel Crucifix, Simon P. Preston, Richard D. Wilkinson",arxiv.org,stat.AP
Beyond Forcing Scenarios: Predicting Climate Change through Response   Operators in a Coupled General Circulation Model,"Global Climate Models are key tools for predicting the future response of the climate system to a variety of natural and anthropogenic forcings. Here we show how to use statistical mechanics to construct operators able to flexibly predict climate change for a variety of climatic variables of interest. We perform our study on a fully coupled model - MPI-ESM v.1.2 - and for the first time we prove the effectiveness of response theory in predicting future climate response to CO$_2$ increase on a vast range of temporal scales, from inter-annual to centennial, and for very diverse climatic quantities. We investigate within a unified perspective the transient climate response and the equilibrium climate sensitivity and assess the role of fast and slow processes. The prediction of the ocean heat uptake highlights the very slow relaxation to a newly established steady state. The change in the Atlantic Meridional Overturning Circulation (AMOC) and of the Antarctic Circumpolar Current (ACC) is accurately predicted. The AMOC strength is initially reduced and then undergoes a slow and only partial recovery. The ACC strength initially increases as a result of changes in the wind stress, then undergoes a slowdown, followed by a recovery leading to a overshoot with respect to the initial value. Finally, we are able to predict accurately the temperature change in the Northern Atlantic.",2019-12-09,http://arxiv.org/abs/1912.03996v2,"Valerio Lembo, Valerio Lucarini, Francesco Ragone",arxiv.org,"physics.ao-ph, cond-mat.stat-mech, physics.comp-ph, physics.geo-ph"
Climate bistability of Earth-like exoplanets,"Before about 500 million years ago, most probably our planet experienced temporary snowball conditions, with continental and sea ices covering a large fraction of its surface. This points to a potential bistability of Earth's climate, that can have at least two different (statistical) equilibrium states for the same external forcing (i.e., solar radiation). Here we explore the probability of finding bistable climates in earth-like exoplanets, and consider the properties of planetary climates obtained by varying the semi-major orbital axis (thus, received stellar radiation), eccentricity and obliquity, and atmospheric pressure. To this goal, we use the Earth-like planet surface temperature model (ESTM), an extension of 1D Energy Balance Models developed to provide a numerically efficient climate estimator for parameter sensitivity studies and long climatic simulations. After verifying that the ESTM is able to reproduce Earth climate bistability, we identify the range of parameter space where climate bistability is detected. An intriguing result of the present work is that the planetary conditions that support climate bistability are remarkably similar to those required for the sustainance of complex, multicellular life on the planetary surface. The interpretation of this result deserves further investigation, given its relevance for the potential distribution of life in exoplanetary systems.",2019-12-11,http://arxiv.org/abs/1912.05392v1,"Murante G., Provenzale A., Vladilo G., Taffoni G., Silv L., Palazzi E., Hardenberg J., Maris M., Londero E., Knapic C., Zorba S",arxiv.org,astro-ph.EP
Reconciling high resolution climate datasets using KrigR,"There is an increasing need for high spatial and temporal resolution climate data for the wide community of researchers interested in climate change and its consequences. Currently, there is a large mismatch between the spatial resolutions of global climate model and reanalysis datasets (at best around 0.25o and 0.1o respectively) and the resolutions needed by many end-users of these datasets, which are typically on the scale of 30 arcseconds (~900m). This need for improved spatial resolution in climate datasets has motivated several groups to statistically downscale various combinations of observational or reanalysis datasets. However, the variety of downscaling methods and inputs used makes it difficult to reconcile the resultant differences between these high-resolution datasets. Here we make use of the KrigR R-package to statistically downscale the world-leading ERA5(-Land) reanalysis data using kriging. We show that kriging can accurately recover spatial heterogeneity of climate data given strong relationships with co-variates; that by preserving the uncertainty associated with the statistical downscaling, one can investigate and account for confidence in high-resolution climate data; and that the statistical uncertainty provided by KrigR can explain much of the difference between widely used high resolution climate datasets (CHELSA, TerraClimate, and WorldClim2) depending on variable, timescale, and region. This demonstrates the advantages of using KrigR to generate customized high spatial and/or temporal resolution climate data.",2021-08-09,http://arxiv.org/abs/2108.03957v1,"Richard Davy, Erik Kusch",arxiv.org,physics.ao-ph
Uncertainty in Climate Science: Not Cause for Inaction,"Using observational data and an elementary rigorous statistical fact it is easily shown that the distribution of Earth's climate is non-stationary. Examination of records of hundreds of local Industrial Era temperature histories in the Northern Hemisphere were used to show this fact. Statistically, the mean of the ensemble has been rising during the Industrial Era. All of this confirms what climate scientists already know. The issue of predictions under uncertainties was tackled as well: a simple balance model was tuned to track an ensemble of climate records. Stochastic parametrizations were created to capture natural and anthropogenic CO2 forcings. The resulting stochastic model was then tested against historical data and then used to make future predictions. This exercise confirmed as well climate science attribution to significant global warming during the Industrial Era to anthropogenic activities. The variability of the model due to uncertainties is simply not large enough to obfuscate a clear rise in the mean temperature in the Industrial Era. Further, even if the variance of the natural CO2 contribution is greatly increased artificially (in the model), the fluctuations cannot account for the current change in the historical mean.   These outcomes weaken the factual validity of the US administration, 2016-2020, claims that there are too many uncertainties in climate and climate science to make climate predictions, and further that contemporary reports of floods, extreme weather, even a rising global mean temperature are simply manifestations of a climate that always fluctuates within a nature-derived statistical distribution.",2021-08-19,http://arxiv.org/abs/2108.08781v2,"Juan M. Restrepo, Michael E. Mann",arxiv.org,"physics.ao-ph, math.DS, 86A08"
A Multi-task Model for Sentiment Aided Stance Detection of Climate   Change Tweets,"Climate change has become one of the biggest challenges of our time. Social media platforms such as Twitter play an important role in raising public awareness and spreading knowledge about the dangers of the current climate crisis. With the increasing number of campaigns and communication about climate change through social media, the information could create more awareness and reach the general public and policy makers. However, these Twitter communications lead to polarization of beliefs, opinion-dominated ideologies, and often a split into two communities of climate change deniers and believers. In this paper, we propose a framework that helps identify denier statements on Twitter and thus classifies the stance of the tweet into one of the two attitudes towards climate change (denier/believer). The sentimental aspects of Twitter data on climate change are deeply rooted in general public attitudes toward climate change. Therefore, our work focuses on learning two closely related tasks: Stance Detection and Sentiment Analysis of climate change tweets. We propose a multi-task framework that performs stance detection (primary task) and sentiment analysis (auxiliary task) simultaneously. The proposed model incorporates the feature-specific and shared-specific attention frameworks to fuse multiple features and learn the generalized features for both tasks. The experimental results show that the proposed framework increases the performance of the primary task, i.e., stance detection by benefiting from the auxiliary task, i.e., sentiment analysis compared to its uni-modal and single-task variants.",2022-11-07,http://arxiv.org/abs/2211.03533v1,"Apoorva Upadhyaya, Marco Fisichella, Wolfgang Nejdl",arxiv.org,cs.CL
On model selection criteria for climate change impact studies,"Climate change impact studies inform policymakers on the estimated damages of future climate change on economic, health and other outcomes. In most studies, an annual outcome variable is observed, e.g. agricultural yield, along with a higher-frequency regressor, e.g. daily temperature. Applied researchers then face a problem of selecting a model to characterize the nonlinear relationship between the outcome and the high-frequency regressor to make a policy recommendation based on the model-implied damage function. We show that existing model selection criteria are only suitable for the policy objective if one of the models under consideration nests the true model. If all models are seen as imperfect approximations to the true nonlinear relationship, the model that performs well in the normal climate conditions is not guaranteed to perform well at the projected climate that is different from the historical norm. We therefore propose a new criterion, the proximity-weighted mean-squared error (PWMSE), that directly targets precision of the damage function at the projected future climate. To make this criterion feasible, we assign higher weights to prior years that can serve as weather analogs to the projected future climate when evaluating competing models using the PWMSE. We show that our approach selects the best approximate regression model that has the smallest weighted error of predicted impacts for a projected future climate. A simulation study and an application revisiting the impact of climate change on agricultural production illustrate the empirical relevance of our theoretical analysis.",2018-08-23,http://arxiv.org/abs/1808.07861v4,"Xiaomeng Cui, Bulat Gafarov, Dalia Ghanem, Todd Kuffner",arxiv.org,"stat.ME, stat.AP, 62M99 (Primary) 62P12, 62F99 (Secondary)"
Definition of Cybernetical Neuroscience,"A new scientific field is introduced and discussed, named cybernetical neuroscience, which studies mathematical models adopted in computational neuroscience by methods of cybernetics -- the science of control and communication in a living organism, machine and society. It also considers the practical application of the results obtained when studying mathematical models. The main tasks and methods, as well as some results of cybernetic neuroscience are considered.",2024-09-14,http://arxiv.org/abs/2409.16314v1,Alexander Fradkov,arxiv.org,"q-bio.NC, math.OC"
Teaching Computational Neuroscience,The problems and beauty of teaching computational neuroscience are discussed by reviewing three new textbooks.,2014-12-18,http://arxiv.org/abs/1412.5909v2,Péter Érdi,arxiv.org,q-bio.NC
Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and   Semantic Understanding Capability of LLM,"Neuroscience research publications encompass a vast wealth of knowledge. Accurately retrieving existing information and discovering new insights from this extensive literature is essential for advancing the field. However, when knowledge is dispersed across multiple sources, current state-of-the-art retrieval methods often struggle to extract the necessary information. A knowledge graph (KG) can integrate and link knowledge from multiple sources, but existing methods for constructing KGs in neuroscience often rely on labeled data and require domain expertise. Acquiring large-scale, labeled data for a specialized area like neuroscience presents significant challenges. This work proposes novel methods for constructing KG from unlabeled large-scale neuroscience research corpus utilizing large language models (LLM), neuroscience ontology, and text embeddings. We analyze the semantic relevance of neuroscience text segments identified by LLM for building the knowledge graph. We also introduce an entity-augmented information retrieval algorithm to extract knowledge from the KG. Several experiments were conducted to evaluate the proposed approaches, and the results demonstrate that our methods significantly enhance knowledge discovery from the unlabeled neuroscience research corpus. It achieves an F1 score of 0.84 for entity extraction, and the knowledge obtained from the KG improves answers to over 54% of the questions.",2025-06-03,http://arxiv.org/abs/2506.03145v1,"Pralaypati Ta, Sriram Venkatesaperumal, Keerthi Ram, Mohanasankar Sivaprakasam",arxiv.org,"cs.CL, cs.AI"
"A Paradigm Shift in Neuroscience Driven by Big Data: State of art,   Challenges, and Proof of Concept","A recent editorial in Nature noted that cognitive neuroscience is at a crossroads where it is a thorny issue to reliably reveal brain-behavior associations. This commentary sketches a big data science way out for cognitive neuroscience, namely population neuroscience. In terms of design, analysis, and interpretations, population neuroscience research takes the design control to an unprecedented level, greatly expands the dimensions of the data analysis space, and paves a paradigm shift for exploring mechanisms on brain-behavior associations.",2022-12-08,http://arxiv.org/abs/2212.04195v2,"Zi-Xuan Zhou, Xi-Nian Zuo",arxiv.org,"q-bio.NC, q-bio.QM, stat.ME"
What can topology tell us about the neural code?,"Neuroscience is undergoing a period of rapid experimental progress and expansion. New mathematical tools, previously unknown in the neuroscience community, are now being used to tackle fundamental questions and analyze emerging data sets. Consistent with this trend, the last decade has seen an uptick in the use of topological ideas and methods in neuroscience. In this talk I will survey recent applications of topology in neuroscience, and explain why topology is an especially natural tool for understanding neural codes. Note: This is a write-up of my talk for the Current Events Bulletin, held at the 2016 Joint Math Meetings in Seattle, WA.",2016-05-06,http://arxiv.org/abs/1605.01905v1,Carina Curto,arxiv.org,q-bio.NC
Is neuroscience facing up to statistical power?,"It has been demonstrated that the statistical power of many neuroscience studies is very low, so that the results are unlikely to be robustly reproducible. How are neuroscientists and the journals in which they publish responding to this problem? Here I review the sample size justifications provided for all 15 papers published in one recent issue of the leading journal Nature Neuroscience. Of these, only one claimed it was adequately powered. The others mostly appealed to the sample sizes used in earlier studies, despite a lack of evidence that these earlier studies were adequately powered. Thus, concerns regarding statistical power in neuroscience have mostly not yet been addressed.",2017-01-05,http://arxiv.org/abs/1701.01219v1,Geoffrey J Goodhill,arxiv.org,"q-bio.NC, stat.AP"
The Roles of Supervised Machine Learning in Systems Neuroscience,"Over the last several years, the use of machine learning (ML) in neuroscience has been rapidly increasing. Here, we review ML's contributions, both realized and potential, across several areas of systems neuroscience. We describe four primary roles of ML within neuroscience: 1) creating solutions to engineering problems, 2) identifying predictive variables, 3) setting benchmarks for simple models of the brain, and 4) serving itself as a model for the brain. The breadth and ease of its applicability suggests that machine learning should be in the toolbox of most systems neuroscientists.",2018-05-21,http://arxiv.org/abs/1805.08239v2,"Joshua I. Glaser, Ari S. Benjamin, Roozbeh Farhoodi, Konrad P. Kording",arxiv.org,"q-bio.NC, cs.LG, stat.ML"
Connecting levels of analysis in the computational era,"Neuroscience and artificial intelligence are closely intertwined, but so are the physics of dynamical system, philosophy and psychology. Each of these fields try in their own way to relate observations at the level of molecules, synapses, neurons or behavior, to a function. An influential conceptual approach to this end was popularized by David Marr, which focused on the interaction between three theoretical 'levels of analysis'. With the convergence of simulation-based approaches, algorithm-oriented Neuro-AI and high-throughput data, we currently see much research organized around four levels of analysis: observations, models, algorithms and functions. Bidirectional interaction between these levels influences how we undertake interdisciplinary science.",2023-05-10,http://arxiv.org/abs/2305.06037v2,"Richard Naud, André Longtin",arxiv.org,q-bio.NC
"Playing With Neuroscience: Past, Present and Future of Neuroimaging and   Games","Videogames have been a catalyst for advances in many research fields, such as artificial intelligence, human-computer interaction or virtual reality. Over the years, research in fields such as artificial intelligence has enabled the design of new types of games, while games have often served as a powerful tool for testing and simulation. Can this also happen with neuroscience? What is the current relationship between neuroscience and games research? what can we expect from the future? In this article, we'll try to answer these questions, analysing the current state-of-the-art at the crossroads between neuroscience and games and envisioning future directions.",2024-03-06,http://arxiv.org/abs/2403.15413v1,"Paolo Burelli, Laurits Dixen",arxiv.org,"q-bio.NC, cs.AI"
Cortex simulation system proposal using distributed computer network   environments,In the dawn of computer science and the eve of neuroscience we participate in rebirth of neuroscience due to new technology that allows us to deeply and precisely explore whole new world that dwells in our brains.,2014-03-22,http://arxiv.org/abs/1403.5701v1,Boris Tomas,arxiv.org,cs.AI
An overview of open source Deep Learning-based libraries for   Neuroscience,"In recent years, deep learning revolutionized machine learning and its applications, producing results comparable to human experts in several domains, including neuroscience. Each year, hundreds of scientific publications present applications of deep neural networks for biomedical data analysis. Due to the fast growth of the domain, it could be a complicated and extremely time-consuming task for worldwide researchers to have a clear perspective of the most recent and advanced software libraries. This work contributes to clarify the current situation in the domain, outlining the most useful libraries that implement and facilitate deep learning application to neuroscience, allowing scientists to identify the most suitable options for their research or clinical projects. This paper summarizes the main developments in Deep Learning and their relevance to Neuroscience; it then reviews neuroinformatic toolboxes and libraries, collected from the literature and from specific hubs of software projects oriented to neuroscience research. The selected tools are presented in tables detailing key features grouped by domain of application (e.g. data type, neuroscience area, task), model engineering (e.g. programming language, model customization) and technological aspect (e.g. interface, code source). The results show that, among a high number of available software tools, several libraries are standing out in terms of functionalities for neuroscience applications. The aggregation and discussion of this information can help the neuroscience community to devolop their research projects more efficiently and quickly, both by means of readily available tools, and by knowing which modules may be improved, connected or added.",2022-12-19,http://arxiv.org/abs/2301.05057v1,"Louis Fabrice Tshimanga, Manfredo Atzori, Federico Del Pup, Maurizio Corbetta",arxiv.org,"q-bio.QM, cs.LG, cs.NE"
Multilayer Brain Networks,"The field of neuroscience is facing an unprecedented expanse in the volume and diversity of available data. Traditionally, network models have provided key insights into the structure and function of the brain. With the advent of big data in neuroscience, both more sophisticated models capable of characterizing the increasing complexity of the data and novel methods of quantitative analysis are needed. Recently multilayer networks, a mathematical extension of traditional networks, have gained increasing popularity in neuroscience due to their ability to capture the full information of multi-model, multi-scale, spatiotemporal data sets. Here, we review multilayer networks and their applications in neuroscience, showing how incorporating the multilayer framework into network neuroscience analysis has uncovered previously hidden features of brain networks. We specifically highlight the use of multilayer networks to model disease, structure-function relationships, network evolution, and link multi-scale data. Finally, we close with a discussion of promising new directions of multilayer network neuroscience research and propose a modified definition of multilayer networks designed to unite and clarify the use of the multilayer formalism in describing real-world systems.",2017-09-07,http://arxiv.org/abs/1709.02325v1,"Michael Vaiana, Sarah Muldoon",arxiv.org,"q-bio.NC, physics.soc-ph"
An interdisciplinary approach to high school curriculum development:   Swarming Powered by Neuroscience,"This article discusses how to create an interactive virtual training program at the intersection of neuroscience, robotics, and computer science for high school students. A four-day microseminar, titled Swarming Powered by Neuroscience (SPN), was conducted virtually through a combination of presentations and interactive computer game simulations, delivered by subject matter experts in neuroscience, mathematics, multi-agent swarm robotics, and education. The objective of this research was to determine if taking an interdisciplinary approach to high school education would enhance the students learning experiences in fields such as neuroscience, robotics, or computer science. This study found an improvement in student engagement for neuroscience by 16.6%, while interest in robotics and computer science improved respectively by 2.7% and 1.8%. The curriculum materials, developed for the SPN microseminar, can be used by high school teachers to further evaluate interdisciplinary instructions across life and physical sciences and computer science.",2021-09-12,http://arxiv.org/abs/2109.05545v1,"Elise Buckley, Joseph D. Monaco, Kevin M. Schultz, Robert Chalmers, Armin Hadzic, Kechen Zhang, Grace M. Hwang, M. Dwight Carr",arxiv.org,q-bio.NC
How causal perspectives can inform problems in computational   neuroscience,"Over the past two decades, considerable strides have been made in advancing neuroscience techniques, yet the translation of these advancements into clinically relevant insights for human mental health remains a challenge. This review addresses a fundamental issue in neuroscience - attributing causality - and advocates for the development of robust causal frameworks. We systematically introduce the necessary definitions and concepts, emphasizing the implicit role of causal frameworks in neuroscience investigations. We illustrate how persistent challenges in neuroscience, such as batch effects and selection biases, can be conceptualized and approached using causal frameworks. Through theoretical development and real-world examples, we show how these causal perspectives highlight numerous shortcomings of existing data collection strategies and analytical approaches. We demonstrate how causal frameworks can inform both experimental design and analysis, particularly for observational studies where traditional randomization is infeasible. Using neuroimaging as a detailed case study, we explore the advantages, shortcomings, and implications for generalizability that these perspectives afford to existing and novel research paradigms. Together, we believe that this perspective offers a framework for conceptualizing, framing, and inspiring innovative approaches to problems in neuroscience.",2025-03-12,http://arxiv.org/abs/2503.10710v1,"Eric W. Bridgeford, Brian S. Caffo, Maya B. Mathur, Russell A. Poldrack",arxiv.org,"q-bio.OT, stat.OT"
On the nature of explanations offered by network science: A perspective   from and for practicing neuroscientists,"Network neuroscience represents the brain as a collection of regions and inter-regional connections. Given its ability to formalize systems-level models, network neuroscience has generated unique explanations of neural function and behavior. The mechanistic status of these explanations and how they can contribute to and fit within the field of neuroscience as a whole has received careful treatment from philosophers. However, these philosophical contributions have not yet reached many neuroscientists. Here we complement formal philosophical efforts by providing an applied perspective from and for neuroscientists. We discuss the mechanistic status of the explanations offered by network neuroscience and how they contribute to, enhance, and interdigitate with other types of explanations in neuroscience. In doing so, we rely on philosophical work concerning the role of causality, scale, and mechanisms in scientific explanations. In particular, we make the distinction between an explanation and the evidence supporting that explanation, and we argue for a scale-free nature of mechanistic explanations. In the course of these discussions, we hope to provide a useful applied framework in which network neuroscience explanations can be exercised across scales and combined with other fields of neuroscience to gain deeper insights into the brain and behavior.",2019-11-12,http://arxiv.org/abs/1911.05031v1,"Maxwell A. Bertolero, Danielle S. Bassett",arxiv.org,q-bio.NC
Epistemic integration and social segregation of AI in neuroscience,"In recent years, Artificial Intelligence (AI) shows a spectacular ability of insertion inside a variety of disciplines which use it for scientific advancements and which sometimes improve it for their conceptual and methodological needs. According to the transverse science framework originally conceived by Shinn and Joerges, AI can be seen as an instrument which is progressively acquiring a universal character through its diffusion across science. In this paper we address empirically one aspect of this diffusion, namely the penetration of AI into a specific field of research. Taking neuroscience as a case study, we conduct a scientometric analysis of the development of AI in this field. We especially study the temporal egocentric citation network around the articles included in this literature, their represented journals and their authors linked together by a temporal collaboration network. We find that AI is driving the constitution of a particular disciplinary ecosystem in neuroscience which is distinct from other subfields, and which is gathering atypical scientific profiles who are coming from neuroscience or outside it. Moreover we observe that this AI community in neuroscience is socially confined in a specific subspace of the neuroscience collaboration network, which also publishes in a small set of dedicated journals that are mostly active in AI research. According to these results, the diffusion of AI in a discipline such as neuroscience didn't really challenge its disciplinary orientations but rather induced the constitution of a dedicated socio-cognitive environment inside this field.",2023-10-02,http://arxiv.org/abs/2310.01046v2,"Sylvain Fontaine, Floriana Gargiulo, Michel Dubois, Paola Tubaro",arxiv.org,"physics.soc-ph, cs.SI"
A Philosophical Understanding of Representation for Neuroscience,"Neuroscientists often describe neural activity as a representation of something, or claim to have found evidence for a neural representation. But what do these statements mean? The reasons to call some neural activity a representation and the assumptions that come with this term are not generally made clear from its common uses in neuroscience. Representation is a central concept in philosophy of mind, with a rich history going back to the ancient period. In order to clarify its usage in neuroscience, here we advance a link between the connotations of this term across these disciplines. We draw on a broad range of discourse in philosophy to distinguish three key aspects of representation: correspondence, functional role, and teleology. We argue that each of these aspects are implied by the explanatory role the term plays in neuroscience. However, evidence related to all three aspects is rarely presented or discussed in the course of individual studies that aim to identify representations. Overlooking the significance of all three aspects hinders communication in neuroscience, as it obscures the limitations of experimental paradigms and conceals gaps in our understanding of the phenomena of primary interest. Working from this three-part view, we discuss how to move toward clearer communication about representations in the brain.",2021-02-12,http://arxiv.org/abs/2102.06592v2,"Ben Baker, Benjamin Lansdell, Konrad Kording",arxiv.org,q-bio.NC
A learning gap between neuroscience and reinforcement learning,"Historically, artificial intelligence has drawn much inspiration from neuroscience to fuel advances in the field. However, current progress in reinforcement learning is largely focused on benchmark problems that fail to capture many of the aspects that are of interest in neuroscience today. We illustrate this point by extending a T-maze task from neuroscience for use with reinforcement learning algorithms, and show that state-of-the-art algorithms are not capable of solving this problem. Finally, we point out where insights from neuroscience could help explain some of the issues encountered.",2021-04-22,http://arxiv.org/abs/2104.10995v3,"Samuel T. Wauthier, Pietro Mazzaglia, Ozan Çatal, Cedric De Boom, Tim Verbelen, Bart Dhoedt",arxiv.org,"cs.LG, cs.AI"
Recommendations for repositories and scientific gateways from a   neuroscience perspective,"Digital services such as repositories and science gateways have become key resources for the neuroscience community, but users often have a hard time orienting themselves in the service landscape to find the best fit for their particular needs. INCF (International Neuroinformatics Coordinating Facility) has developed a set of recommendations and associated criteria for choosing or setting up and running a repository or scientific gateway, intended for the neuroscience community, with a FAIR neuroscience perspective. These recommendations have neurosciences as their primary use case but are often general. Considering the perspectives of researchers and providers of repositories as well as scientific gateways, the recommendations harmonize and complement existing work on criteria for repositories and best practices. The recommendations cover a range of important areas including accessibility, licensing, community responsibility and technical and financial sustainability of a service.",2022-01-03,http://arxiv.org/abs/2201.00727v1,"Malin Sandström, Mathew Abrams, Jan Bjaalie, Mona Hicks, David Kennedy, Arvind Kumar, JB Poline, Prasun Roy, Paul Tiesinga, Thomas Wachtler, Wojtek Goscinski",arxiv.org,"cs.CY, cs.DL, q-bio.NC"
A perspective on neuroscience data standardization with Neurodata   Without Borders,"Neuroscience research has evolved to generate increasingly large and complex experimental data sets, and advanced data science tools are taking on central roles in neuroscience research. Neurodata Without Borders (NWB), a standard language for neurophysiology data, has recently emerged as a powerful solution for data management, analysis, and sharing. We here discuss our efforts to implement NWB data science pipelines. We describe general principles and specific use cases that illustrate successes, challenges, and non-trivial decisions in software engineering. We hope that our experience can provide guidance for the neuroscience community and help bridge the gap between experimental neuroscience and data science.",2023-10-06,http://arxiv.org/abs/2310.04317v2,"Andrea Pierré, Tuan Pham, Jonah Pearl, Sandeep Robert Datta, Jason T. Ritt, Alexander Fleischmann",arxiv.org,q-bio.NC
An introduction to reinforcement learning for neuroscience,"Reinforcement learning (RL) has a rich history in neuroscience, from early work on dopamine as a reward prediction error signal (Schultz et al., 1997) to recent work proposing that the brain could implement a form of 'distributional reinforcement learning' popularized in machine learning (Dabney et al., 2020). There has been a close link between theoretical advances in reinforcement learning and neuroscience experiments throughout this literature, and the theories describing the experimental data have therefore become increasingly complex. Here, we provide an introduction and mathematical background to many of the methods that have been used in systems neroscience. We start with an overview of the RL problem and classical temporal difference algorithms, followed by a discussion of 'model-free', 'model-based', and intermediate RL algorithms. We then introduce deep reinforcement learning and discuss how this framework has led to new insights in neuroscience. This includes a particular focus on meta-reinforcement learning (Wang et al., 2018) and distributional RL (Dabney et al., 2020). Finally, we discuss potential shortcomings of the RL formalism for neuroscience and highlight open questions in the field. Code that implements the methods discussed and generates the figures is also provided.",2023-11-13,http://arxiv.org/abs/2311.07315v3,Kristopher T. Jensen,arxiv.org,"q-bio.NC, cs.LG"
Matching domain experts by training from scratch on domain knowledge,"Recently, large language models (LLMs) have outperformed human experts in predicting the results of neuroscience experiments (Luo et al., 2024). What is the basis for this performance? One possibility is that statistical patterns in that specific scientific literature, as opposed to emergent reasoning abilities arising from broader training, underlie LLMs' performance. To evaluate this possibility, we trained (next word prediction) a relatively small 124M-parameter GPT-2 model on 1.3 billion tokens of domain-specific knowledge. Despite being orders of magnitude smaller than larger LLMs trained on trillions of tokens, small models achieved expert-level performance in predicting neuroscience results. Small models trained on the neuroscience literature succeeded when they were trained from scratch using a tokenizer specifically trained on neuroscience text or when the neuroscience literature was used to finetune a pretrained GPT-2. Our results indicate that expert-level performance may be attained by even small LLMs through domain-specific, auto-regressive training approaches.",2024-05-15,http://arxiv.org/abs/2405.09395v2,"Xiaoliang Luo, Guangzhi Sun, Bradley C. Love",arxiv.org,"q-bio.NC, cs.AI, cs.CL"
A Dynamical Cartography of the Epistemic Diffusion of Artificial   Intelligence in Neuroscience,"Neuroscience and AI have an intertwined history, largely relayed in the literature of both fields. In recent years, due to the engineering orientations of AI research and the monopoly of industry for its large-scale applications, the mutual expansion of neuroscience and AI in fundamental research seems challenged. In this paper, we bring some empirical evidences that, on the contrary, AI and neuroscience are continuing to grow together, but with a pronounced interest in the fields of study related to neurodegenerative diseases since the 1990s. With a temporal knowledge cartography of neuroscience drawn with advanced document embedding techniques, we draw the dynamical shaping of the discipline since the 1970s and identified the conceptual articulation of AI with this particular subfield mentioned before. However, a further analysis of the underlying citation network of the studied corpus shows that the produced AI technologies remain confined in the different subfields and are not transferred from one subfield to another. This invites us to discuss the genericity capability of AI in the context of an intradisciplinary development, especially in the diffusion of its associated metrology.",2025-07-02,http://arxiv.org/abs/2507.01651v1,Sylvain Fontaine,arxiv.org,"cs.DL, physics.soc-ph, q-bio.NC"
NDAI-NeuroMAP: A Neuroscience-Specific Embedding Model for   Domain-Specific Retrieval,"We present NDAI-NeuroMAP, the first neuroscience-domain-specific dense vector embedding model engineered for high-precision information retrieval tasks. Our methodology encompasses the curation of an extensive domain-specific training corpus comprising 500,000 carefully constructed triplets (query-positive-negative configurations), augmented with 250,000 neuroscience-specific definitional entries and 250,000 structured knowledge-graph triplets derived from authoritative neurological ontologies. We employ a sophisticated fine-tuning approach utilizing the FremyCompany/BioLORD-2023 foundation model, implementing a multi-objective optimization framework combining contrastive learning with triplet-based metric learning paradigms. Comprehensive evaluation on a held-out test dataset comprising approximately 24,000 neuroscience-specific queries demonstrates substantial performance improvements over state-of-the-art general-purpose and biomedical embedding models. These empirical findings underscore the critical importance of domain-specific embedding architectures for neuroscience-oriented RAG systems and related clinical natural language processing applications.",2025-07-04,http://arxiv.org/abs/2507.03329v1,"Devendra Patel, Aaditya Jain, Jayant Verma, Divyansh Rajput, Sunil Mahala, Ketki Suresh Khapare, Jayateja Kalla",arxiv.org,cs.AI
On directed information theory and Granger causality graphs,"Directed information theory deals with communication channels with feedback. When applied to networks, a natural extension based on causal conditioning is needed. We show here that measures built from directed information theory in networks can be used to assess Granger causality graphs of stochastic processes. We show that directed information theory includes measures such as the transfer entropy, and that it is the adequate information theoretic framework needed for neuroscience applications, such as connectivity inference problems.",2010-02-07,http://arxiv.org/abs/1002.1446v1,"P. O. Amblard, O. J. J. Michel",arxiv.org,"cs.IT, math.IT"
Speed and accuracy in a visual motion discrimination task as performed   by rats,"We find that rats, like primates and humans, perform better on the random dot motion task when they take more time to respond. We provide evidence that this improvement is due to stimulus integration. Rats increase their response latency modestly as a function of trial difficulty. Rats can modulate response latency more strongly on a trial by trial basis, apparently on the basis of reward-related parameters.",2012-06-01,http://arxiv.org/abs/1206.0311v1,"Pamela Reinagel, Emily Mankin, Adam Calhoun",arxiv.org,q-bio.NC
What can a mathematician do in neuroscience?,"Mammalian brain is one of the most complex objects in the known universe, as it governs every aspect of animal's and human behavior. It is fair to say that we have a very limited knowledge of how the brain operates and functions. Computational Neuroscience is a scientific discipline that attempts to understand and describe the brain in terms of mathematical modeling. This user-friendly review tries to introduce this relatively new field to mathematicians and physicists by showing examples of recent trends. It also discusses briefly future prospects for constructing an integrated theory of brain function.",2014-05-16,http://arxiv.org/abs/1405.4239v1,Jan Karbowski,arxiv.org,q-bio.NC
An Introductory Review of Information Theory in the Context of   Computational Neuroscience,"This paper introduces several fundamental concepts in information theory from the perspective of their origins in engineering. Understanding such concepts is important in neuroscience for two reasons. Simply applying formulae from information theory without understanding the assumptions behind their definitions can lead to erroneous results and conclusions. Furthermore, this century will see a convergence of information theory and neuroscience; information theory will expand its foundations to incorporate more comprehensively biological processes thereby helping reveal how neuronal networks achieve their remarkable information processing abilities.",2011-07-15,http://arxiv.org/abs/1107.2984v1,"Mark D. McDonnell, Shiro Ikeda, Jonathan H. Manton",arxiv.org,"cs.IT, math.IT"
Network neuroscience and the connectomics revolution,"Connectomics and network neuroscience offer quantitative scientific frameworks for modeling and analyzing networks of structurally and functionally interacting neurons, neuronal populations, and macroscopic brain areas. This shift in perspective and emphasis on distributed brain function has provided fundamental insight into the role played by the brain's network architecture in cognition, disease, development, and aging. In this chapter, we review the core concepts of human connectomics at the macroscale. From the construction of networks using functional and diffusion MRI data, to their subsequent analysis using methods from network neuroscience, this review highlights key findings, commonly-used methodologies, and discusses several emerging frontiers in connectomics.",2020-10-04,http://arxiv.org/abs/2010.01591v1,Richard Betzel,arxiv.org,q-bio.NC
Data Processing of Functional Optical Microscopy for Neuroscience,"Functional optical imaging in neuroscience is rapidly growing with the development of new optical systems and fluorescence indicators. To realize the potential of these massive spatiotemporal datasets for relating neuronal activity to behavior and stimuli and uncovering local circuits in the brain, accurate automated processing is increasingly essential. In this review, we cover recent computational developments in the full data processing pipeline of functional optical microscopy for neuroscience data and discuss ongoing and emerging challenges.",2022-01-10,http://arxiv.org/abs/2201.03537v1,"Hadas Benisty, Alexander Song, Gal Mishne, Adam S. Charles",arxiv.org,"eess.IV, q-bio.NC, q-bio.QM"
PiEEG-16 to Measure 16 EEG Channels with Raspberry Pi for Brain-Computer   Interfaces and EEG devices,"This article introduces a cost-effective gateway into the fascinating world of neuroscience: the PIEEG-16, a versatile shield for RaspberryPi designed to measure 16 channels of various biosignals, including EEG (electroencephalography), EMG (electromyography), and ECG (electrocardiography) without any data transfer over the network (Wi-Fi, Bluetooth) and processing and feature ectraction directly on the Raspberry in real-time. This innovative tool opens up new possibilities for neuroscience research and brain-computer interface experiments. By combining the power of RaspberryPi with specialized biosignal measurement capabilities, the PIEEG-16 represents a significant step forward in democratizing neuroscience research and exploration.",2024-09-08,http://arxiv.org/abs/2409.07491v1,Ildar Rakhmatulin,arxiv.org,"eess.SP, q-bio.NC"
Separating minimal from radical embodied cognitive neuroscience,"Mougenot and Matheson (2024) make a compelling case for the development of a mechanistic cognitive neuroscience that is embodied. However, their analysis of extant work under this header plays down important distinctions between ""minimal"" and ""radical"" embodiment. The former remains firmly neurocentric and therefore has limited potential to move the needle in understanding the functional contributions of neural dynamics to cognition in the context of wider organism-environment dynamics.",2024-09-17,http://arxiv.org/abs/2410.01830v1,Matthieu M. de Wit,arxiv.org,q-bio.NC
"BrainKnow -- Extracting, Linking, and Synthesizing Neuroscience   Knowledge","The exponential growth of neuroscience literature presents a significant challenge for researchers seeking to efficiently access and utilize relevant information. To address this issue, we introduce the Brain Knowledge Engine (BrainKnow), an automated system designed to extract, link, and synthesize neuroscience knowledge from scientific publications. BrainKnow constructs a comprehensive knowledge graph encompassing 3,626,931 relationships across 37,011 neuroscience concepts, derived from 1,817,744 articles. This vast repository of knowledge is accessible through a user-friendly web interface, facilitating efficient navigation and data retrieval. BrainKnow employs advanced graph network algorithms, specifically Node2Vec, to enhance knowledge recommendation and visualization. This enables users to explore semantic relationships between concepts, predict potential new relationships, and gain a deeper understanding of the interconnectedness within neuroscience. Additionally, BrainKnow ensures real-time updates by synchronizing with PubMed, providing researchers with access to the most current information. BrainKnow serves as a valuable resource for neuroscience researchers, offering a powerful tool for exploring, synthesizing, and leveraging the vast and complex knowledge base of the field.",2024-03-07,http://arxiv.org/abs/2403.04346v5,"Cunqing Huangfu, Kang Sun, Yi Zeng, Yuwei Wang, Dongsheng Wang, Zizhe Ruan",arxiv.org,"cs.DL, q-bio.NC, 92-04, J.3"
A Bio-Inspired Research Paradigm of Collision Perception Neurons   Enabling Neuro-Robotic Integration: The LGMD Case,"Compared to human vision, locust visual systems excel at rapid and precise collision detection, despite relying on only hundreds of thousands of neurons organized through a few neuropils. This efficiency makes them an attractive model system for developing artificial collision-detecting systems. Specifically, researchers have identified collision-selective neurons in the locust's optic lobe, called lobula giant movement detectors (LGMDs), which respond specifically to approaching objects. Research upon LGMD neurons began in the early 1970s. Initially, due to their large size, these neurons were identified as motion detectors, but their role as looming detectors was recognized over time. Since then, progress in neuroscience, computational modeling of LGMD's visual neural circuits, and LGMD-based robotics have advanced in tandem, each field supporting and driving the others. Today, with a deeper understanding of LGMD neurons, LGMD-based models have significantly improved collision-free navigation in mobile robots including ground and aerial robots. This review highlights recent developments in LGMD research from the perspectives of neuroscience, computational modeling, and robotics. It emphasizes a biologically plausible research paradigm, where insights from neuroscience inform real-world applications, which would in turn validate and advance neuroscience. With strong support from extensive research and growing application demand, this paradigm has reached a mature stage and demonstrates versatility across different areas of neuroscience research, thereby enhancing our understanding of the interconnections between neuroscience, computational modeling, and robotics. Furthermore, this paradigm would shed light upon the modeling and robotic research into other motion-sensitive neurons or neural circuits.",2025-01-06,http://arxiv.org/abs/2501.02982v2,"Ziyan Qin, Jigen Peng, Shigang Yue, Qinbing Fu",arxiv.org,"cs.NE, cs.AI, q-bio.NC"
Brighter than the sun: Powerscape visualizations illustrate power needs   in neuroscience and psychology,"Participant needs to achieve a given power are frequently underestimated. This is particularly problematic when effect sizes are small, such as is common in neuroscience and psychology. We provide tools to make these demands immediately obvious in the form of a powerscape visualization.",2015-12-31,http://arxiv.org/abs/1512.09368v1,Pascal Wallisch,arxiv.org,q-bio.QM
Summary of Information Theoretic Quantities,"Information theory is a practical and theoretical framework developed for the study of communication over noisy channels. Its probabilistic basis and capacity to relate statistical structure to function make it ideally suited for studying information flow in the nervous system. As a framework it has a number of useful properties: it provides a general measure sensitive to any relationship, not only linear effects; its quantities have meaningful units which in many cases allow direct comparison between different experiments; and it can be used to study how much information can be gained by observing neural responses in single experimental trials, rather than in averages over multiple trials. A variety of information theoretic quantities are in common use in neuroscience - including the Shannon entropy, Kullback-Leibler divergence, and mutual information. In this entry, we introduce and define these quantities. Further details on how these quantities can be estimated in practice are provided in the entry ""Estimation of Information-Theoretic Quantities"" and examples of application of these techniques in neuroscience can be found in the entry ""Applications of Information-Theoretic Quantities in Neuroscience"".",2015-01-08,http://arxiv.org/abs/1501.01854v1,"Robin A. A. Ince, Stefano Panzeri, Simon R. Schultz",arxiv.org,q-bio.NC
Detecting and Tracking The Real-time Hot Topics: A Study on   Computational Neuroscience,"In this study, following the idea of our previous paper (Wang, et al., 2013a), we improve the method to detect and track hot topics in a specific field by using the real-time article usage data. With the ""usage count"" data provided by Web of Science, we take the field of computational neuroscience as an example to make analysis. About 10 thousand articles in the field of Computational Neuroscience are queried in Web of Science, when the records, including the usage count data of each paper, have been harvested and updated weekly from October 19, 2015 to March 21, 2016. The hot topics are defined by the most frequently used keywords aggregated from the articles. The analysis reveals that hot topics in Computational Neuroscience are related to the key technologies, like ""fmri"", ""eeg"", ""erp"", etc. Furthermore, using the weekly updated data, we track the dynamical changes of the topics. The characteristic of immediacy of usage data makes it possible to track the ""heat"" of hot topics timely and dynamically.",2016-08-19,http://arxiv.org/abs/1608.05517v1,"Xianwen Wang, Zhichao Fang",arxiv.org,"cs.DL, cs.IR"
Biological Blueprints for Next Generation AI Systems,"Diverse subfields of neuroscience have enriched artificial intelligence for many decades. With recent advances in machine learning and artificial neural networks, many neuroscientists are partnering with AI researchers and machine learning experts to analyze data and construct models. This paper attempts to demonstrate the value of such collaborations by providing examples of how insights derived from neuroscience research are helping to develop new machine learning algorithms and artificial neural network architectures. We survey the relevant neuroscience necessary to appreciate these insights and then describe how we can translate our current understanding of the relevant neurobiology into algorithmic techniques and architectural designs. Finally, we characterize some of the major challenges facing current AI technology and suggest avenues for overcoming these challenges that draw upon research in developmental and comparative cognitive neuroscience.",2019-12-01,http://arxiv.org/abs/1912.00421v1,"Thomas Dean, Chaofei Fan, Francis E. Lewis, Megumi Sano",arxiv.org,q-bio.NC
"Causality in cognitive neuroscience: concepts, challenges, and   distributional robustness","While probabilistic models describe the dependence structure between observed variables, causal models go one step further: they predict, for example, how cognitive functions are affected by external interventions that perturb neuronal activity. In this review and perspective article, we introduce the concept of causality in the context of cognitive neuroscience and review existing methods for inferring causal relationships from data. Causal inference is an ambitious task that is particularly challenging in cognitive neuroscience. We discuss two difficulties in more detail: the scarcity of interventional data and the challenge of finding the right variables. We argue for distributional robustness as a guiding principle to tackle these problems. Robustness (or invariance) is a fundamental principle underlying causal methodology. A causal model of a target variable generalises across environments or subjects as long as these environments leave the causal mechanisms intact. Consequently, if a candidate model does not generalise, then either it does not consist of the target variable's causes or the underlying variables do not represent the correct granularity of the problem. In this sense, assessing generalisability may be useful when defining relevant variables and can be used to partially compensate for the lack of interventional data.",2020-02-14,http://arxiv.org/abs/2002.06060v2,"Sebastian Weichwald, Jonas Peters",arxiv.org,"q-bio.NC, stat.AP, stat.ME"
Learning to learn online with neuromodulated synaptic plasticity in   spiking neural networks,"We propose that in order to harness our understanding of neuroscience toward machine learning, we must first have powerful tools for training brain-like models of learning. Although substantial progress has been made toward understanding the dynamics of learning in the brain, neuroscience-derived models of learning have yet to demonstrate the same performance capabilities as methods in deep learning such as gradient descent. Inspired by the successes of machine learning using gradient descent, we demonstrate that models of neuromodulated synaptic plasticity from neuroscience can be trained in Spiking Neural Networks (SNNs) with a framework of learning to learn through gradient descent to address challenging online learning problems. This framework opens a new path toward developing neuroscience inspired online learning algorithms.",2022-06-25,http://arxiv.org/abs/2206.12520v2,"Samuel Schmidgall, Joe Hays",arxiv.org,"cs.NE, cs.LG"
Present and future frameworks of theoretical neuroscience: outcomes of a   community discussion,"We organized a workshop on the ""Present and Future Frameworks of Theoretical Neuroscience"", with the support of the National Science Foundation. The objective was to identify the challenges and strategies that this field will need to tackle in order to incorporate vast and multi-scale streams of experimental data from the technologies developed by the BRAIN initiative. The participants, divided in workgroups, identified five key areas that, while not exhaustive, cover multiple aspects of current challenges needed to be developed: Dynamics-statistics; multi-scale integration; coding; brain-body integration; and structure of neuroscience theories. While each area is different, there were coincidences on finding theoretical paths to incorporate biophysics, energetics, and ethology with more abstract coding and computational approaches. Each workgroup has continued to work after the meeting to develop the ideas seeded there, which are started to being published. Here, we provide a perspective of the discussions of each workgroup that point to building on the present foundations of theoretical neuroscience and extend them by incorporating multi-scale information with the objective of providing mechanistic insights into the nervous system.",2020-04-03,http://arxiv.org/abs/2004.01665v1,"Horacio G. Rotstein, Fidel Santamaria",arxiv.org,q-bio.NC
Probing artificial neural networks: insights from neuroscience,"A major challenge in both neuroscience and machine learning is the development of useful tools for understanding complex information processing systems. One such tool is probes, i.e., supervised models that relate features of interest to activation patterns arising in biological or artificial neural networks. Neuroscience has paved the way in using such models through numerous studies conducted in recent decades. In this work, we draw insights from neuroscience to help guide probing research in machine learning. We highlight two important design choices for probes $-$ direction and expressivity $-$ and relate these choices to research goals. We argue that specific research goals play a paramount role when designing a probe and encourage future probing studies to be explicit in stating these goals.",2021-04-16,http://arxiv.org/abs/2104.08197v1,"Anna A. Ivanova, John Hewitt, Noga Zaslavsky",arxiv.org,"cs.LG, cs.CL"
Reinforcement Learning and its Connections with Neuroscience and   Psychology,"Reinforcement learning methods have recently been very successful at performing complex sequential tasks like playing Atari games, Go and Poker. These algorithms have outperformed humans in several tasks by learning from scratch, using only scalar rewards obtained through interaction with their environment. While there certainly has been considerable independent innovation to produce such results, many core ideas in reinforcement learning are inspired by phenomena in animal learning, psychology and neuroscience. In this paper, we comprehensively review a large number of findings in both neuroscience and psychology that evidence reinforcement learning as a promising candidate for modeling learning and decision making in the brain. In doing so, we construct a mapping between various classes of modern RL algorithms and specific findings in both neurophysiological and behavioral literature. We then discuss the implications of this observed relationship between RL, neuroscience and psychology and its role in advancing research in both AI and brain science.",2020-06-25,http://arxiv.org/abs/2007.01099v5,"Ajay Subramanian, Sharad Chitlangia, Veeky Baths",arxiv.org,"cs.LG, q-bio.NC"
Graph Neural Networks in Network Neuroscience,"Noninvasive medical neuroimaging has yielded many discoveries about the brain connectivity. Several substantial techniques mapping morphological, structural and functional brain connectivities were developed to create a comprehensive road map of neuronal activities in the human brain -namely brain graph. Relying on its non-Euclidean data type, graph neural network (GNN) provides a clever way of learning the deep graph structure and it is rapidly becoming the state-of-the-art leading to enhanced performance in various network neuroscience tasks. Here we review current GNN-based methods, highlighting the ways that they have been used in several applications related to brain graphs such as missing brain graph synthesis and disease classification. We conclude by charting a path toward a better application of GNN models in network neuroscience field for neurological disorder diagnosis and population graph integration. The list of papers cited in our work is available at https://github.com/basiralab/GNNs-in-Network-Neuroscience.",2021-06-07,http://arxiv.org/abs/2106.03535v2,"Alaa Bessadok, Mohamed Ali Mahjoub, Islem Rekik",arxiv.org,"cs.LG, q-bio.NC"
Large-scale Foundation Models and Generative AI for BigData Neuroscience,"Recent advances in machine learning have made revolutionary breakthroughs in computer games, image and natural language understanding, and scientific discovery. Foundation models and large-scale language models (LLMs) have recently achieved human-like intelligence thanks to BigData. With the help of self-supervised learning (SSL) and transfer learning, these models may potentially reshape the landscapes of neuroscience research and make a significant impact on the future. Here we present a mini-review on recent advances in foundation models and generative AI models as well as their applications in neuroscience, including natural language and speech, semantic memory, brain-machine interfaces (BMIs), and data augmentation. We argue that this paradigm-shift framework will open new avenues for many neuroscience research directions and discuss the accompanying challenges and opportunities.",2023-10-27,http://arxiv.org/abs/2310.18377v1,"Ran Wang, Zhe Sage Chen",arxiv.org,"q-bio.NC, cs.AI, cs.HC, cs.LG, cs.MM"
Launching Your VR Neuroscience Laboratory,"The proliferation and refinement of affordable virtual reality (VR) technologies and wearable sensors have opened new frontiers in cognitive and behavioral neuroscience. This chapter offers a broad overview of VR for anyone interested in leveraging it as a research tool. In the first section, it examines the fundamental functionalities of VR and outlines important considerations that inform the development of immersive content that stimulates the senses. In the second section, the focus of the discussion shifts to the implementation of VR in the context of the neuroscience lab. Practical advice is offered on adapting commercial, off-theshelf devices to specific research purposes. Further, methods are explored for recording, synchronizing, and fusing heterogeneous forms of data obtained through the VR system or add-on sensors, as well as for labeling events and capturing game play.",2024-05-21,http://arxiv.org/abs/2405.13171v1,"Ying Choon Wu, Christopher Maymon, Jonathon Paden, Weichen Liu",arxiv.org,"cs.HC, q-bio.NC"
A theory of neural emulators,"A central goal in neuroscience is to provide explanations for how animal nervous systems can generate actions and cognitive states such as consciousness while artificial intelligence (AI) and machine learning (ML) seek to provide models that are increasingly better at prediction. Despite many decades of research we have made limited progress on providing neuroscience explanations yet there is an increased use of AI and ML methods in neuroscience for prediction of behavior and even cognitive states. Here we propose emulator theory (ET) and neural emulators as circuit- and scale-independent predictive models of biological brain activity and emulator theory (ET) as an alternative research paradigm in neuroscience. ET proposes that predictive models trained solely on neural dynamics and behaviors can generate functionally indistinguishable systems from their sources. That is, compared to the biological organisms which they model, emulators may achieve indistinguishable behavior and cognitive states - including consciousness - without any mechanistic explanations. We posit ET via several conjectures, discuss the nature of endogenous and exogenous activation of neural circuits, and discuss neural causality of phenomenal states. ET provides the conceptual and empirical framework for prediction-based models of neural dynamics and behavior without explicit representations of idiosyncratically evolved nervous systems.",2024-05-22,http://arxiv.org/abs/2405.13394v1,Catalin C. Mitelut,arxiv.org,"q-bio.NC, cs.AI"
Efficient coding with chaotic neural networks: A journey from   neuroscience to physics and back,"This essay, derived from a lecture at ""The Physics Modeling of Thought"" workshop in Berlin in winter 2023, explores the mutually beneficial relationship between theoretical neuroscience and statistical physics through the lens of efficient coding and computation in cortical circuits. It highlights how the study of neural networks has enhanced our understanding of complex, nonequilibrium, and disordered systems, while also demonstrating how neuroscientific challenges have spurred novel developments in physics. The paper traces the evolution of ideas from seminal work on chaos in random neural networks to recent developments in efficient coding and the partial suppression of chaotic fluctuations. It emphasizes how concepts from statistical physics, such as phase transitions and critical phenomena, have been instrumental in elucidating the computational capabilities of neural networks.   By examining the interplay between order and disorder in neural computation, the essay illustrates the deep connection between theoretical neuroscience and the statistical physics of nonequilibrium systems. This synthesis underscores the ongoing importance of interdisciplinary approaches in advancing both fields, offering fresh perspectives on the fundamental principles governing information processing in biological and artificial systems. This multidisciplinary approach not only advances our understanding of neural computation and complex systems but also points toward future challenges at the intersection of neuroscience and physics.",2024-08-04,http://arxiv.org/abs/2408.01949v1,Jonathan Kadmon,arxiv.org,"q-bio.NC, nlin.CD"
Information thermodynamics: from physics to neuroscience,"This paper provides a perspective on applying the concepts of information thermodynamics, developed recently in non-equilibrium statistical physics, to problems in theoretical neuroscience. Historically, information and energy in neuroscience have been treated separately, in contrast to physics approaches, where the relationship of entropy production with heat is a central idea. It is argued here that also in neural systems information and energy can be considered within the same theoretical framework. Starting from basic ideas of thermodynamics and information theory on a classic Brownian particle, it is shown how noisy neural networks can infer its probabilistic motion. The decoding of the particle motion by neurons is performed with some accuracy and it has some energy cost, and both can be determined using information thermodynamics. In a similar fashion, we also discuss how neural networks in the brain can learn the particle velocity, and maintain that information in the weights of plastic synapses from a physical point of view. Generally, it is shown how the framework of stochastic and information thermodynamics can be used practically to study neural inference, learning, and information storing.",2024-09-26,http://arxiv.org/abs/2409.17599v1,Jan Karbowski,arxiv.org,"q-bio.NC, cond-mat.dis-nn, cond-mat.stat-mech, physics.bio-ph"
Reconstruction of Partial Dissimilarity Matrices for Cognitive   Neuroscience,"In cognitive neuroscience research, Representational Dissimilarity Matrices (RDMs) are often incomplete because pairwise similarity judgments cannot always be exhaustively collected as the number of pairs rapidly increases with the number of conditions. Existing methods to fill these missing values, such as deep neural network imputation, are powerful but computationally demanding and relatively opaque. We introduce a simple algorithm based on geometric inference that fills missing dissimilarity matrix entries using known distances. We use tests on publicly available empirical cognitive neuroscience datasets, as well as simulations, to demonstrate the method's effectiveness and robustness across varying sparsity and matrix sizes. We have made this geometric reconstruction algorithm, implemented in Python and MATLAB, publicly available. This method provides a fast and accurate solution for completing partial dissimilarity matrices in the cognitive neurosciences.",2025-05-31,http://arxiv.org/abs/2506.00484v2,"Denise Moerel, Tijl Grootswagers",arxiv.org,q-bio.NC
Exploring the Human Connectome Topology in Group Studies,"Visually comparing brain networks, or connectomes, is an essential task in the field of neuroscience. Especially relevant to the field of clinical neuroscience, group studies that examine differences between populations or changes over time within a population enable neuroscientists to reason about effective diagnoses and treatments for a range of neuropsychiatric disorders. In this paper, we specifically explore how visual analytics tools can be used to facilitate various clinical neuroscience tasks, in which observation and analysis of meaningful patterns in the connectome can support patient diagnosis and treatment. We conduct a survey of visualization tasks that enable clinical neuroscience activities, and further explore how existing connectome visualization tools support or fail to support these tasks. Based on our investigation of these tasks, we introduce a novel visualization tool, NeuroCave, to support group studies analyses. We discuss how our design decisions (the use of immersive visualization, the use of hierarchical clustering and dimensionality reduction techniques, and the choice of visual encodings) are motivated by these tasks. We evaluate NeuroCave through two use cases that illustrate the utility of interactive connectome visualization in clinical neuroscience contexts. In the first use case, we study sex differences using functional connectomes and discover hidden connectome patterns associated with well-known cognitive differences in spatial and verbal abilities. In the second use case, we show how the utility of visualizing the brain in different topological space coupled with clustering information can reveal the brain's intrinsic structure.",2017-06-30,http://arxiv.org/abs/1706.10297v1,"Johnson J. G. Keiriz, Liang Zhan, Morris Chukhman, Olu Ajilore, Alex D. Leow, Angus G. Forbes",arxiv.org,"q-bio.NC, cs.HC"
Technological Competence is a Precondition for Effective Implementation   of Virtual Reality Head Mounted Displays in Human Neuroscience: A   Technological Review and Meta-analysis,"Immersive virtual reality (VR) emerges as a promising research and clinical tool. However, several studies suggest that VR induced adverse symptoms and effects (VRISE) may undermine the health and safety standards, and the reliability of the scientific results. In the current literature review, the technical reasons for the adverse symptomatology are investigated to provide suggestions and technological knowledge for the implementation of VR head-mounted display (HMD) systems in cognitive neuroscience. The technological systematic literature indicated features pertinent to display, sound, motion tracking, navigation, ergonomic interactions, user experience, and computer hardware that should be considered by the researchers. Subsequently, a meta-analysis of 44 neuroscientific or neuropsychological studies involving VR HMD systems was performed. The meta-analysis of the VR studies demonstrated that new generation HMDs induced significantly less VRISE and marginally fewer dropouts.Importantly, the commercial versions of the new generation HMDs with ergonomic interactions had zero incidents of adverse symptomatology and dropouts. HMDs equivalent to or greater than the commercial versions of contemporary HMDs accompanied with ergonomic interactions are suitable for implementation in cognitive neuroscience. In conclusion, researchers technological competency, along with meticulous methods and reports pertinent to software, hardware, and VRISE, are paramount to ensure the health and safety standards and the reliability of neuroscientific results.",2021-01-20,http://arxiv.org/abs/2101.08123v1,"Panagiotis Kourtesis, Simona Collina, Leonidas A. A. Doumas, Sarah E. MacPherson",arxiv.org,"cs.HC, cs.CY, cs.MM, B.8; C.4; D.0; J.4"
Development of theoretical frameworks in neuroscience: a pressing need   in a sea of data,"Neuroscience is undergoing dramatic progress because of the vast data streams derived from the new technologies product of the BRAIN initiative and other enterprises. As any other scientific field, neuroscience benefits from having clear definitions of its theoretical components and their interactions. This allows generating theories that integrate knowledge, provide mechanistic insights, and predict results under new experimental conditions. However, theoretical neuroscience is a heterogeneous field that has not yet agreed on how to build theories or whether it is desirable to have an overarching theory or whether theories are simply tools to understand the brain. Here we advocate for the need of developing theoretical frameworks as a basis of generating common theoretical structures. We enumerate the elements of theoretical frameworks we deem necessary for any theory in neuroscience. In particular, we address the notions of paradigms, models, and scales of organizations. We then identify areas with pressing needs to develop brain theories: integration of statistical and dynamic approaches; multi-scale integration; coding; and interpretability in the context of Artificial Intelligence. We also point out that future theoretical frameworks would benefit from the incorporation of the principles of Evolution as a fundamental structure rather than purely mathematical or engineering principles. Rather than providing definite answers, the objective of this paper is to serve as an initial and succinct presentation of these topics to encourage discussion and further in depth development of each topic.",2022-09-20,http://arxiv.org/abs/2209.09953v1,"Horacio G. Rotstein, Fidel Santamaria",arxiv.org,"q-bio.NC, q-bio.PE"
"Brain as a complex system, harnessing systems neuroscience tools &   notions for an empirical approach","Finding general principles underlying brain function has been appealing to scientists. Indeed, in some branches of science like physics and chemistry (and to some degree biology) a general theory often can capture the essence of a wide range of phenomena. Whether we can find such principles in neuroscience, and [assuming they do exist] what those principles are, are important questions. Abstracting the brain as a complex system is one of the perspectives that may help us answer this question.   While it is commonly accepted that the brain is a (or even the) prominent example of a complex system, the far reaching implications of this are still arguably overlooked in our approaches to neuroscientific questions. One of the reasons for the lack of attention could be the apparent difference in foci of investigations in these two fields -- neuroscience and complex systems. This thesis is an effort toward providing a bridge between systems neuroscience and complex systems by harnessing systems neuroscience tools & notions for building empirical approaches toward the brain as a complex system.   Perhaps, in the spirit of searching for principles, we should abstract and approach the brain as a complex adaptive system as the more complete perspective (rather than just a complex system). In the end, the brain, even the most ""complex system"", need to survive in the environment. Indeed, in the field of complex adaptive systems, the intention is understanding very similar questions in nature. As an outlook, we also touch on some research directions pertaining to the adaptivity of the brain as well.",2023-12-20,http://arxiv.org/abs/2312.13478v1,Shervin Safavi,arxiv.org,q-bio.NC
Universal Differential Equations as a Common Modeling Language for   Neuroscience,"The unprecedented availability of large-scale datasets in neuroscience has spurred the exploration of artificial deep neural networks (DNNs) both as empirical tools and as models of natural neural systems. Their appeal lies in their ability to approximate arbitrary functions directly from observations, circumventing the need for cumbersome mechanistic modeling. However, without appropriate constraints, DNNs risk producing implausible models, diminishing their scientific value. Moreover, the interpretability of DNNs poses a significant challenge, particularly with the adoption of more complex expressive architectures. In this perspective, we argue for universal differential equations (UDEs) as a unifying approach for model development and validation in neuroscience. UDEs view differential equations as parameterizable, differentiable mathematical objects that can be augmented and trained with scalable deep learning techniques. This synergy facilitates the integration of decades of extensive literature in calculus, numerical analysis, and neural modeling with emerging advancements in AI into a potent framework. We provide a primer on this burgeoning topic in scientific machine learning and demonstrate how UDEs fill in a critical gap between mechanistic, phenomenological, and data-driven models in neuroscience. We outline a flexible recipe for modeling neural systems with UDEs and discuss how they can offer principled solutions to inherent challenges across diverse neuroscience applications such as understanding neural computation, controlling neural systems, neural decoding, and normative modeling.",2024-03-21,http://arxiv.org/abs/2403.14510v1,"Ahmed ElGazzar, Marcel van Gerven",arxiv.org,cs.CE
What Neuroscience Can Teach AI About Learning in Continuously Changing   Environments,"Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.",2025-07-02,http://arxiv.org/abs/2507.02103v1,"Daniel Durstewitz, Bruno Averbeck, Georgia Koppe",arxiv.org,"cs.AI, q-bio.NC, I.2; I.6; A.1"
An analysis of the abstracts presented at the annual meetings of the   Society for Neuroscience from 2001 to 2006,"We extracted and processed abstract data from the SFN annual meeting abstracts during the period 2001-2006, using techniques and software from natural language processing, database management, and data visualization and analysis. An important first step in the process was the application of data cleaning and disambiguation methods to construct a unified database, since the data were too noisy to be of full utility in the raw form initially available. The resulting co-author graph in 2006, for example, had 39,645 nodes (with an estimated 6% error rate in our disambiguation of similar author names) and 13,979 abstracts, with an average of 1.5 abstracts per author, 4.3 authors per abstract, and 5.96 collaborators per author (including all authors on shared abstracts). Recent work in related areas has focused on reputational indices such as highly cited papers or scientists and journal impact factors, and to a lesser extent on creating visual maps of the knowledge space. In contrast, there has been relatively less work on the demographics and community structure, the dynamics of the field over time to examine major research trends and the structure of the sources of research funding. In this paper we examined each of these areas in order to gain an objective overview of contemporary neuroscience. Some interesting findings include a high geographical concentration of neuroscience research in north eastern United States, a surprisingly large transient population (60% of the authors appear in only one out of the six studied years), the central role played by the study of neurodegenerative disorders in the neuroscience community structure, and an apparent growth of behavioral/systems neuroscience with a corresponding shrinkage of cellular/molecular neuroscience over the six year period.",2007-10-12,http://arxiv.org/abs/0710.2523v2,"J. M. Lin, J. W. Bohland, P. Andrews, G. Burns, C. B. Allen, P. P. Mitra",arxiv.org,"physics.data-an, q-bio.NC"
Neurosciences and 6G: Lessons from and Needs of Communicative Brains,"This paper presents the first comprehensive tutorial on a promising research field located at the frontier of two well-established domains: Neurosciences and wireless communications, motivated by the ongoing efforts to define how the sixth generation of mobile networks (6G) will be. In particular, this tutorial first provides a novel integrative approach that bridges the gap between these two, seemingly disparate fields. Then, we present the state-of-the-art and key challenges of these two topics. In particular, we propose a novel systematization that divides the contributions into two groups, one focused on what neurosciences will offer to 6G in terms of new applications and systems architecture (Neurosciences for Wireless), and the other focused on how wireless communication theory and 6G systems can provide new ways to study the brain (Wireless for Neurosciences). For the first group, we concretely explain how current scientific understanding of the brain would enable new application for 6G within the context of a new type of service that we dub braintype communications and that has more stringent requirements than human- and machine-type communication. In this regard, we expose the key requirements of brain-type communication services and we discuss how future wireless networks can be equipped to deal with such services. Meanwhile, for the second group, we thoroughly explore modern communication system paradigms, including Internet of Bio-nano Things and chaosbased communications, in addition to highlighting how complex systems tools can help bridging 6G and neuroscience applications. Brain-controlled vehicles are then presented as our case study. All in all, this tutorial is expected to provide a largely missing articulation between these two emerging fields while delineating concrete ways to move forward in such an interdisciplinary endeavor.",2020-04-04,http://arxiv.org/abs/2004.01834v1,"Renan C. Moioli, Pedro H. J. Nardelli, Michael Taynnan Barros, Walid Saad, Amin Hekmatmanesh, Pedro Gória, Arthur S. de Sena, Merim Dzaferagic, Harun Siljak, Werner van Leekwijck, Dick Carrillo, Steven Latré",arxiv.org,"eess.SP, cs.ET, cs.IT, math.IT, q-bio.NC"
Category Theory and Higher Dimensional Algebra: potential descriptive   tools in neuroscience,"We explain the notion of colimit in category theory as a potential tool for describing structures and their communication, and the notion of higher dimensional algebra as a potential yoga for dealing with processes and processes of processes.",2003-06-13,http://arxiv.org/abs/math/0306223v2,"R. Brown, T. Porter",arxiv.org,math.CT
Beware of the Small-World neuroscientist!,"The SW has undeniably been one of the most popular network descriptors in the neuroscience literature. Two main reasons for its lasting popularity are its apparent ease of computation and the intuitions it is thought to provide on how networked systems operate. Over the last few years, some pitfalls of the SW construct and, more generally, of network summary measures, have widely been acknowledged.",2016-03-01,http://arxiv.org/abs/1603.00200v1,"David Papo, Massimiliano Zanin, Johann H. Martínez, Javier M. Buldú",arxiv.org,"q-bio.NC, physics.soc-ph"
Building machines that adapt and compute like brains,"Building machines that learn and think like humans is essential not only for cognitive science, but also for computational neuroscience, whose ultimate goal is to understand how cognition is implemented in biological brains. A new cognitive computational neuroscience should build cognitive-level and neural- level models, understand their relationships, and test both types of models with both brain and behavioral data.",2017-11-11,http://arxiv.org/abs/1711.04203v1,"Nikolaus Kriegeskorte, Robert M. Mok",arxiv.org,"cs.AI, q-bio.NC"
Before and beyond the Wilson-Cowan equations,"The Wilson-Cowan equations represent a landmark in the history of computational neuroscience. Among the insights Wilson and Cowan offered for neuroscience, they crystallized an approach to modeling neural dynamics and brain function. Although their iconic equations are used in various guises today, the ideas that led to their formulation and the relationship to other approaches are not well known. Here, we give a little context to some of the biological and theoretical concepts that lead to the Wilson-Cowan equations and discuss how to extend beyond them.",2019-07-18,http://arxiv.org/abs/1907.07821v2,"Carson C. Chow, Yahya Karimipanah",arxiv.org,q-bio.NC
Synaptic metaplasticity in binarized neural networks,"Unlike the brain, artificial neural networks, including state-of-the-art deep neural networks for computer vision, are subject to ""catastrophic forgetting"": they rapidly forget the previous task when trained on a new one. Neuroscience suggests that biological synapses avoid this issue through the process of synaptic consolidation and metaplasticity: the plasticity itself changes upon repeated synaptic events. In this work, we show that this concept of metaplasticity can be transferred to a particular type of deep neural networks, binarized neural networks, to reduce catastrophic forgetting.",2021-01-19,http://arxiv.org/abs/2101.07592v1,"Axel Laborieux, Maxence Ernoult, Tifenn Hirtzlin, Damien Querlioz",arxiv.org,cs.NE
A first passage problem for a bivariate diffusion process: numerical   solution with an application to neuroscience,We consider a bivariate diffusion process and we study the first passage time of one component through a boundary. We prove that its probability density is the unique solution of a new integral equation and we propose a numerical algorithm for its solution. Convergence properties of this algorithm are discussed and the method is applied to the study of the integrated Brownian Motion and to the integrated Ornstein Uhlenbeck process. Finally a model of neuroscience interest is also discussed.,2012-04-24,http://arxiv.org/abs/1204.5307v2,"Elisa Benedetto, Laura Sacerdote, Cristina Zucca",arxiv.org,"math.PR, q-bio.NC"
Inadequate experimental methods and erroneous epilepsy diagnostic   criteria result in confounding acquired focal epilepsy with genetic absence   epilepsy,"Here we provide a thorough discussion of the study conducted by Rodgers et al. (J Neurosci. 2015; 35(24):9194-204. doi: 10.1523/JNEUROSCI.0919-15.2015) to investigate focal seizures and acquired epileptogenesis induced by head injury in the rat. This manuscript serves as supplementary document for our letter to the Editor to appear in the Journal of Neuroscience. We find that the subject article suffers from poor experimental design, very selective consideration of antecedent literature, and application of inappropriate epilepsy diagnostic criteria which, together, lead to unwarranted conclusions.",2015-09-03,http://arxiv.org/abs/1509.01206v1,"Raimondo D'Ambrosio, Clifford L. Eastman, John W. Miller",arxiv.org,q-bio.NC
Promise and Pitfalls of Extending Google's PageRank Algorithm to   Citation Networks,"We review our recent work on applying the Google PageRank algorithm to find scientific ""gems"" among all Physical Review publications, and its extension to CiteRank, to find currently popular research directions. These metrics provide a meaningful extension to traditionally-used importance measures, such as the number of citations and journal impact factor. We also point out some pitfalls of over-relying on quantitative metrics to evaluate scientific quality.",2009-01-17,http://arxiv.org/abs/0901.2640v1,"Sergei Maslov, S. Redner",arxiv.org,"physics.soc-ph, physics.data-an"
Deep learning tools for the measurement of animal behavior in   neuroscience,"Recent advances in computer vision have made accurate, fast and robust measurement of animal behavior a reality. In the past years powerful tools specifically designed to aid the measurement of behavior have come to fruition. Here we discuss how capturing the postures of animals - pose estimation - has been rapidly advancing with new deep learning methods. While challenges still remain, we envision that the fast-paced development of new deep learning tools will rapidly change the landscape of realizable real-world neuroscience.",2019-09-30,http://arxiv.org/abs/1909.13868v2,"Mackenzie W. Mathis, Alexander Mathis",arxiv.org,"cs.CV, q-bio.NC, q-bio.QM"
Network Dynamics Governed by Lyapunov Functions: From Memory to   Classification,"In 1982 John Hopfield published a neural network model for memory retrieval, a model that became a cornerstone in theoretical neuroscience. A key ingredient of the Hopfield model was the use of a network dynamics that is governed by a Lyapunov function. In a recent paper, Krotov and Hopfield showed how a Lyapunov function governs a biological plausible learning rule for the neural networks' connectivity. By doing so, they bring an intriguing approach to classification tasks, and show the relevance of the broader framework across decades in the field.",2020-04-17,http://arxiv.org/abs/2004.08091v2,"Merav Stern, Eric Shea-Brown",arxiv.org,"q-bio.NC, cond-mat.dis-nn"
"Predictive Coding, Variational Autoencoders, and Biological Connections","This paper reviews predictive coding, from theoretical neuroscience, and variational autoencoders, from machine learning, identifying the common origin and mathematical framework underlying both areas. As each area is prominent within its respective field, more firmly connecting these areas could prove useful in the dialogue between neuroscience and machine learning. After reviewing each area, we discuss two possible correspondences implied by this perspective: cortical pyramidal dendrites as analogous to (non-linear) deep networks and lateral inhibition as analogous to normalizing flows. These connections may provide new directions for further investigations in each field.",2020-11-15,http://arxiv.org/abs/2011.07464v2,Joseph Marino,arxiv.org,cs.NE
Meta-learning in natural and artificial intelligence,"Meta-learning, or learning to learn, has gained renewed interest in recent years within the artificial intelligence community. However, meta-learning is incredibly prevalent within nature, has deep roots in cognitive science and psychology, and is currently studied in various forms within neuroscience. The aim of this review is to recast previous lines of research in the study of biological intelligence within the lens of meta-learning, placing these works into a common framework. More recent points of interaction between AI and neuroscience will be discussed, as well as interesting new directions that arise under this perspective.",2020-11-26,http://arxiv.org/abs/2011.13464v1,Jane X. Wang,arxiv.org,cs.AI
Overfitting the literature to one set of stimuli and data,"The fast-growing field of Computational Cognitive Neuroscience is on track to meet its first crisis. A large number of papers in this nascent field are developing and testing novel analysis methods using the same stimuli and neuroimaging datasets. Publication bias and confirmatory exploration will result in overfitting to the limited available data. The field urgently needs to collect more good quality open neuroimaging data using a variety of experimental stimuli, to test the generalisability of current published results, and allow for more robust results in future work.",2021-02-19,http://arxiv.org/abs/2102.09729v2,"Tijl Grootswagers, Amanda K Robinson",arxiv.org,q-bio.NC
Coupling Functions in Neuroscience,"The interactions play one of the central roles in the brain mediating various processes and functions. They are particularly important for the brain as a complex system that has many different functions from the same structural connectivity. When studying such neural interactions the coupling functions are very suitable, as inherently they can reveal the underlaying functional mechanism. This chapter overviews some recent and widely used aspects of coupling functions for studying neural interactions. Coupling functions are discussed in connection to two different levels of brain interactions - that of neuron interactions and brainwave cross-frequency interactions. Aspects relevant to this from both, theory and methods, are presented. Although the discussion is based on neuroscience, there are strong implications from, and to, other fields as well.",2020-08-17,http://arxiv.org/abs/2008.07612v1,Tomislav Stankovski,arxiv.org,"nlin.AO, q-bio.NC"
ExBrainable: An Open-Source GUI for CNN-based EEG Decoding and Model   Interpretation,"We have developed a graphic user interface (GUI), ExBrainable, dedicated to convolutional neural networks (CNN) model training and visualization in electroencephalography (EEG) decoding. Available functions include model training, evaluation, and parameter visualization in terms of temporal and spatial representations. We demonstrate these functions using a well-studied public dataset of motor-imagery EEG and compare the results with existing knowledge of neuroscience. The primary objective of ExBrainable is to provide a fast, simplified, and user-friendly solution of EEG decoding for investigators across disciplines to leverage cutting-edge methods in brain/neuroscience research.",2022-01-10,http://arxiv.org/abs/2201.04065v1,"Ya-Lin Huang, Chia-Ying Hsieh, Jian-Xue Huang, Chun-Shu Wei",arxiv.org,"eess.SP, cs.LG, q-bio.NC"
Context sequence theory: a common explanation for multiple types of   learning,"Although principles of neuroscience like reinforcement learning, visual perception and attention have been applied in machine learning models, there is a huge gap between machine learning and mammalian learning. Based on the advances in neuroscience, we propose the context sequence theory to give a common explanation for multiple types of learning in mammals and hope that can provide a new insight into the construct of machine learning models.",2022-07-17,http://arxiv.org/abs/2208.04707v1,"Yu Mingcan, Wang Junying",arxiv.org,"q-bio.NC, cs.AI, cs.LG"
naplib-python: Neural Acoustic Data Processing and Analysis Tools in   Python,"Recently, the computational neuroscience community has pushed for more transparent and reproducible methods across the field. In the interest of unifying the domain of auditory neuroscience, naplib-python provides an intuitive and general data structure for handling all neural recordings and stimuli, as well as extensive preprocessing, feature extraction, and analysis tools which operate on that data structure. The package removes many of the complications associated with this domain, such as varying trial durations and multi-modal stimuli, and provides a general-purpose analysis framework that interfaces easily with existing toolboxes used in the field.",2023-04-04,http://arxiv.org/abs/2304.01799v1,"Gavin Mischler, Vinay Raghavan, Menoua Keshishian, Nima Mesgarani",arxiv.org,"q-bio.NC, q-bio.QM"
Bifurcation and periodic solutions to neuroscience models with a small   parameter,"The existence of periodic solutions is proven for some neuroscience models with a small parameter. Moreover, the stability of such solutions is investigated, as well. The results are based on a theoretical research dealing with the functional differential equation with parameters $$ \dot{x}(t)=L(\tau) x_t + \varepsilon f(t, x_t), $$ where $L: \mathbb{R}_+\rightarrow \mathcal{L}(C; \mathbb{R})$ and $f: \mathbb{R} \times C \rightarrow \mathbb{R}$ are, respectively, linear and nonlinear operators, and $\varepsilon>0$ is a small enough parameter. The theoretical results are applied to a Parkinson's disease model, where the obtained conclusions are illustrated by numerical simulations.",2023-09-12,http://arxiv.org/abs/2309.06398v1,José Oyarce,arxiv.org,"math.DS, 34K13, 34K18, 34C20, 46N60, 92C20"
A scaling limit for additive functionals,"Inspired by models for synchronous spiking activity in neuroscience,   we consider a scaling-limit framework for sequences of strong Markov   processes. Within this framework, we establish the convergence of   certain additive functionals toward L\'evy subordinators, which are   of interest in synchronous input drive modeling in neuronal models.   After proving an abstract theorem in full generality, we provide   detailed and explicit conclusions in the case of reflected   one-dimensional diffusions. Specializing even further, we provide an   in-depth analysis of the limiting behavior of a sequence of   integrated Wright-Fisher diffusions. In neuroscience, such   diffusions serve to parametrize synchrony in doubly-stochastic   models of spiking activity. Additional explicit examples involving   the Feller diffusion and the Brownian motion with drift are also   given.",2024-10-08,http://arxiv.org/abs/2410.06383v1,"Thibaud Taillefumier, Gordan Zitkovic",arxiv.org,"math.PR, 60F17, 92B99, 60J55, 60J60, 60G51"
Human Creativity and AI,"With the advancement of science and technology, the philosophy of creativity has undergone significant reinterpretation. This paper investigates contemporary research in the fields of psychology, cognitive neuroscience, and the philosophy of creativity, particularly in the context of the development of artificial intelligence (AI) techniques. It aims to address the central question: Can AI exhibit creativity? The paper reviews the historical perspectives on the philosophy of creativity and explores the influence of psychological advancements on the study of creativity. Furthermore, it analyzes various definitions of creativity and examines the responses of naturalism and cognitive neuroscience to the concept of creativity.",2025-04-25,http://arxiv.org/abs/2507.08001v1,Shengyi Xie,arxiv.org,"cs.AI, cs.HC"
Why are probabilistic laws governing quantum mechanics and neurobiology?,We address the question: Why are dynamical laws governing in quantum mechanics and in neuroscience of probabilistic nature instead of being deterministic? We discuss some ideas showing that the probabilistic option offers advantages over the deterministic one.,2004-06-14,http://arxiv.org/abs/quant-ph/0406098v1,H. Kroger,arxiv.org,quant-ph
Do we understand the emergent dynamics of grid cell activity?,We examine the qualitative and quantitative properties of continuous attractor networks in explaining the dynamics of grid cells.,2007-08-04,http://arxiv.org/abs/0708.0594v1,"Yoram Burak, Ila R. Fiete",arxiv.org,"q-bio.NC, q-bio.TO"
Notes on Leibniz thought experiment,"Leibniz thought experiment of perception, sensing, and thinking is reconsidered. We try to understand Leibniz picture in view of our knowledge of basic neuroscience. In particular we can see how the emergence of consciousness could in principle be understood.",2013-09-03,http://arxiv.org/abs/1309.0846v1,Markos Maniatis,arxiv.org,q-bio.NC
The OpenPicoAmp : an open-source planar lipid bilayer amplifier for   hands-on learning of neuroscience,"Neuroscience education can be promoted by the availability of low cost and engaging teaching materials. To address this issue, we developed an open-source lipid bilayer amplifier, the OpenPicoAmp, which is appropriate for use in introductory courses in biophysics or neurosciences dealing with the electrical properties of the cell membrane. The amplifier is designed using the common lithographic printed circuit board fabrication process and off-the-shelf electronic components. In addition, we propose a specific design for experimental chambers allowing the insertion of a commercially available polytetrafluoroethylene film. This experimental setup can be used in simple experiments in which students monitor the bilayer formation by capacitance measurement and record unitary currents produced by ionic channels like gramicidin A. Used in combination with a low-cost data acquisition board this system provides a complete solution for hands-on lessons, therefore improving the effectiveness in teaching basic neurosciences or biophysics.",2014-03-28,http://arxiv.org/abs/1403.7439v4,"Vadim Shlyonsky, Freddy Dupuis, David Gall",arxiv.org,"physics.ed-ph, physics.bio-ph, q-bio.NC"
BluePyOpt: Leveraging open source software and cloud infrastructure to   optimise model parameters in neuroscience,"At many scales in neuroscience, appropriate mathematical models take the form of complex dynamical systems. Parametrising such models to conform to the multitude of available experimental constraints is a global nonlinear optimisation problem with a complex fitness landscape, requiring numerical techniques to find suitable approximate solutions. Stochastic optimisation approaches, such as evolutionary algorithms, have been shown to be effective, but often the setting up of such optimisations and the choice of a specific search algorithm and its parameters is non-trivial, requiring domain-specific expertise. Here we describe BluePyOpt, a Python package targeted at the broad neuroscience community to simplify this task. BluePyOpt is an extensible framework for data-driven model parameter optimisation that wraps and standardises several existing open-source tools. It simplifies the task of creating and sharing these optimisations, and the associated techniques and knowledge. This is achieved by abstracting the optimisation and evaluation tasks into various reusable and flexible discrete elements according to established best-practices. Further, BluePyOpt provides methods for setting up both small- and large-scale optimisations on a variety of platforms, ranging from laptops to Linux clusters and cloud-based compute infrastructures. The versatility of the BluePyOpt framework is demonstrated by working through three representative neuroscience specific use cases.",2016-03-01,http://arxiv.org/abs/1603.00500v1,"Werner Van Geit, Michael Gevaert, Giuseppe Chindemi, Christian Rössert, Jean-Denis Courcol, Eilif Muller, Felix Schürmann, Idan Segev, Henry Markram",arxiv.org,q-bio.NC
Brain Network Architecture: Implications for Human Learning,"Human learning is a complex phenomenon that requires adaptive processes across a range of temporal and spacial scales. While our understanding of those processes at single scales has increased exponentially over the last few years, a mechanistic understanding of the entire phenomenon has remained elusive. We propose that progress has been stymied by the lack of a quantitative framework that can account for the full range of neurophysiological and behavioral dynamics both across scales in the systems and also across different types of learning. We posit that network neuroscience offers promise in meeting this challenge. Built on the mathematical fields of complex systems science and graph theory, network neuroscience embraces the interconnected and hierarchical nature of human learning, offering insights into the emergent properties of adaptability. In this review, we discuss the utility of network neuroscience as a tool to build a quantitative framework in which to study human learning, which seeks to explain the full chain of events in the brain from sensory input to motor output, being both biologically plausible and able to make predictions about how an intervention at a single level of the chain may cause alterations in another level of the chain. We close by laying out important remaining challenges in network neuroscience in explicitly bridging spatial scales at which neurophysiological processes occur, and underscore the utility of such a quantitative framework for education and therapy.",2016-09-07,http://arxiv.org/abs/1609.01790v1,"Marcelo G. Mattar, Danielle S. Bassett",arxiv.org,q-bio.NC
Generative Models for Network Neuroscience: Prospects and Promise,"Network neuroscience is the emerging discipline concerned with investigating the complex patterns of interconnections found in neural systems, and to identify principles with which to understand them. Within this discipline, one particularly powerful approach is network generative modeling, in which wiring rules are algorithmically implemented to produce synthetic network architectures with the same properties as observed in empirical network data. Successful models can highlight the principles by which a network is organized and potentially uncover the mechanisms by which it grows and develops. Here we review the prospects and promise of generative models for network neuroscience. We begin with a primer on network generative models, with a discussion of compressibility and predictability, utility in intuiting mechanisms, and a short history on their use in network science broadly. We then discuss generative models in practice and application, paying particular attention to the critical need for cross-validation. Next, we review generative models of biological neural networks, both at the cellular and large-scale level, and across a variety of species including \emph{C. elegans}, \emph{Drosophila}, mouse, rat, cat, macaque, and human. We offer a careful treatment of a few relevant distinctions, including differences between generative models and null models, sufficiency and redundancy, inferring and claiming mechanism, and functional and structural connectivity. We close with a discussion of future directions, outlining exciting frontiers both in empirical data collection efforts as well as in method and theory development that, together, further the utility of the generative network modeling approach for network neuroscience.",2017-08-26,http://arxiv.org/abs/1708.07958v1,"Richard F. Betzel, Danielle S. Bassett",arxiv.org,q-bio.NC
Solved problems and remaining challenges for Granger causality analysis   in neuroscience: A response to Stokes and Purdon (2017),"Granger-Geweke causality (GGC) is a powerful and popular method for identifying directed functional (`causal') connectivity in neuroscience. In a recent paper, Stokes and Purdon [1] raise several concerns about its use. They make two primary claims: (1) that GGC estimates may be severely biased or of high variance, and (2) that GGC fails to reveal the full structural/causal mechanisms of a system. However, these claims rest, respectively, on an incomplete evaluation of the literature, and a misconception about what GGC can be said to measure. Here we explain how existing approaches (as implemented, for example, in our popular MVGC software [2,3]) resolve the first issue, and discuss the frequently-misunderstood distinction between functional and effective neural connectivity which underlies Stokes and Purdon's second claim.   [1] Patrick A. Stokes and Patrick. L. Purdon (2017), A study of problems encountered in Granger causality analysis from a neuroscience perspective, Proc. Natl. Acad. Sci. USA 114(34):7063-7072.   [2] Lionel Barnett and Anil K. Seth (2012), The MVGC Multivariate Granger Causality Matlab toolbox, http://users.sussex.ac.uk/~lionelb/MVGC/   [3] Lionel Barnett and Anil K. Seth (2014), The MVGC multivariate Granger causality toolbox: A new approach to Granger-causal inference, J. Neurosci. Methods 223:50-68",2017-08-26,http://arxiv.org/abs/1708.08001v2,"Lionel Barnett, Adam B. Barrett, Anil K. Seth",arxiv.org,stat.ME
Network models in neuroscience,"From interacting cellular components to networks of neurons and neural systems, interconnected units comprise a fundamental organizing principle of the nervous system. Understanding how their patterns of connections and interactions give rise to the many functions of the nervous system is a primary goal of neuroscience. Recently, this pursuit has begun to benefit from the development of new mathematical tools that can relate a system's architecture to its dynamics and function. These tools, which are known collectively as network science, have been used with increasing success to build models of neural systems across spatial scales and species. Here we discuss the nature of network models in neuroscience. We begin with a review of model theory from a philosophical perspective to inform our view of networks as models of complex systems in general, and of the brain in particular. We then summarize the types of models that are frequently studied in network neuroscience along three primary dimensions: from data representations to first-principles theory, from biophysical realism to functional phenomenology, and from elementary descriptions to coarse-grained approximations. We then consider ways to validate these models, focusing on approaches that perturb a system to probe its function. We close with a description of important frontiers in the construction of network models and their relevance for understanding increasingly complex functions of neural systems.",2018-07-31,http://arxiv.org/abs/1807.11935v1,"Danielle S. Bassett, Perry Zurn, Joshua I. Gold",arxiv.org,q-bio.NC
The extent and drivers of gender imbalance in neuroscience reference   lists,"Like many scientific disciplines, neuroscience has increasingly attempted to confront pervasive gender imbalances within the field. While much of the conversation has centered around publishing and conference participation, recent research in other fields has called attention to the prevalence of gender bias in citation practices. Because of the downstream effects that citations can have on visibility and career advancement, understanding and eliminating gender bias in citation practices is vital for addressing inequity in a scientific community. In this study, we sought to determine whether there is evidence of gender bias in the citation practices of neuroscientists. Using data from five top neuroscience journals, we find that reference lists tend to include more papers with men as first and last author than would be expected if gender were not a factor in referencing. Importantly, we show that this overcitation of men and undercitation of women is driven largely by the citation practices of men, and is increasing over time as the field becomes more diverse. We develop a co-authorship network to assess homophily in researchers' social networks, and we find that men tend to overcite men even when their social networks are representative. We discuss possible mechanisms and consider how individual researchers might address these findings in their own practices.",2020-01-03,http://arxiv.org/abs/2001.01002v2,"Jordan D. Dworkin, Kristin A. Linn, Erin G. Teich, Perry Zurn, Russell T. Shinohara, Danielle S. Bassett",arxiv.org,"cs.SI, cs.DL"
Bridging the neuroscience and physics of time,"As a neuroscientist and a theoretical physicist, both working on time, we have decided to open a direct dialogue to examine if the apparent discrepancies regarding the nature of time can be composed.",2021-09-05,http://arxiv.org/abs/2110.01976v1,"Dean Buonomano, Carlo Rovelli",arxiv.org,"physics.hist-ph, physics.bio-ph"
Brain-Inspired Continual Learning-Robust Feature Distillation and   Re-Consolidation for Class Incremental Learning,"Artificial intelligence (AI) and neuroscience share a rich history, with advancements in neuroscience shaping the development of AI systems capable of human-like knowledge retention. Leveraging insights from neuroscience and existing research in adversarial and continual learning, we introduce a novel framework comprising two core concepts: feature distillation and re-consolidation. Our framework, named Robust Rehearsal, addresses the challenge of catastrophic forgetting inherent in continual learning (CL) systems by distilling and rehearsing robust features. Inspired by the mammalian brain's memory consolidation process, Robust Rehearsal aims to emulate the rehearsal of distilled experiences during learning tasks. Additionally, it mimics memory re-consolidation, where new experiences influence the integration of past experiences to mitigate forgetting. Extensive experiments conducted on CIFAR10, CIFAR100, and real-world helicopter attitude datasets showcase the superior performance of CL models trained with Robust Rehearsal compared to baseline methods. Furthermore, examining different optimization training objectives-joint, continual, and adversarial learning-we highlight the crucial role of feature learning in model performance. This underscores the significance of rehearsing CL-robust samples in mitigating catastrophic forgetting. In conclusion, aligning CL approaches with neuroscience insights offers promising solutions to the challenge of catastrophic forgetting, paving the way for more robust and human-like AI systems.",2024-04-22,http://arxiv.org/abs/2404.14588v1,"Hikmat Khan, Nidhal Carla Bouaynaya, Ghulam Rasool",arxiv.org,"cs.LG, cs.CV"
Report on Candidate Computational Indicators for Conscious Valenced   Experience,"This report enlists 13 functional conditions cashed out in computational terms that have been argued to be constituent of conscious valenced experience. These are extracted from existing empirical and theoretical literature on, among others, animal sentience, medical disorders, anaesthetics, philosophy, evolution, neuroscience, and artificial intelligence.",2024-04-25,http://arxiv.org/abs/2404.16696v1,Andres Campero,arxiv.org,"q-bio.NC, cs.AI"
Real-Time Machine Learning Strategies for a New Kind of Neuroscience   Experiments,"Function and dysfunctions of neural systems are tied to the temporal evolution of neural states. The current limitations in showing their causal role stem largely from the absence of tools capable of probing the brain's internal state in real-time. This gap restricts the scope of experiments vital for advancing both fundamental and clinical neuroscience. Recent advances in real-time machine learning technologies, particularly in analyzing neural time series as nonlinear stochastic dynamical systems, are beginning to bridge this gap. These technologies enable immediate interpretation of and interaction with neural systems, offering new insights into neural computation. However, several significant challenges remain. Issues such as slow convergence rates, high-dimensional data complexities, structured noise, non-identifiability, and a general lack of inductive biases tailored for neural dynamics are key hurdles. Overcoming these challenges is crucial for the full realization of real-time neural data analysis for the causal investigation of neural computation and advanced perturbation based brain machine interfaces. In this paper, we provide a comprehensive perspective on the current state of the field, focusing on these persistent issues and outlining potential paths forward. We emphasize the importance of large-scale integrative neuroscience initiatives and the role of meta-learning in overcoming these challenges. These approaches represent promising research directions that could redefine the landscape of neuroscience experiments and brain-machine interfaces, facilitating breakthroughs in understanding brain function, and treatment of neurological disorders.",2024-09-02,http://arxiv.org/abs/2409.01280v2,"Ayesha Vermani, Matthew Dowling, Hyungju Jeon, Ian Jordan, Josue Nassar, Yves Bernaerts, Yuan Zhao, Steven Van Vaerenbergh, Il Memming Park",arxiv.org,q-bio.NC
NeuroAI for AI Safety,"As AI systems become increasingly powerful, the need for safe AI has become more pressing. Humans are an attractive model for AI safety: as the only known agents capable of general intelligence, they perform robustly even under conditions that deviate significantly from prior experiences, explore the world safely, understand pragmatics, and can cooperate to meet their intrinsic goals. Intelligence, when coupled with cooperation and safety mechanisms, can drive sustained progress and well-being. These properties are a function of the architecture of the brain and the learning algorithms it implements. Neuroscience may thus hold important keys to technical AI safety that are currently underexplored and underutilized. In this roadmap, we highlight and critically evaluate several paths toward AI safety inspired by neuroscience: emulating the brain's representations, information processing, and architecture; building robust sensory and motor systems from imitating brain data and bodies; fine-tuning AI systems on brain data; advancing interpretability using neuroscience methods; and scaling up cognitively-inspired architectures. We make several concrete recommendations for how neuroscience can positively impact AI safety.",2024-11-27,http://arxiv.org/abs/2411.18526v2,"Patrick Mineault, Niccolò Zanichelli, Joanne Zichen Peng, Anton Arkhipov, Eli Bingham, Julian Jara-Ettinger, Emily Mackevicius, Adam Marblestone, Marcelo Mattar, Andrew Payne, Sophia Sanborn, Karen Schroeder, Zenna Tavares, Andreas Tolias, Anthony Zador",arxiv.org,"cs.AI, cs.LG"
Towards Chip-on-Chip Neuroscience: Fast Mining of Frequent Episodes   Using Graphics Processors,"Computational neuroscience is being revolutionized with the advent of multi-electrode arrays that provide real-time, dynamic, perspectives into brain function. Mining event streams from these chips is critical to understanding the firing patterns of neurons and to gaining insight into the underlying cellular activity. We present a GPGPU solution to mining spike trains. We focus on mining frequent episodes which captures coordinated events across time even in the presence of intervening background/""junk"" events. Our algorithmic contributions are two-fold: MapConcatenate, a new computation-to-core mapping scheme, and a two-pass elimination approach to quickly find supported episodes from a large number of candidates. Together, they help realize a real-time ""chip-on-chip"" solution to neuroscience data mining, where one chip (the multi-electrode array) supplies the spike train data and another (the GPGPU) mines it at a scale unachievable previously. Evaluation on both synthetic and real datasets demonstrate the potential of our approach.",2009-05-13,http://arxiv.org/abs/0905.2200v1,"Yong Cao, Debprakash Patnaik, Sean Ponce, Jeremy Archuleta, Patrick Butler, Wu-chun Feng, Naren Ramakrishnan",arxiv.org,"cs.DC, cs.DB"
An Integrated e-science Analysis Base for Computation Neuroscience   Experiments and Analysis,"Recent developments in data management and imaging technologies have significantly affected diagnostic and extrapolative research in the understanding of neurodegenerative diseases. However, the impact of these new technologies is largely dependent on the speed and reliability with which the medical data can be visualised, analysed and interpreted. The EUs neuGRID for Users (N4U) is a follow-on project to neuGRID, which aims to provide an integrated environment to carry out computational neuroscience experiments. This paper reports on the design and development of the N4U Analysis Base and related Information Services, which addresses existing research and practical challenges by offering an integrated medical data analysis environment with the necessary building blocks for neuroscientists to optimally exploit neuroscience workflows, large image datasets and algorithms in order to conduct analyses. The N4U Analysis Base enables such analyses by indexing and interlinking the neuroimaging and clinical study datasets stored on the N4U Grid infrastructure, algorithms and scientific workflow definitions along with their associated provenance information.",2014-02-24,http://arxiv.org/abs/1402.5757v1,"Kamran Munir, Saad Liaquat Kiani, Khawar Hasham, Richard McClatchey, Andrew Branson, Jetendr Shamdasani, the N4U Consortium",arxiv.org,"cs.SE, cs.CE"
Applications of Information Theory to Analysis of Neural Data,"Information theory is a practical and theoretical framework developed for the study of communication over noisy channels. Its probabilistic basis and capacity to relate statistical structure to function make it ideally suited for studying information flow in the nervous system. It has a number of useful properties: it is a general measure sensitive to any relationship, not only linear effects; it has meaningful units which in many cases allow direct comparison between different experiments; and it can be used to study how much information can be gained by observing neural responses in single trials, rather than in averages over multiple trials. A variety of information theoretic quantities are commonly used in neuroscience - (see entry ""Definitions of Information-Theoretic Quantities""). In this entry we review some applications of information theory in neuroscience to study encoding of information in both single neurons and neuronal populations.",2015-01-08,http://arxiv.org/abs/1501.01860v1,"Simon R. Schultz, Robin A. A. Ince, Stefano Panzeri",arxiv.org,q-bio.NC
Mathematical frameworks for oscillatory network dynamics in neuroscience,"The tools of weakly coupled phase oscillator theory have had a profound impact on the neuroscience community, providing insight into a variety of network behaviours ranging from central pattern generation to synchronisation, as well as predicting novel network states such as chimeras. However, there are many instances when this theory is expected to break down, say in the presence of strong coupling, or must be carefully interpreted, as in the presence of stochastic forcing. There are also surprises in the dynamical complexity of the attractors that can robustly appear - for example, heteroclinic network attractors. In this review we present a set of mathematical tools that are suitable for addressing the dynamics of oscillatory neural networks, broadening from a standard phase oscillator perspective to provide a practical framework for further successful applications of mathematics to understanding network dynamics in neuroscience.",2015-06-18,http://arxiv.org/abs/1506.05828v1,"Peter Ashwin, Stephen Coombes, Rachel Nicks",arxiv.org,"nlin.AO, q-bio.NC"
Towards an integration of deep learning and neuroscience,"Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) these cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses.",2016-06-13,http://arxiv.org/abs/1606.03813v1,"Adam Marblestone, Greg Wayne, Konrad Kording",arxiv.org,q-bio.NC
The Future of Data Analysis in the Neurosciences,"Neuroscience is undergoing faster changes than ever before. Over 100 years our field qualitatively described and invasively manipulated single or few organisms to gain anatomical, physiological, and pharmacological insights. In the last 10 years neuroscience spawned quantitative big-sample datasets on microanatomy, synaptic connections, optogenetic brain-behavior assays, and high-level cognition. While growing data availability and information granularity have been amply discussed, we direct attention to a routinely neglected question: How will the unprecedented data richness shape data analysis practices? Statistical reasoning is becoming more central to distill neurobiological knowledge from healthy and pathological brain recordings. We believe that large-scale data analysis will use more models that are non-parametric, generative, mixing frequentist and Bayesian aspects, and grounded in different statistical inferences.",2016-08-05,http://arxiv.org/abs/1608.03465v1,"Danilo Bzdok, B. T. Thomas Yeo",arxiv.org,"q-bio.NC, stat.ML"
Predicting Long-term Outcomes of Educational Interventions Using the   Evolutionary Causal Matrices and Markov Chain Based on Educational   Neuroscience,"We developed a prediction model based on the evolutionary causal matrices (ECM) and the Markov Chain to predict long-term influences of educational interventions on adolescents development. Particularly, we created a computational model predicting longitudinal influences of different types of stories of moral exemplars on adolescents voluntary service participation. We tested whether the developed prediction model can properly predict a long-term longitudinal trend of change in voluntary service participation rate by comparing prediction results and surveyed data. Furthermore, we examined which type of intervention would most effectively promote service engagement and what is the minimum required frequency of intervention to produce a large effect. We discussed the implications of the developed prediction model in educational interventions based on educational neuroscience.",2016-12-01,http://arxiv.org/abs/1612.00129v1,"Hyemin Han, Kangwook Lee, Firat Soylu",arxiv.org,stat.AP
On the interpretability and computational reliability of   frequency-domain Granger causality,"This is a comment to the paper 'A study of problems encountered in Granger causality analysis from a neuroscience perspective'. We agree that interpretation issues of Granger Causality in Neuroscience exist (partially due to the historical unfortunate use of the name 'causality', as nicely described in previous literature). On the other hand we think that the paper uses a formulation of Granger causality which is outdated (albeit still used), and in doing so it dismisses the measure based on a suboptimal use of it. Furthermore, since data from simulated systems are used, the pitfalls that are found with the used formulation are intended to be general, and not limited to neuroscience. It would be a pity if this paper, even written in good faith, became a wildcard against all possible applications of Granger Causality, regardless of the hard work of colleagues aiming to seriously address the methodological and interpretation pitfalls. In order to provide a balanced view, we replicated their simulations used the updated State Space implementation, proposed already some years ago, in which the pitfalls are mitigated or directly solved.",2017-08-23,http://arxiv.org/abs/1708.06990v1,"Luca Faes, Sebastiano Stramaglia, Daniele Marinazzo",arxiv.org,"stat.ME, math.ST, stat.AP, stat.TH"
Network neuroscience for optimizing brain-computer interfaces,"Human-machine interactions are being increasingly explored to create alternative ways of communication and to improve our daily life. Based on a classification of the user's intention from the user's underlying neural activity, brain-computer interfaces (BCIs) allow direct interactions with the external environment while bypassing the traditional effector of the musculoskeletal system. Despite the enormous potential of BCIs, there are still a number of challenges that limit their societal impact, ranging from the correct decoding of a human's thoughts, to the application of effective learning strategies. Despite several important engineering advances, the basic neuroscience behind these challenges remains poorly explored. Indeed, BCIs involve complex dynamic changes related to neural plasticity at a diverse range of spatiotemporal scales. One promising antidote to this complexity lies in network science, which provides a natural language in which to model the organizational principles of brain architecture and function as manifest in its interconnectivity. Here, we briefly review the main limitations currently affecting BCIs, and we offer our perspective on how they can be addressed by means of network theoretic approaches. We posit that the emerging field of network neuroscience will prove to be an effective tool to unlock human-machine interactions.",2018-07-15,http://arxiv.org/abs/1807.05616v1,"Fabrizio De Vico Fallani, Danielle S. Bassett",arxiv.org,q-bio.NC
Cognitive computational neuroscience,"To learn how cognition is implemented in the brain, we must build computational models that can perform cognitive tasks, and test such models with brain and behavioral experiments. Cognitive science has developed computational models of human cognition, decomposing task performance into computational components. However, its algorithms still fall short of human intelligence and are not grounded in neurobiology. Computational neuroscience has investigated how interacting neurons can implement component functions of brain computation. However, it has yet to explain how those components interact to explain human cognition and behavior. Modern technologies enable us to measure and manipulate brain activity in unprecedentedly rich ways in animals and humans. However, experiments will yield theoretical insight only when employed to test brain-computational models. It is time to assemble the pieces of the puzzle of brain computation. Here we review recent work in the intersection of cognitive science, computational neuroscience, and artificial intelligence. Computational models that mimic brain information processing during perceptual, cognitive, and control tasks are beginning to be developed and tested with brain and behavioral data.",2018-07-31,http://arxiv.org/abs/1807.11819v1,"Nikolaus Kriegeskorte, Pamela K. Douglas",arxiv.org,q-bio.NC
Symphony of high-dimensional brain,"This paper is the final part of the scientific discussion organised by the Journal ""Physics of Life Rviews"" about the simplicity revolution in neuroscience and AI. This discussion was initiated by the review paper ""The unreasonable effectiveness of small neural ensembles in high-dimensional brain"". Phys Life Rev 2019, doi 10.1016/j.plrev.2018.09.005, arXiv:1809.07656. The topics of the discussion varied from the necessity to take into account the difference between the theoretical random distributions and ""extremely non-random"" real distributions and revise the common machine learning theory, to different forms of the curse of dimensionality and high-dimensional pitfalls in neuroscience. V. K{\r{u}}rkov{\'a}, A. Tozzi and J.F. Peters, R. Quian Quiroga, P. Varona, R. Barrio, G. Kreiman, L. Fortuna, C. van Leeuwen, R. Quian Quiroga, and V. Kreinovich, A.N. Gorban, V.A. Makarov, and I.Y. Tyukin participated in the discussion. In this paper we analyse the symphony of opinions and the possible outcomes of the simplicity revolution for machine learning and neuroscience.",2019-06-27,http://arxiv.org/abs/1906.12222v1,"Alexander N. Gorban, Valeri A. Makarov, Ivan Y. Tyukin",arxiv.org,"q-bio.NC, cs.AI"
A Framework towards Quantifying Human Restorativeness in Virtual Built   Environments,"The impact of built environment on the human restorativeness has long been argued; however, the interrelations between neuroscience and the built environment, and the degree to which the built environment contributes to increased human restorativeness has not been completely understood yet. Understanding the interrelations between neuroscience and the built environment is critical as 90% of time in a typical day is spent indoors and architectural features impact the productivity, health and comfort of occupants. The goal of this study is to bring a structured understanding of architecture and neuroscience interactions in designed facilities and quantification of the impact of design on human experience. The authors first built two virtual environments (i.e., restorative and non-restorative) using the architectural designs features related to human restorativeness identified by previous research efforts. Next, user experiments were conducted in the two built virtual environments including 22 people. The subjects were asked to conduct navigational tasks while their bodily responses recorded by body area sensors (e.g., EEG, GSR, and Eye-tracking). The result showed that human responses in restorative and non-restorative environment had statistically significant difference. This study serves as the first step of understanding human responses in the virtual environment, and designing spaces that maximize human experience.",2019-02-14,http://arxiv.org/abs/1902.05208v1,"Zhengbo Zou, Semiha Ergan",arxiv.org,cs.CY
Deep Learning for Cognitive Neuroscience,"Neural network models can now recognise images, understand text, translate languages, and play many human games at human or superhuman levels. These systems are highly abstracted, but are inspired by biological brains and use only biologically plausible computations. In the coming years, neural networks are likely to become less reliant on learning from massive labelled datasets, and more robust and generalisable in their task performance. From their successes and failures, we can learn about the computational requirements of the different tasks at which brains excel. Deep learning also provides the tools for testing cognitive theories. In order to test a theory, we need to realise the proposed information-processing system at scale, so as to be able to assess its feasibility and emergent behaviours. Deep learning allows us to scale up from principles and circuit models to end-to-end trainable models capable of performing complex tasks. There are many levels at which cognitive neuroscientists can use deep learning in their work, from inspiring theories to serving as full computational models. Ongoing advances in deep learning bring us closer to understanding how cognition and perception may be implemented in the brain -- the grand challenge at the core of cognitive neuroscience.",2019-03-04,http://arxiv.org/abs/1903.01458v1,"Katherine R. Storrs, Nikolaus Kriegeskorte",arxiv.org,"q-bio.NC, cs.LG"
The Ikshana Hypothesis of Human Scene Understanding,"In recent years, deep neural networks (DNNs) achieved state-of-the-art performance on several computer vision tasks. However, the one typical drawback of these DNNs is the requirement of massive labeled data. Even though few-shot learning methods address this problem, they often use techniques such as meta-learning and metric-learning on top of the existing methods. In this work, we address this problem from a neuroscience perspective by proposing a hypothesis named Ikshana, which is supported by several findings in neuroscience. Our hypothesis approximates the refining process of conceptual gist in the human brain while understanding a natural scene/image. While our hypothesis holds no particular novelty in neuroscience, it provides a novel perspective for designing DNNs for vision tasks. By following the Ikshana hypothesis, we design a novel neural-inspired CNN architecture named IkshanaNet. The empirical results demonstrate the effectiveness of our method by outperforming several baselines on the entire and subsets of the Cityscapes and the CamVid semantic segmentation benchmarks.",2021-01-21,http://arxiv.org/abs/2101.10837v4,Venkata Satya Sai Ajay Daliparthi,arxiv.org,cs.CV
A Pilot Study Exploring Spreadsheet Risk in Scientific Research,"This paper discusses the risks and potential impacts of spreadsheet errors in scientific research data in a Neuroscience research centre in the UK.   Spreadsheets usage in neuroscience, or indeed any medical discipline, is a largely unreported area of spreadsheet research. This paper presents a case study exploring the possible risks and impacts of spreadsheet errors in the neuroscience research centre at the University of Newcastle. Data was collected using an online questionnaire with 17 participants and two detailed semi-structured interviews.   The analysis highlights that errors in research data may lead to severe impacts such as misleading science and damaged personal and organisational reputations. In addition, many risks factors arise from using spreadsheets such as inadequate design and a lack of training.   Spreadsheets are used widely in business and the impacts and risks in these fields have been studied and highlighted in detail. However, scientific research and spreadsheets have also a significant relationship that has not been clarified. The paper also draws out the similarities in spreadsheet practice between the scientific and business communities.",2017-03-23,http://arxiv.org/abs/1703.09785v1,"Ghada AlTarawneh, Simon Thorne",arxiv.org,cs.CY
Understanding Attention: In Minds and Machines,"Attention is a complex and broad concept, studied across multiple disciplines spanning artificial intelligence, cognitive science, psychology, neuroscience, and related fields. Although many of the ideas regarding attention do not significantly overlap among these fields, there is a common theme of adaptive control of limited resources. In this work, we review the concept and variants of attention in artificial neural networks (ANNs). We also discuss the origin of attention from the neuroscience point of view parallel to that of ANNs. Instead of having seemingly disconnected dialogues between varied disciplines, we suggest grounding the ideas on common conceptual frameworks for a systematic analysis of attention and towards possible unification of ideas in AI and Neuroscience.",2020-12-04,http://arxiv.org/abs/2012.02659v1,"Shriraj P. Sawant, Shruti Singh",arxiv.org,"cs.AI, cs.LG, cs.NE"
Morphological Computation and Learning to Learn In Natural Intelligent   Systems And AI,"At present, artificial intelligence in the form of machine learning is making impressive progress, especially the field of deep learning (DL) [1]. Deep learning algorithms have been inspired from the beginning by nature, specifically by the human brain, in spite of our incomplete knowledge about its brain function. Learning from nature is a two-way process as discussed in [2][3][4], computing is learning from neuroscience, while neuroscience is quickly adopting information processing models. The question is, what can the inspiration from computational nature at this stage of the development contribute to deep learning and how much models and experiments in machine learning can motivate, justify and lead research in neuroscience and cognitive science and to practical applications of artificial intelligence.",2020-04-05,http://arxiv.org/abs/2004.02304v1,Gordana Dodig-Crnkovic,arxiv.org,cs.AI
"If deep learning is the answer, then what is the question?","Neuroscience research is undergoing a minor revolution. Recent advances in machine learning and artificial intelligence (AI) research have opened up new ways of thinking about neural computation. Many researchers are excited by the possibility that deep neural networks may offer theories of perception, cognition and action for biological brains. This perspective has the potential to radically reshape our approach to understanding neural systems, because the computations performed by deep networks are learned from experience, not endowed by the researcher. If so, how can neuroscientists use deep networks to model and understand biological brains? What is the outlook for neuroscientists who seek to characterise computations or neural codes, or who wish to understand perception, attention, memory, and executive functions? In this Perspective, our goal is to offer a roadmap for systems neuroscience research in the age of deep learning. We discuss the conceptual and methodological challenges of comparing behaviour, learning dynamics, and neural representation in artificial and biological systems. We highlight new research questions that have emerged for neuroscience as a direct consequence of recent advances in machine learning.",2020-04-16,http://arxiv.org/abs/2004.07580v2,"Andrew Saxe, Stephanie Nelli, Christopher Summerfield",arxiv.org,q-bio.NC
Thermodynamic Formalism in Neuronal Dynamics and Spike Train Statistics,"The Thermodynamic Formalism provides a rigorous mathematical framework to study quantitative and qualitative aspects of dynamical systems. At its core there is a variational principle corresponding, in its simplest form, to the Maximum Entropy principle. It is used as a statistical inference procedure to represent, by specific probability measures (Gibbs measures), the collective behaviour of complex systems. This framework has found applications in different domains of science. In particular, it has been fruitful and influential in neurosciences. In this article, we review how the Thermodynamic Formalism can be exploited in the field of theoretical neuroscience, as a conceptual and operational tool, to link the dynamics of interacting neurons and the statistics of action potentials from either experimental data or mathematical models. We comment on perspectives and open problems in theoretical neuroscience that could be addressed within this formalism.",2020-11-18,http://arxiv.org/abs/2011.09512v1,"Rodrigo Cofré, Cesar Maldonado, Bruno Cessac",arxiv.org,"q-bio.NC, math-ph, math.MP, nlin.CD"
The population doctrine in cognitive neuroscience,"A major shift is happening within neurophysiology: a population doctrine is drawing level with the single-neuron doctrine that has long dominated the field. Population-level ideas have so far had their greatest impact in motor neuroscience, but they hold great promise for resolving open questions in cognition as well. Here, we codify the population doctrine and survey recent work that leverages this view to specifically probe cognition. Our discussion is organized around five core concepts that provide a foundation for population-level thinking: (1) state spaces, (2) manifolds, (3) coding dimensions, (4) subspaces, and (5) dynamics. The work we review illustrates the progress and promise that population neurophysiology holds for cognitive neuroscience$-$for delivering new insight into attention, working memory, decision-making, executive function, learning, and reward processing.",2021-03-31,http://arxiv.org/abs/2104.00145v2,"R. Becket Ebitz, Benjamin Y. Hayden",arxiv.org,q-bio.NC
Explanatory models in neuroscience: Part 2 -- constraint-based   intelligibility,"Computational modeling plays an increasingly important role in neuroscience, highlighting the philosophical question of how computational models explain. In the context of neural network models for neuroscience, concerns have been raised about model intelligibility, and how they relate (if at all) to what is found in the brain. We claim that what makes a system intelligible is an understanding of the dependencies between its behavior and the factors that are causally responsible for that behavior. In biological systems, many of these dependencies are naturally ""top-down"": ethological imperatives interact with evolutionary and developmental constraints under natural selection. We describe how the optimization techniques used to construct NN models capture some key aspects of these dependencies, and thus help explain why brain systems are as they are -- because when a challenging ecologically-relevant goal is shared by a NN and the brain, it places tight constraints on the possible mechanisms exhibited in both kinds of systems. By combining two familiar modes of explanation -- one based on bottom-up mechanism (whose relation to neural network models we address in a companion paper) and the other on top-down constraints, these models illuminate brain function.",2021-04-03,http://arxiv.org/abs/2104.01489v2,"Rosa Cao, Daniel Yamins",arxiv.org,"q-bio.NC, cs.NE"
The brain is a computer is a brain: neuroscience's internal debate and   the social significance of the Computational Metaphor,"The Computational Metaphor, comparing the brain to the computer and vice versa, is the most prominent metaphor in neuroscience and artificial intelligence (AI). Its appropriateness is highly debated in both fields, particularly with regards to whether it is useful for the advancement of science and technology. Considerably less attention, however, has been devoted to how the Computational Metaphor is used outside of the lab, and particularly how it may shape society's interactions with AI. As such, recently publicized concerns over AI's role in perpetuating racism, genderism, and ableism suggest that the term ""artificial intelligence"" is misplaced, and that a new lexicon is needed to describe these computational systems. Thus, there is an essential question about the Computational Metaphor that is rarely asked by neuroscientists: whom does it help and whom does it harm? This essay invites the neuroscience community to consider the social implications of the field's most controversial metaphor.",2021-07-18,http://arxiv.org/abs/2107.14042v1,"Alexis T. Baria, Keith Cross",arxiv.org,"cs.CY, cs.AI"
"Time, Frequency & Time-Varying Causality Measures in Neuroscience","This article proposes a systematic methodological review and objective criticism of existing methods enabling the derivation of time-varying Granger-causality statistics in neuroscience. The increasing interest and the huge number of publications related to this topic calls for this systematic review which describes the very complex methodological aspects. The capacity to describe the causal links between signals recorded at different brain locations during a neuroscience experiment is of primary interest for neuroscientists, who often have very precise prior hypotheses about the relationships between recorded brain signals that arise at a specific time and in a specific frequency band. The ability to compute a time-varying frequency-specific causality statistic is therefore essential. Two steps are necessary to achieve this: the first consists of finding a statistic that can be interpreted and that directly answers the question of interest. The second concerns the model that underlies the causality statistic and that has this time-frequency specific causality interpretation. In this article, we will review Granger-causality statistics with their spectral and time-varying extensions.",2017-04-11,http://arxiv.org/abs/1704.03177v1,"Sezen Cekic, Didier Grandjean, Olivier Renaud",arxiv.org,stat.AP
Generalizable Machine Learning in Neuroscience using Graph Neural   Networks,"Although a number of studies have explored deep learning in neuroscience, the application of these algorithms to neural systems on a microscopic scale, i.e. parameters relevant to lower scales of organization, remains relatively novel. Motivated by advances in whole-brain imaging, we examined the performance of deep learning models on microscopic neural dynamics and resulting emergent behaviors using calcium imaging data from the nematode C. elegans. We show that neural networks perform remarkably well on both neuron-level dynamics prediction, and behavioral state classification. In addition, we compared the performance of structure agnostic neural networks and graph neural networks to investigate if graph structure can be exploited as a favorable inductive bias. To perform this experiment, we designed a graph neural network which explicitly infers relations between neurons from neural activity and leverages the inferred graph structure during computations. In our experiments, we found that graph neural networks generally outperformed structure agnostic models and excel in generalization on unseen organisms, implying a potential path to generalizable machine learning in neuroscience.",2020-10-16,http://arxiv.org/abs/2010.08569v1,"Paul Y. Wang, Sandalika Sapra, Vivek Kurien George, Gabriel A. Silva",arxiv.org,"cs.LG, q-bio.NC"
Relating transformers to models and neural representations of the   hippocampal formation,"Many deep neural network architectures loosely based on brain networks have recently been shown to replicate neural firing patterns observed in the brain. One of the most exciting and promising novel architectures, the Transformer neural network, was developed without the brain in mind. In this work, we show that transformers, when equipped with recurrent position encodings, replicate the precisely tuned spatial representations of the hippocampal formation; most notably place and grid cells. Furthermore, we show that this result is no surprise since it is closely related to current hippocampal models from neuroscience. We additionally show the transformer version offers dramatic performance gains over the neuroscience version. This work continues to bind computations of artificial and brain networks, offers a novel understanding of the hippocampal-cortical interaction, and suggests how wider cortical areas may perform complex tasks beyond current neuroscience models such as language comprehension.",2021-12-07,http://arxiv.org/abs/2112.04035v2,"James C. R. Whittington, Joseph Warren, Timothy E. J. Behrens",arxiv.org,"cs.NE, cs.LG, q-bio.NC"
Exploring hyper-parameter spaces of neuroscience models on high   performance computers with Learning to Learn,Neuroscience models commonly have a high number of degrees of freedom and only specific regions within the parameter space are able to produce dynamics of interest. This makes the development of tools and strategies to efficiently find these regions of high importance to advance brain research. Exploring the high dimensional parameter space using numerical simulations has been a frequently used technique in the last years in many areas of computational neuroscience. High performance computing (HPC) can provide today a powerful infrastructure to speed up explorations and increase our general understanding of the model's behavior in reasonable times.,2022-02-28,http://arxiv.org/abs/2202.13822v1,"Alper Yegenoglu, Anand Subramoney, Thorsten Hater, Cristian Jimenez-Romero, Wouter Klijn, Aaron Perez Martin, Michiel van der Vlag, Michael Herty, Abigail Morrison, Sandra Diaz-Pier",arxiv.org,cs.NE
"""Task-relevant autoencoding"" enhances machine learning for human   neuroscience","In human neuroscience, machine learning can help reveal lower-dimensional neural representations relevant to subjects' behavior. However, state-of-the-art models typically require large datasets to train, so are prone to overfitting on human neuroimaging data that often possess few samples but many input dimensions. Here, we capitalized on the fact that the features we seek in human neuroscience are precisely those relevant to subjects' behavior. We thus developed a Task-Relevant Autoencoder via Classifier Enhancement (TRACE), and tested its ability to extract behaviorally-relevant, separable representations compared to a standard autoencoder, a variational autoencoder, and principal component analysis for two severely truncated machine learning datasets. We then evaluated all models on fMRI data from 59 subjects who observed animals and objects. TRACE outperformed all models nearly unilaterally, showing up to 12% increased classification accuracy and up to 56% improvement in discovering ""cleaner"", task-relevant representations. These results showcase TRACE's potential for a wide variety of data related to human behavior.",2022-08-17,http://arxiv.org/abs/2208.08478v2,"Seyedmehdi Orouji, Vincent Taschereau-Dumouchel, Aurelio Cortese, Brian Odegaard, Cody Cushing, Mouslim Cherkaoui, Mitsuo Kawato, Hakwan Lau, Megan A. K. Peters",arxiv.org,"q-bio.NC, cs.LG"
Implementing engrams from a machine learning perspective: matching for   prediction,"Despite evidence for the existence of engrams as memory support structures in our brains, there is no consensus framework in neuroscience as to what their physical implementation might be. Here we propose how we might design a computer system to implement engrams using neural networks, with the main aim of exploring new ideas using machine learning techniques, guided by challenges in neuroscience. Building on autoencoders, we propose latent neural spaces as indexes for storing and retrieving information in a compressed format. We consider this technique as a first step towards predictive learning: autoencoders are designed to compare reconstructed information with the original information received, providing a kind of predictive ability, which is an attractive evolutionary argument. We then consider how different states in latent neural spaces corresponding to different types of sensory input could be linked by synchronous activation, providing the basis for a sparse implementation of memory using concept neurons. Finally, we list some of the challenges and questions that link neuroscience and data science and that could have implications for both fields, and conclude that a more interdisciplinary approach is needed, as many scientists have already suggested.",2023-03-01,http://arxiv.org/abs/2303.01253v1,Jesus Marco de Lucas,arxiv.org,"q-bio.NC, cs.AI, I.2.0"
SuperNeuro: A Fast and Scalable Simulator for Neuromorphic Computing,"In many neuromorphic workflows, simulators play a vital role for important tasks such as training spiking neural networks (SNNs), running neuroscience simulations, and designing, implementing and testing neuromorphic algorithms. Currently available simulators are catered to either neuroscience workflows (such as NEST and Brian2) or deep learning workflows (such as BindsNET). While the neuroscience-based simulators are slow and not very scalable, the deep learning-based simulators do not support certain functionalities such as synaptic delay that are typical of neuromorphic workloads. In this paper, we address this gap in the literature and present SuperNeuro, which is a fast and scalable simulator for neuromorphic computing, capable of both homogeneous and heterogeneous simulations as well as GPU acceleration. We also present preliminary results comparing SuperNeuro to widely used neuromorphic simulators such as NEST, Brian2 and BindsNET in terms of computation times. We demonstrate that SuperNeuro can be approximately 10--300 times faster than some of the other simulators for small sparse networks. On large sparse and large dense networks, SuperNeuro can be approximately 2.2 and 3.4 times faster than the other simulators respectively.",2023-05-04,http://arxiv.org/abs/2305.02510v1,"Prasanna Date, Chathika Gunaratne, Shruti Kulkarni, Robert Patton, Mark Coletti, Thomas Potok",arxiv.org,"cs.NE, cs.ET, D.0"
Multilevel Monte Carlo for a class of Partially Observed Processes in   Neuroscience,"In this paper we consider Bayesian parameter inference associated to a class of partially observed stochastic differential equations (SDE) driven by jump processes. Such type of models can be routinely found in applications, of which we focus upon the case of neuroscience. The data are assumed to be observed regularly in time and driven by the SDE model with unknown parameters. In practice the SDE may not have an analytically tractable solution and this leads naturally to a time-discretization. We adapt the multilevel Markov chain Monte Carlo method of [11], which works with a hierarchy of time discretizations and show empirically and theoretically that this is preferable to using one single time discretization. The improvement is in terms of the computational cost needed to obtain a pre-specified numerical error. Our approach is illustrated on models that are found in neuroscience.",2023-10-10,http://arxiv.org/abs/2310.06533v2,"Mohamed Maama, Ajay Jasra, Kengo Kamatani",arxiv.org,"q-bio.NC, stat.ME"
Efficient gPC-based quantification of probabilistic robustness for   systems in neuroscience,"Robustness analysis is very important in biology and neuroscience, to unravel behavioural patterns of systems that are conserved despite large parametric uncertainties. To make studies of probabilistic robustness more efficient and scalable when addressing complex models in neuroscience, we propose an alternative to computationally expensive Monte Carlo (MC) methods by introducing and analysing the generalised polynomial chaos (gPC) framework for uncertainty quantification. We consider both intrusive and non-intrusive gPC approaches, which turn out to be scalable and allow for a fast comprehensive exploration of parameter spaces. Focusing on widely used models of neural dynamics as case studies, we explore the trade-off between efficiency and accuracy of gPC methods, and we adopt the proposed methodology to investigate parametric uncertainties in models that feature multiple dynamic regimes.",2024-06-19,http://arxiv.org/abs/2406.13489v3,"Uros Sutulovic, Daniele Proverbio, Rami Katz, Giulia Giordano",arxiv.org,q-bio.QM
Enhancing spatial auditory attention decoding with neuroscience-inspired   prototype training,"The spatial auditory attention decoding (Sp-AAD) technology aims to determine the direction of auditory attention in multi-talker scenarios via neural recordings. Despite the success of recent Sp-AAD algorithms, their performance is hindered by trial-specific features in EEG data. This study aims to improve decoding performance against these features. Studies in neuroscience indicate that spatial auditory attention can be reflected in the topological distribution of EEG energy across different frequency bands. This insight motivates us to propose Prototype Training, a neuroscience-inspired method for Sp-AAD. This method constructs prototypes with enhanced energy distribution representations and reduced trial-specific characteristics, enabling the model to better capture auditory attention features. To implement prototype training, an EEGWaveNet that employs the wavelet transform of EEG is further proposed. Detailed experiments indicate that the EEGWaveNet with prototype training outperforms other competitive models on various datasets, and the effectiveness of the proposed method is also validated. As a training method independent of model architecture, prototype training offers new insights into the field of Sp-AAD.",2024-07-09,http://arxiv.org/abs/2407.06498v1,"Zelin Qiu, Jianjun Gu, Dingding Yao, Junfeng Li",arxiv.org,cs.HC
Simulation of Neural Responses to Classical Music Using Organoid   Intelligence Methods,"Music is a complex auditory stimulus capable of eliciting significant changes in brain activity, influencing cognitive processes such as memory, attention, and emotional regulation. However, the underlying mechanisms of music-induced cognitive processes remain largely unknown. Organoid intelligence and deep learning models show promise for simulating and analyzing these neural responses to classical music, an area significantly unexplored in computational neuroscience. Hence, we present the PyOrganoid library, an innovative tool that facilitates the simulation of organoid learning models, integrating sophisticated machine learning techniques with biologically inspired organoid simulations. Our study features the development of the Pianoid model, a ""deep organoid learning"" model that utilizes a Bidirectional LSTM network to predict EEG responses based on audio features from classical music recordings. This model demonstrates the feasibility of using computational methods to replicate complex neural processes, providing valuable insights into music perception and cognition. Likewise, our findings emphasize the utility of synthetic models in neuroscience research and highlight the PyOrganoid library's potential as a versatile tool for advancing studies in neuroscience and artificial intelligence.",2024-07-25,http://arxiv.org/abs/2407.18413v1,Daniel Szelogowski,arxiv.org,"cs.NE, cs.AI, cs.LG, cs.SD, eess.AS, I.2; I.6; J.3; J.4; J.5"
Expansion microscopy reveals neural circuit organization in genetic   animal models,"Expansion Microscopy is a super-resolution technique in which physically enlarging samples in an isotropic manner increases inter-molecular distances such that nano-scale structures can be resolved using light microscopy. This is particularly useful in neuroscience as many important structures are smaller than the diffraction limit. Since its invention in 2015, a variety of Expansion Microscopy protocols have been generated and applied to advance knowledge in many prominent organisms in neuroscience, including zebrafish, mice, Drosophila, and C. elegans. Here we review the last decade of Expansion Microscopy-enabled advances with a focus on neuroscience.",2024-11-11,http://arxiv.org/abs/2411.06676v1,"Shakila Behzadi, Jacquelin Ho, Zainab Tanvir, Gal Haspel, Limor Freifeld, Kristen E. Severi",arxiv.org,q-bio.NC
Modality-Driven Design for Multi-Step Dexterous Manipulation: Insights   from Neuroscience,"Multi-step dexterous manipulation is a fundamental skill in household scenarios, yet remains an underexplored area in robotics. This paper proposes a modular approach, where each step of the manipulation process is addressed with dedicated policies based on effective modality input, rather than relying on a single end-to-end model. To demonstrate this, a dexterous robotic hand performs a manipulation task involving picking up and rotating a box. Guided by insights from neuroscience, the task is decomposed into three sub-skills, 1)reaching, 2)grasping and lifting, and 3)in-hand rotation, based on the dominant sensory modalities employed in the human brain. Each sub-skill is addressed using distinct methods from a practical perspective: a classical controller, a Vision-Language-Action model, and a reinforcement learning policy with force feedback, respectively. We tested the pipeline on a real robot to demonstrate the feasibility of our approach. The key contribution of this study lies in presenting a neuroscience-inspired, modality-driven methodology for multi-step dexterous manipulation.",2024-12-15,http://arxiv.org/abs/2412.11337v1,"Naoki Wake, Atsushi Kanehira, Daichi Saito, Jun Takamatsu, Kazuhiro Sasabuchi, Hideki Koike, Katsushi Ikeuchi",arxiv.org,"cs.RO, cs.AI, cs.CV"
Deep reinforcement learning with time-scale invariant memory,"The ability to estimate temporal relationships is critical for both animals and artificial agents. Cognitive science and neuroscience provide remarkable insights into behavioral and neural aspects of temporal credit assignment. In particular, scale invariance of learning dynamics, observed in behavior and supported by neural data, is one of the key principles that governs animal perception: proportional rescaling of temporal relationships does not alter the overall learning efficiency. Here we integrate a computational neuroscience model of scale invariant memory into deep reinforcement learning (RL) agents. We first provide a theoretical analysis and then demonstrate through experiments that such agents can learn robustly across a wide range of temporal scales, unlike agents built with commonly used recurrent memory architectures such as LSTM. This result illustrates that incorporating computational principles from neuroscience and cognitive science into deep neural networks can enhance adaptability to complex temporal dynamics, mirroring some of the core properties of human learning.",2024-12-19,http://arxiv.org/abs/2412.15292v1,"Md Rysul Kabir, James Mochizuki-Freeman, Zoran Tiganj",arxiv.org,"cs.AI, cs.LG"
Multilayer Networks in Neuroimaging,"Recent advances in network science, applied to \textit{in vivo} brain recordings, have paved the way for better understanding of the structure and function of the brain. However, despite its obvious usefulness in neuroscience, traditional network science lacks tools for -- so important -- simultaneous investigation of the inter-relationship between the two domains. In this chapter, I explore the increasing role of multilayer networks in building brain generative models and abilities of such models to uncover the full information about the brain complex spatiotemporal interactions that span across multiple scales and modalities. First, I begin with the theoretical foundation of brain networks accompanied by a brief overview of traditional networks and their role in constructing multilayer network models. Then, I delve into the applications of multilayer networks in neuroscience, particularly in deciphering structure-function relationship, modelling diseases, and integrating multi-scale and multi-modal data. Finally, I demonstrate how incorporating the multilayer framework into network neuroscience has brought to light previously hidden features of brain networks and, how multilayer networks can provide new insights and a description of the structure and function of the brain.",2025-01-31,http://arxiv.org/abs/2501.19024v1,Vesna Vuksanovic,arxiv.org,"q-bio.NC, q-bio.QM"
ExKG-LLM: Leveraging Large Language Models for Automated Expansion of   Cognitive Neuroscience Knowledge Graphs,"The paper introduces ExKG-LLM, a framework designed to automate the expansion of cognitive neuroscience knowledge graphs (CNKG) using large language models (LLMs). It addresses limitations in existing tools by enhancing accuracy, completeness, and usefulness in CNKG. The framework leverages a large dataset of scientific papers and clinical reports, applying state-of-the-art LLMs to extract, optimize, and integrate new entities and relationships. Evaluation metrics include precision, recall, and graph density. Results show significant improvements: precision (0.80, +6.67%), recall (0.81, +15.71%), F1 score (0.805, +11.81%), and increased edge nodes (21.13% and 31.92%). Graph density slightly decreased, reflecting a broader but more fragmented structure. Engagement rates rose by 20%, while CNKG diameter increased to 15, indicating a more distributed structure. Time complexity improved to O(n log n), but space complexity rose to O(n2), indicating higher memory usage. ExKG-LLM demonstrates potential for enhancing knowledge generation, semantic search, and clinical decision-making in cognitive neuroscience, adaptable to broader scientific fields.",2025-03-09,http://arxiv.org/abs/2503.06479v1,"Ali Sarabadani, Kheirolah Rahsepar Fard, Hamid Dalvand",arxiv.org,cs.AI
Comparing Retrieval Strategies to Capture Interdisciplinary Scientific   Research: A Bibliometric Evaluation of the Integration of Neuroscience and   Computer Science,"Interdisciplinary scientific research is increasingly important in knowledge production, funding policies, and academic discussions on scholarly communication. While many studies focus on interdisciplinary corpora defined a priori - usually through keyword-based searches within assumed interdisciplinary domains - few explore interdisciplinarity as an emergent intersection between two distinct fields. Thus, methodological proposals for building databases at the intersection of two fields of knowledge are scarce. The goal of this article is to develop and compare different strategies for defining an interdisciplinary corpus between two bodies of knowledge. As a case study, we focus on the intersection between neuroscience and computer science. To this end, we develop and compare four retrieval strategies, two of them based on keywords and two based on citation and reference patterns. Our results show that keyword-based strategies provide both better precision and recall. While we focus on comparing strategies for the study of the intersection between the fields of neuroscience and computer science, this proposed methodological reflection is applicable to a wide range of interdisciplinary domains.",2025-05-30,http://arxiv.org/abs/2506.03187v1,"Malena Mendez Isla, Agustin Mauro, Diego Kozlowski",arxiv.org,"cs.DL, cs.IR"
BrainFLORA: Uncovering Brain Concept Representation via Multimodal   Neural Embeddings,"Understanding how the brain represents visual information is a fundamental challenge in neuroscience and artificial intelligence. While AI-driven decoding of neural data has provided insights into the human visual system, integrating multimodal neuroimaging signals, such as EEG, MEG, and fMRI, remains a critical hurdle due to their inherent spatiotemporal misalignment. Current approaches often analyze these modalities in isolation, limiting a holistic view of neural representation. In this study, we introduce BrainFLORA, a unified framework for integrating cross-modal neuroimaging data to construct a shared neural representation. Our approach leverages multimodal large language models (MLLMs) augmented with modality-specific adapters and task decoders, achieving state-of-the-art performance in joint-subject visual retrieval task and has the potential to extend multitasking. Combining neuroimaging analysis methods, we further reveal how visual concept representations align across neural modalities and with real world object perception. We demonstrate that the brain's structured visual concept representations exhibit an implicit mapping to physical-world stimuli, bridging neuroscience and machine learning from different modalities of neural imaging. Beyond methodological advancements, BrainFLORA offers novel implications for cognitive neuroscience and brain-computer interfaces (BCIs). Our code is available at https://github.com/ncclab-sustech/BrainFLORA.",2025-07-13,http://arxiv.org/abs/2507.09747v1,"Dongyang Li, Haoyang Qin, Mingyang Wu, Chen Wei, Quanying Liu",arxiv.org,cs.NE
A Minimal Intervention Definition of Reverse Engineering a Neural   Circuit,"In neuroscience, researchers have developed informal notions of what it means to reverse engineer a system, e.g., being able to model or simulate a system in some sense. A recent influential paper of Jonas and Kording, that examines a microprocessor using techniques from neuroscience, suggests that common techniques to understand neural systems are inadequate. Part of the difficulty, as a previous work of Lazebnik noted, lies in lack of formal language. We provide a theoretical framework for defining reverse engineering of computational systems, motivated by the neuroscience context. Of specific interest are recent works where, increasingly, interventions are being made to alter the function of the neural circuitry to both understand the system and treat disorders. Starting from Lazebnik's viewpoint that understanding a system means you can ``fix it'', and motivated by use-cases in neuroscience, we propose the following requirement on reverse engineering: once an agent claims to have reverse-engineered a neural circuit, they subsequently need to be able to: (a) provide a minimal set of interventions to change the input/output (I/O) behavior of the circuit to a desired behavior; (b) arrive at this minimal set of interventions while operating under bounded rationality constraints (e.g., limited memory) to rule out brute-force approaches. Under certain assumptions, we show that this reverse engineering goal falls within the class of undecidable problems. Next, we examine some canonical computational systems and reverse engineering goals (as specified by desired I/O behaviors) where reverse engineering can indeed be performed. Finally, using an exemplar network, the ``reward network'' in the brain, we summarize the state of current neuroscientific understanding, and discuss how computer-science and information-theoretic concepts can inform goals of future neuroscience studies.",2021-10-02,http://arxiv.org/abs/2110.00889v1,"Keerthana Gurushankar, Pulkit Grover",arxiv.org,"cs.IT, math.IT, q-bio.NC"
Nature's Insight: A Novel Framework and Comprehensive Analysis of   Agentic Reasoning Through the Lens of Neuroscience,"Autonomous AI is no longer a hard-to-reach concept, it enables the agents to move beyond executing tasks to independently addressing complex problems, adapting to change while handling the uncertainty of the environment. However, what makes the agents truly autonomous? It is agentic reasoning, that is crucial for foundation models to develop symbolic logic, statistical correlations, or large-scale pattern recognition to process information, draw inferences, and make decisions. However, it remains unclear why and how existing agentic reasoning approaches work, in comparison to biological reasoning, which instead is deeply rooted in neural mechanisms involving hierarchical cognition, multimodal integration, and dynamic interactions. In this work, we propose a novel neuroscience-inspired framework for agentic reasoning. Grounded in three neuroscience-based definitions and supported by mathematical and biological foundations, we propose a unified framework modeling reasoning from perception to action, encompassing four core types, perceptual, dimensional, logical, and interactive, inspired by distinct functional roles observed in the human brain. We apply this framework to systematically classify and analyze existing AI reasoning methods, evaluating their theoretical foundations, computational designs, and practical limitations. We also explore its implications for building more generalizable, cognitively aligned agents in physical and virtual environments. Finally, building on our framework, we outline future directions and propose new neural-inspired reasoning methods, analogous to chain-of-thought prompting. By bridging cognitive neuroscience and AI, this work offers a theoretical foundation and practical roadmap for advancing agentic reasoning in intelligent systems. The associated project can be found at: https://github.com/BioRAILab/Awesome-Neuroscience-Agent-Reasoning .",2025-05-07,http://arxiv.org/abs/2505.05515v1,"Zinan Liu, Haoran Li, Jingyi Lu, Gaoyuan Ma, Xu Hong, Giovanni Iacca, Arvind Kumar, Shaojun Tang, Lin Wang",arxiv.org,"q-bio.NC, cs.LG"
Neuromatch Academy: Teaching Computational Neuroscience with global   accessibility,"Neuromatch Academy designed and ran a fully online 3-week Computational Neuroscience summer school for 1757 students with 191 teaching assistants working in virtual inverted (or flipped) classrooms and on small group projects. Fourteen languages, active community management, and low cost allowed for an unprecedented level of inclusivity and universal accessibility.",2020-12-15,http://arxiv.org/abs/2012.08973v1,"Tara van Viegen, Athena Akrami, Kate Bonnen, Eric DeWitt, Alexandre Hyafil, Helena Ledmyr, Grace W. Lindsay, Patrick Mineault, John D. Murray, Xaq Pitkow, Aina Puce, Madineh Sedigh-Sarvestani, Carsen Stringer, Titipat Achakulvisut, Elnaz Alikarami, Melvin Selim Atay, Eleanor Batty, Jeffrey C. Erlich, Byron V. Galbraith, Yueqi Guo, Ashley L. Juavinett, Matthew R. Krause, Songting Li, Marius Pachitariu, Elizabeth Straley, Davide Valeriani, Emma Vaughan, Maryam Vaziri-Pashkam, Michael L. Waskom, Gunnar Blohm, Konrad Kording, Paul Schrater, Brad Wyble, Sean Escola, Megan A. K. Peters",arxiv.org,q-bio.OT
"Online detection and sorting of extracellularly recorded action   potentials in human medial temporal lobe recordings, in vivo","Understanding the function of complex cortical circuits requires the simultaneous recording of action potentials from many neurons in awake and behaving animals. Practically, this can be achieved by extracellularly recording from multiple brain sites using single wire electrodes. However, in densely packed neural structures such as the human hippocampus, a single electrode can record the activity of multiple neurons. Thus, analytic techniques that differentiate action potentials of different neurons are required. Offline spike sorting approaches are currently used to detect and sort action potentials after finishing the experiment. Because the opportunities to record from the human brain are relatively rare, it is desirable to analyze large numbers of simultaneous recordings quickly using online sorting and detection algorithms. In this way, the experiment can be optimized for the particular response properties of the recorded neurons. Here we present and evaluate a method that is capable of detecting and sorting extracellular single-wire recordings in realtime. We demonstrate the utility of the method by applying it to an extensive data set we acquired from chronically-implanted depth electrodes in the hippocampus of human epilepsy patients. This dataset is particularly challenging because it was recorded in a noisy clinical environment. This method will allow the development of closed-loop experiments, which immediately adapt the experimental stimuli and/or tasks to the neural response observed.",2006-04-26,http://arxiv.org/abs/q-bio/0604033v1,"Ueli Rutishauser, Erin M. Schuman, Adam N. Mamelak",arxiv.org,"q-bio.QM, q-bio.NC"
"A primer on information theory, with applications to neuroscience","Given the constant rise in quantity and quality of data obtained from neural systems on many scales ranging from molecular to systems', information-theoretic analyses became increasingly necessary during the past few decades in the neurosciences. Such analyses can provide deep insights into the functionality of such systems, as well as a rigid mathematical theory and quantitative measures of information processing in both healthy and diseased states of neural systems. This chapter will present a short introduction to the fundamentals of information theory, especially suited for people having a less firm background in mathematics and probability theory. To begin, the fundamentals of probability theory such as the notion of probability, probability distributions, and random variables will be reviewed. Then, the concepts of information and entropy (in the sense of Shannon), mutual information, and transfer entropy (sometimes also referred to as conditional mutual information) will be outlined. As these quantities cannot be computed exactly from measured data in practice, estimation techniques for information-theoretic quantities will be presented. The chapter will conclude with the applications of information theory in the field of neuroscience, including questions of possible medical applications and a short review of software packages that can be used for information-theoretic analyses of neural data.",2013-04-08,http://arxiv.org/abs/1304.2333v2,Felix Effenberger,arxiv.org,"cs.IT, math.IT, q-bio.NC"
Stochastic neural field equations: A rigorous footing,"We extend the theory of neural fields which has been developed in a deterministic framework by considering the influence spatio-temporal noise. The outstanding problem that we here address is the development of a theory that gives rigorous meaning to stochastic neural field equations, and conditions ensuring that they are well-posed. Previous investigations in the field of computational and mathematical neuroscience have been numerical for the most part. Such questions have been considered for a long time in the theory of stochastic partial differential equations, where at least two different approaches have been developed, each having its advantages and disadvantages. It turns out that both approaches have also been used in computational and mathematical neuroscience, but with much less emphasis on the underlying theory. We present a review of two existing theories and show how they can be used to put the theory of stochastic neural fields on a rigorous footing. We also provide general conditions on the parameters of the stochastic neural field equations under which we guarantee that these equations are well-posed. In so doing we relate each approach to previous work in computational and mathematical neuroscience. We hope this will provide a reference that will pave the way for future studies (both theoretical and applied) of these equations, where basic questions of existence and uniqueness will no longer be a cause for concern.",2013-11-21,http://arxiv.org/abs/1311.5446v1,"Olivier Faugeras, James Inglis",arxiv.org,math.PR
Realistic Thermodynamic and Statistical-Mechanical Measures for Neural   Synchronization,"Synchronized brain rhythms, associated with diverse cognitive functions, have been observed in electrical recordings of brain activity. Neural synchronization may be well described by using the population-averaged global potential $V_G$ in computational neuroscience. The time-averaged fluctuation of $V_G$ plays the role of a ""thermodynamic"" order parameter $\cal {O}$ used for describing the synchrony-asynchrony transition in neural systems. Population spike synchronization may be well visualized in the raster plot of neural spikes. The degree of neural synchronization seen in the raster plot is well measured in terms of a ""statistical-mechanical"" spike-based measure $M_s$ introduced by considering the occupation and the pacing patterns of spikes. The global potential $V_G$ is also used to give a reference global cycle for the calculation of $M_s$. Hence, $V_G$ becomes an important collective quantity because it is associated with calculation of both $\cal {O}$ and $M_s$. However, it is practically difficult to directly get $V_G$ in real experiments. To overcome this difficulty, instead of $V_G$, we employ the instantaneous population spike rate (IPSR) which can be obtained in experiments, and develop realistic thermodynamic and statistical-mechanical measures, based on IPSR, to make practical characterization of the neural synchronization in both computational and experimental neuroscience. Particularly, more accurate characterization of weak sparse spike synchronization can be achieved in terms of realistic statistical-mechanical IPSR-based measure, in comparison with the conventional measure based on $V_G$.",2014-03-05,http://arxiv.org/abs/1403.1255v1,"Sang-Yoon Kim, Woochang Lim",arxiv.org,"q-bio.NC, physics.bio-ph"
Mean-field behavior as a result of noisy local dynamics in   self-organized criticality: Neuroscience implications,"Motivated by recent experiments in neuroscience which indicate that neuronal avalanches exhibit scale invariant behavior similar to self-organized critical systems, we study the role of noisy (non-conservative) local dynamics on the critical behavior of a sandpile model which can be taken to mimic the dynamics of neuronal avalanches. We find that despite the fact that noise breaks the strict local conservation required to attain criticality, our system exhibit true criticality for a wide range of noise in various dimensions, given that conservation is respected \textit{on the average}. Although the system remains critical, exhibiting finite-size scaling, the value of critical exponents change depending on the intensity of local noise. Interestingly, for sufficiently strong noise level, the critical exponents approach and saturate at their mean-field values, consistent with empirical measurements of neuronal avalanches. This is confirmed for both two and three dimensional models. However, addition of noise does not affect the exponents at the upper critical dimension ($D=4$). In addition to extensive finite-size scaling analysis of our systems, we also employ a useful time-series analysis method in order to establish true criticality of noisy systems. Finally, we discuss the implications of our work in neuroscience as well as some implications for general phenomena of criticality in non-equilibrium systems.",2014-05-14,http://arxiv.org/abs/1405.3703v1,"S. Amin Moosavi, Afshin Montakhab",arxiv.org,"cond-mat.stat-mech, cond-mat.dis-nn"
Leaders and followers: Quantifying consistency in spatio-temporal   propagation patterns,"Repetitive spatio-temporal propagation patterns are encountered in fields as wide-ranging as climatology, social communication and network science. In neuroscience, perfectly consistent repetitions of the same global propagation pattern are called a synfire pattern. For any recording of sequences of discrete events (in neuroscience terminology: sets of spike trains) the questions arise how closely it resembles such a synfire pattern and which are the spike trains that lead/follow. Here we address these questions and introduce an algorithm built on two new indicators, termed SPIKE-Order and Spike Train Order, that define the Synfire Indicator value, which allows to sort multiple spike trains from leader to follower and to quantify the consistency of the temporal leader-follower relationships for both the original and the optimized sorting. We demonstrate our new approach using artificially generated datasets before we apply it to analyze the consistency of propagation patterns in two real datasets from neuroscience (Giant Depolarized Potentials in mice slices) and climatology (El Ni~no sea surface temperature recordings). The new algorithm is distinguished by conceptual and practical simplicity, low computational cost, as well as flexibility and universality.",2016-10-25,http://arxiv.org/abs/1610.07986v4,"Thomas Kreuz, Eero Satuvuori, Martin Pofahl, Mario Mulansky",arxiv.org,"physics.data-an, q-bio.NC"
"In reply to Faes et al. and Barnett et al. regarding ""A study of   problems encountered in Granger causality analysis from a neuroscience   perspective""","This reply is in response to commentaries by Barnett, Barrett, and Seth (arXiv:1708.08001) and Faes, Stramaglia, and Marinazzo (arXiv:1708.06990) on our paper entitled ""A study of problems encountered in Granger causality analysis from a neuroscience perspective."" (PNAS 114(34):7063-7072. 2017). In our paper, we analyzed several properties of Granger-Geweke causality (GGC) and discussed potential problems in neuroscience applications. We demonstrated: (i) that GGC, estimated using separate model fits, is either severely biased, particularly when the true model is known, or a high variance is introduced to overcome the bias; and (ii) that GGC does not reflect some component dynamics of the system. The commentaries by both Faes et al. and Barnett et al. point out that the computational problems of (i) are resolved by using recent computational methods. We acknowledge that these problems are indeed resolved by these methods. However, the traditional computation using separate model fits continues to be presented and applied. More fundamentally, the interpretational problems stemming from (ii) are not in anyway addressed by the improved methods because they are inherent to the definition of GGC. These properties are indeed acknowledged by both commentaries. We have no misconception of the GGC measure and do not claim that these properties are facially wrong. But we do discuss at length how these properties make it inappropriate and misleading for common types of scientific questions, how presentation of GGC results without model estimates are not decipherable, and how the absence of clear statements of questions of interest present further opportunities for misinterpretation.",2017-09-29,http://arxiv.org/abs/1709.10248v1,"Patrick A. Stokes, Patrick L. Purdon",arxiv.org,"stat.ME, q-bio.NC"
Brain Modularity Mediates the Relation between Task Complexity and   Performance,"Recent work in cognitive neuroscience has focused on analyzing the brain as a network, rather than as a collection of independent regions. Prior studies taking this approach have found that individual differences in the degree of modularity of the brain network relate to performance on cognitive tasks. However, inconsistent results concerning the direction of this relationship have been obtained, with some tasks showing better performance as modularity increases and other tasks showing worse performance. A recent theoretical model (Chen & Deem, 2015) suggests that these inconsistencies may be explained on the grounds that high-modularity networks favor performance on simple tasks whereas low-modularity networks favor performance on more complex tasks. The current study tests these predictions by relating modularity from resting-state fMRI to performance on a set of simple and complex behavioral tasks. Complex and simple tasks were defined on the basis of whether they did or did not draw on executive attention. Consistent with predictions, we found a negative correlation between individuals' modularity and their performance on a composite measure combining scores from the complex tasks but a positive correlation with performance on a composite measure combining scores from the simple tasks. These results and theory presented here provide a framework for linking measures of whole brain organization from network neuroscience to cognitive processing.",2017-11-24,http://arxiv.org/abs/1711.09133v1,"Qiuhai Yue, Randi Martin, Simon Fischer-Baum, Aurora I. Ramos-Nuñez, Fengdan Ye, Michael W. Deem",arxiv.org,q-bio.NC
Reverse engineering neural networks from many partial recordings,"Much of neuroscience aims at reverse engineering the brain, but we only record a small number of neurons at a time. We do not currently know if reverse engineering the brain requires us to simultaneously record most neurons or if multiple recordings from smaller subsets suffice. This is made even more important by the development of novel techniques that allow recording from selected subsets of neurons, e.g. using optical techniques. To get at this question, we analyze a neural network, trained on the MNIST dataset, using only partial recordings and characterize the dependency of the quality of our reverse engineering on the number of simultaneously recorded ""neurons"". We find that reverse engineering of the nonlinear neural network is meaningfully possible if a sufficiently large number of neurons is simultaneously recorded but that this number can be considerably smaller than the number of neurons. Moreover, recording many times from small random subsets of neurons yields surprisingly good performance. Application in neuroscience suggests to approximate the I/O function of an actual neural system, we need to record from a much larger number of neurons. The kind of scaling analysis we perform here can, and arguably should be used to calibrate approaches that can dramatically scale up the size of recorded data sets in neuroscience.",2019-07-02,http://arxiv.org/abs/1907.01588v1,"Elahe Arani, Sofia Triantafillou, Konrad P. Kording",arxiv.org,q-bio.NC
Inverse Rational Control with Partially Observable Continuous Nonlinear   Dynamics,"Continuous control and planning remains a major challenge in robotics and machine learning. Neuroscience offers the possibility of learning from animal brains that implement highly successful controllers, but it is unclear how to relate an animal's behavior to control principles. Animals may not always act optimally from the perspective of an external observer, but may still act rationally: we hypothesize that animals choose actions with highest expected future subjective value according to their own internal model of the world. Their actions thus result from solving a different optimal control problem from those on which they are evaluated in neuroscience experiments. With this assumption, we propose a novel framework of model-based inverse rational control that learns the agent's internal model that best explains their actions in a task described as a partially observable Markov decision process (POMDP). In this approach we first learn optimal policies generalized over the entire model space of dynamics and subjective rewards, using an extended Kalman filter to represent the belief space, a neural network in the actor-critic framework to optimize the policy, and a simplified basis for the parameter space. We then compute the model that maximizes the likelihood of the experimentally observable data comprising the agent's sensory observations and chosen actions. Our proposed method is able to recover the true model of simulated agents within theoretical error bounds given by limited data. We illustrate this method by applying it to a complex naturalistic task currently used in neuroscience experiments. This approach provides a foundation for interpreting the behavioral and neural dynamics of highly adapted controllers in animal brains.",2019-08-13,http://arxiv.org/abs/1908.04696v1,"Saurabh Daptardar, Paul Schrater, Xaq Pitkow",arxiv.org,"cs.AI, cs.SY, eess.SY, q-bio.NC"
From deep learning to mechanistic understanding in neuroscience: the   structure of retinal prediction,"Recently, deep feedforward neural networks have achieved considerable success in modeling biological sensory processing, in terms of reproducing the input-output map of sensory neurons. However, such models raise profound questions about the very nature of explanation in neuroscience. Are we simply replacing one complex system (a biological circuit) with another (a deep network), without understanding either? Moreover, beyond neural representations, are the deep network's computational mechanisms for generating neural responses the same as those in the brain? Without a systematic approach to extracting and understanding computational mechanisms from deep neural network models, it can be difficult both to assess the degree of utility of deep learning approaches in neuroscience, and to extract experimentally testable hypotheses from deep networks. We develop such a systematic approach by combining dimensionality reduction and modern attribution methods for determining the relative importance of interneurons for specific visual computations. We apply this approach to deep network models of the retina, revealing a conceptual understanding of how the retina acts as a predictive feature extractor that signals deviations from expectations for diverse spatiotemporal stimuli. For each stimulus, our extracted computational mechanisms are consistent with prior scientific literature, and in one case yields a new mechanistic hypothesis. Thus overall, this work not only yields insights into the computational mechanisms underlying the striking predictive capabilities of the retina, but also places the framework of deep networks as neuroscientific models on firmer theoretical foundations, by providing a new roadmap to go beyond comparing neural representations to extracting and understand computational mechanisms.",2019-12-12,http://arxiv.org/abs/1912.06207v1,"Hidenori Tanaka, Aran Nayebi, Niru Maheswaranathan, Lane McIntosh, Stephen A. Baccus, Surya Ganguli",arxiv.org,"q-bio.NC, cs.LG, physics.bio-ph"
Regions of Interest as nodes of dynamic functional brain networks,"The properties of functional brain networks strongly depend on how their nodes are chosen. Commonly, nodes are defined by Regions of Interest (ROIs), pre-determined groupings of fMRI measurement voxels. Earlier, we have demonstrated that the functional homogeneity of ROIs, captured by their spatial consistency, varies widely across ROIs in commonly-used brain atlases. Here, we ask how ROIs behave as nodes of dynamic brain networks. To this end, we use two measures: spatiotemporal consistency measures changes in spatial consistency across time and network turnover quantifies the changes in the local network structure around a ROI. We find that spatial consistency varies non-uniformly in space and time, which is reflected in the variation of spatiotemporal consistency across ROIs. Further, we see time-dependent changes in the network neighborhoods of the ROIs, reflected in high network turnover. Network turnover is nonuniformly distributed across ROIs: ROIs with high spatiotemporal consistency have low network turnover. Finally, we reveal that there is rich voxel-level correlation structure inside ROIs. Because the internal structure and the connectivity of ROIs vary in time, the common approach of using static node definitions may be surprisingly inaccurate. Therefore, network neuroscience would greatly benefit from node definition strategies tailored for dynamical networks.",2017-10-11,http://arxiv.org/abs/1710.04056v2,"Elisa Ryyppö, Enrico Glerean, Elvira Brattico, Jari Saramäki, Onerva Korhonen",arxiv.org,q-bio.NC
Incorporating structured assumptions with probabilistic graphical models   in fMRI data analysis,"With the wide adoption of functional magnetic resonance imaging (fMRI) by cognitive neuroscience researchers, large volumes of brain imaging data have been accumulated in recent years. Aggregating these data to derive scientific insights often faces the challenge that fMRI data are high-dimensional, heterogeneous across people, and noisy. These challenges demand the development of computational tools that are tailored both for the neuroscience questions and for the properties of the data. We review a few recently developed algorithms in various domains of fMRI research: fMRI in naturalistic tasks, analyzing full-brain functional connectivity, pattern classification, inferring representational similarity and modeling structured residuals. These algorithms all tackle the challenges in fMRI similarly: they start by making clear statements of assumptions about neural data and existing domain knowledge, incorporating those assumptions and domain knowledge into probabilistic graphical models, and using those models to estimate properties of interest or latent structures in the data. Such approaches can avoid erroneous findings, reduce the impact of noise, better utilize known properties of the data, and better aggregate data across groups of subjects. With these successful cases, we advocate wider adoption of explicit model construction in cognitive neuroscience. Although we focus on fMRI, the principle illustrated here is generally applicable to brain data of other modalities.",2020-05-11,http://arxiv.org/abs/2005.04879v2,"Ming Bo Cai, Michael Shvartsman, Anqi Wu, Hejia Zhang, Xia Zhu",arxiv.org,"stat.AP, cs.LG, q-bio.NC"
Detecting Multiple Change Points Using Adaptive Regression Splines with   Application to Neural Recordings,"Time series, as frequently the case in neuroscience, are rarely stationary, but often exhibit abrupt changes due to attractor transitions or bifurcations in the dynamical systems producing them. A plethora of methods for detecting such change points in time series statistics have been developed over the years, in addition to test criteria to evaluate their significance. Issues to consider when developing change point analysis methods include computational demands, difficulties arising from either limited amount of data or a large number of covariates, and arriving at statistical tests with sufficient power to detect as many changes as contained in potentially high-dimensional time series. Here, a general method called Paired Adaptive Regressors for Cumulative Sum is developed for detecting multiple change points in the mean of multivariate time series. The method's advantages over alternative approaches are demonstrated through a series of simulation experiments. This is followed by a real data application to neural recordings from rat medial prefrontal cortex during learning. Finally, the method's flexibility to incorporate useful features from state-of-the-art change point detection techniques is discussed, along with potential drawbacks and suggestions to remedy them.",2018-02-10,http://arxiv.org/abs/1802.03627v3,"Hazem Toutounji, Daniel Durstewitz",arxiv.org,"stat.ME, q-bio.NC, q-bio.QM"
On the role of theory and modeling in neuroscience,"In recent years, the field of neuroscience has gone through rapid experimental advances and a significant increase in the use of quantitative and computational methods. This growth has created a need for clearer analyses of the theory and modeling approaches used in the field. This issue is particularly complex in neuroscience because the field studies phenomena across a wide range of scales and often requires consideration of these phenomena at varying degrees of abstraction, from precise biophysical interactions to the computations they implement. We argue that a pragmatic perspective of science, in which descriptive, mechanistic, and normative approaches each play a distinct role in defining and bridging levels of abstraction will facilitate neuroscientific practice. This analysis leads to methodological suggestions, including selecting a level of abstraction that is appropriate for a given problem, identifying transfer functions to connect models and data, and the use of models themselves as a form of experiment.",2020-03-30,http://arxiv.org/abs/2003.13825v6,"Daniel Levenstein, Veronica A. Alvarez, Asohan Amarasingham, Habiba Azab, Zhe Sage Chen, Richard C. Gerkin, Andrea Hasenstaub, Ramakrishnan Iyer, Renaud B. Jolivet, Sarah Marzen, Joseph D. Monaco, Astrid A. Prinz, Salma Quraishi, Fidel Santamaria, Sabyasachi Shivkumar, Matthew F. Singh, Roger Traub, Horacio G. Rotstein, Farzan Nadim, A. David Redish",arxiv.org,"q-bio.NC, q-bio.QM"
Current practice in software development for computational neuroscience   and how to improve it,"Almost all research work in computational neuroscience involves software. As researchers try to understand ever more complex systems, there is a continual need for software with new capabilities. Because of the wide range of questions being investigated, new software is often developed rapidly by individuals or small groups. In these cases, it can be hard to demonstrate that the software gives the right results. Software developers are often open about the code they produce and willing to share it, but there is little appreciation among potential users of the great diversity of software development practices and end results, and how this affects the suitability of software tools for use in research projects. To help clarify these issues, we have reviewed a range of software tools and asked how the culture and practice of software development affects their validity and trustworthiness. We identified four key questions that can be used to categorize software projects and correlate them with the type of product that results. The first question addresses what is being produced. The other three concern why, how, and by whom the work is done. The answers to these questions show strong correlations with the nature of the software being produced, and its suitability for particular purposes. Based on our findings, we suggest ways in which current software development practice in computational neuroscience can be improved and propose checklists to help developers, reviewers and scientists to assess the quality whether particular pieces of software are ready for use in research.",2012-05-14,http://arxiv.org/abs/1205.3025v2,"Marc-Oliver Gewaltig, Robert Cannon",arxiv.org,"q-bio.NC, cs.CY, cs.SE"
Top-down inference in an early visual cortex inspired hierarchical   Variational Autoencoder,"Interpreting computations in the visual cortex as learning and inference in a generative model of the environment has received wide support both in neuroscience and cognitive science. However, hierarchical computations, a hallmark of visual cortical processing, has remained impervious for generative models because of a lack of adequate tools to address it. Here we capitalize on advances in Variational Autoencoders (VAEs) to investigate the early visual cortex with sparse coding hierarchical VAEs trained on natural images. We design alternative architectures that vary both in terms of the generative and the recognition components of the two latent-layer VAE. We show that representations similar to the one found in the primary and secondary visual cortices naturally emerge under mild inductive biases. Importantly, a nonlinear representation for texture-like patterns is a stable property of the high-level latent space resistant to the specific architecture of the VAE, reminiscent of the secondary visual cortex. We show that a neuroscience-inspired choice of the recognition model, which features a top-down processing component is critical for two signatures of computations with generative models: learning higher order moments of the posterior beyond the mean and image inpainting. Patterns in higher order response statistics provide inspirations for neuroscience to interpret response correlations and for machine learning to evaluate the learned representations through more detailed characterization of the posterior.",2022-06-01,http://arxiv.org/abs/2206.00436v1,"Ferenc Csikor, Balázs Meszéna, Bence Szabó, Gergő Orbán",arxiv.org,"q-bio.NC, cs.LG, stat.ML"
Modular Acquisition and Stimulation System for Timestamp-Driven   Neuroscience Experiments,"Dedicated systems are fundamental for neuroscience experimental protocols that require timing determinism and synchronous stimuli generation. We developed a data acquisition and stimuli generator system for neuroscience research, optimized for recording timestamps from up to 6 spiking neurons and entirely specified in a high-level Hardware Description Language (HDL). Despite the logic complexity penalty of synthesizing from such a language, it was possible to implement our design in a low-cost small reconfigurable device. Under a modular framework, we explored two different memory arbitration schemes for our system, evaluating both their logic element usage and resilience to input activity bursts. One of them was designed with a decoupled and latency insensitive approach, allowing for easier code reuse, while the other adopted a centralized scheme, constructed specifically for our application. The usage of a high-level HDL allowed straightforward and stepwise code modifications to transform one architecture into the other. The achieved modularity is very useful for rapidly prototyping novel electronic instrumentation systems tailored to scientific research.",2015-04-07,http://arxiv.org/abs/1504.01718v1,"Paulo Matias, Rafael Tuma Guariento, Lirio Onofre Baptista de Almeida, Jan Frans Willem Slaets",arxiv.org,"q-bio.QM, cs.AR"
On-line detection of qualitative dynamical changes in nonlinear systems:   the resting-oscillation case,"Motivated by neuroscience applications, we introduce the concept of qualitative detection, that is, the problem of determining on-line the current qualitative dynamical behavior (e.g., resting, oscillating, bursting, spiking etc.) of a nonlinear system. The approach is thought for systems characterized by i) large parameter variability and redundancy, ii) a small number of possible robust, qualitatively different dynamical behaviors and, iii) the presence of sharply different characteristic timescales. These properties are omnipresent in neurosciences and hamper quantitative modeling and fitting of experimental data. As a result, novel control theoretical strategies are needed to face neuroscience challenges like on-line epileptic seizure detection. The proposed approach aims at detecting the current dynamical behavior of the system and whether a qualitative change is likely to occur without quantitatively fitting any model nor asymptotically estimating any parameter. We talk of qualitative detection. We rely on the qualitative properties of the system dynamics, extracted via singularity and singular perturbation theories, to design low dimensional qualitative detectors. We introduce this concept on a general class of singularly perturbed systems and then solve the problem for an analytically tractable class of two-dimensional systems with a single unknown sigmoidal nonlinearity and two sharply separated timescales. Numerical results are provided to show the performance of the designed qualitative detector.",2016-11-17,http://arxiv.org/abs/1611.05820v3,"Ying Tang, Alessio Franci, Romain Postoyan",arxiv.org,math.OC
Tracking Naturalistic Linguistic Predictions with Deep Neural Language   Models,"Prediction in language has traditionally been studied using simple designs in which neural responses to expected and unexpected words are compared in a categorical fashion. However, these designs have been contested as being `prediction encouraging', potentially exaggerating the importance of prediction in language understanding. A few recent studies have begun to address these worries by using model-based approaches to probe the effects of linguistic predictability in naturalistic stimuli (e.g. continuous narrative). However, these studies so far only looked at very local forms of prediction, using models that take no more than the prior two words into account when computing a word's predictability. Here, we extend this approach using a state-of-the-art neural language model that can take roughly 500 times longer linguistic contexts into account. Predictability estimates from the neural network offer a much better fit to EEG data from subjects listening to naturalistic narrative than simpler models, and reveal strong surprise responses akin to the P200 and N400. These results show that predictability effects in language are not a side-effect of simple designs, and demonstrate the practical use of recent advances in AI for the cognitive neuroscience of language.",2019-09-10,http://arxiv.org/abs/1909.04400v1,"Micha Heilbron, Benedikt Ehinger, Peter Hagoort, Floris P. de Lange",arxiv.org,q-bio.NC
Bridging the gap between emotion and joint action,"Our daily human life is filled with a myriad of joint action moments, be it children playing, adults working together (i.e., team sports), or strangers navigating through a crowd. Joint action brings individuals (and embodiment of their emotions) together, in space and in time. Yet little is known about how individual emotions propagate through embodied presence in a group, and how joint action changes individual emotion. In fact, the multi-agent component is largely missing from neuroscience-based approaches to emotion, and reversely joint action research has not found a way yet to include emotion as one of the key parameters to model socio-motor interaction. In this review, we first identify the gap and then stockpile evidence showing strong entanglement between emotion and acting together from various branches of sciences. We propose an integrative approach to bridge the gap, highlight five research avenues to do so in behavioral neuroscience and digital sciences, and address some of the key challenges in the area faced by modern societies.",2021-08-13,http://arxiv.org/abs/2108.06264v1,"M. M. N. Bieńkiewicz, A. Smykovskyi, T. Olugbade, S. Janaqi, A. Camurri, N. Bianchi-Berthouze, M. Björkman, B. G. Bardy",arxiv.org,"q-bio.NC, cs.LG, cs.MA, cs.RO, math.DS"
Overcoming the Domain Gap in Contrastive Learning of Neural Action   Representations,"A fundamental goal in neuroscience is to understand the relationship between neural activity and behavior. For example, the ability to extract behavioral intentions from neural data, or neural decoding, is critical for developing effective brain machine interfaces. Although simple linear models have been applied to this challenge, they cannot identify important non-linear relationships. Thus, a self-supervised means of identifying non-linear relationships between neural dynamics and behavior, in order to compute neural representations, remains an important open problem. To address this challenge, we generated a new multimodal dataset consisting of the spontaneous behaviors generated by fruit flies, Drosophila melanogaster -- a popular model organism in neuroscience research. The dataset includes 3D markerless motion capture data from six camera views of the animal generating spontaneous actions, as well as synchronously acquired two-photon microscope images capturing the activity of descending neuron populations that are thought to drive actions. Standard contrastive learning and unsupervised domain adaptation techniques struggle to learn neural action representations (embeddings computed from the neural data describing action labels) due to large inter-animal differences in both neural and behavioral modalities. To overcome this deficiency, we developed simple yet effective augmentations that close the inter-animal domain gap, allowing us to extract behaviorally relevant, yet domain agnostic, information from neural data. This multimodal dataset and our new set of augmentations promise to accelerate the application of self-supervised learning methods in neuroscience.",2021-11-29,http://arxiv.org/abs/2111.14595v1,"Semih Günel, Florian Aymanns, Sina Honari, Pavan Ramdya, Pascal Fua",arxiv.org,cs.CV
Brain-inspired Graph Spiking Neural Networks for Commonsense Knowledge   Representation and Reasoning,"How neural networks in the human brain represent commonsense knowledge, and complete related reasoning tasks is an important research topic in neuroscience, cognitive science, psychology, and artificial intelligence. Although the traditional artificial neural network using fixed-length vectors to represent symbols has gained good performance in some specific tasks, it is still a black box that lacks interpretability, far from how humans perceive the world. Inspired by the grandmother-cell hypothesis in neuroscience, this work investigates how population encoding and spiking timing-dependent plasticity (STDP) mechanisms can be integrated into the learning of spiking neural networks, and how a population of neurons can represent a symbol via guiding the completion of sequential firing between different neuron populations. The neuron populations of different communities together constitute the entire commonsense knowledge graph, forming a giant graph spiking neural network. Moreover, we introduced the Reward-modulated spiking timing-dependent plasticity (R-STDP) mechanism to simulate the biological reinforcement learning process and completed the related reasoning tasks accordingly, achieving comparable accuracy and faster convergence speed than the graph convolutional artificial neural networks. For the fields of neuroscience and cognitive science, the work in this paper provided the foundation of computational modeling for further exploration of the way the human brain represents commonsense knowledge. For the field of artificial intelligence, this paper indicated the exploration direction for realizing a more robust and interpretable neural network by constructing a commonsense knowledge representation and reasoning spiking neural networks with solid biological plausibility.",2022-07-11,http://arxiv.org/abs/2207.05561v1,"Hongjian Fang, Yi Zeng, Jianbo Tang, Yuwei Wang, Yao Liang, Xin Liu",arxiv.org,"cs.NE, cs.AI, cs.LG, q-bio.NC"
SPAIC: A Spike-based Artificial Intelligence Computing Framework,"Neuromorphic computing is an emerging research field that aims to develop new intelligent systems by integrating theories and technologies from multi-disciplines such as neuroscience and deep learning. Currently, there have been various software frameworks developed for the related fields, but there is a lack of an efficient framework dedicated for spike-based computing models and algorithms. In this work, we present a Python based spiking neural network (SNN) simulation and training framework, aka SPAIC that aims to support brain-inspired model and algorithm researches integrated with features from both deep learning and neuroscience. To integrate different methodologies from the two overwhelming disciplines, and balance between flexibility and efficiency, SPAIC is designed with neuroscience-style frontend and deep learning backend structure. We provide a wide range of examples including neural circuits Simulation, deep SNN learning and neuromorphic applications, demonstrating the concise coding style and wide usability of our framework. The SPAIC is a dedicated spike-based artificial intelligence computing platform, which will significantly facilitate the design, prototype and validation of new models, theories and applications. Being user-friendly, flexible and high-performance, it will help accelerate the rapid growth and wide applicability of neuromorphic computing research.",2022-07-26,http://arxiv.org/abs/2207.12750v1,"Chaofei Hong, Mengwen Yuan, Mengxiao Zhang, Xiao Wang, Chegnjun Zhang, Jiaxin Wang, Gang Pan, Zhaohui Wu, Huajin Tang",arxiv.org,cs.NE
How does artificial intelligence contribute to iEEG research?,"Artificial intelligence (AI) is a fast-growing field focused on modeling and machine implementation of various cognitive functions with an increasing number of applications in computer vision, text processing, robotics, neurotechnology, bio-inspired computing and others. In this chapter, we describe how AI methods can be applied in the context of intracranial electroencephalography (iEEG) research. IEEG data is unique as it provides extremely high-quality signals recorded directly from brain tissue. Applying advanced AI models to these data carries the potential to further our understanding of many fundamental questions in neuroscience. At the same time, as an invasive technique, iEEG lends itself well to long-term, mobile brain-computer interface applications, particularly for communication in severely paralyzed individuals. We provide a detailed overview of these two research directions in the application of AI techniques to iEEG. That is, (1) the development of computational models that target fundamental questions about the neurobiological nature of cognition (AI-iEEG for neuroscience) and (2) applied research on monitoring and identification of event-driven brain states for the development of clinical brain-computer interface systems (AI-iEEG for neurotechnology). We explain key machine learning concepts, specifics of processing and modeling iEEG data and details of state-of-the-art iEEG-based neurotechnology and brain-computer interfaces.",2022-07-26,http://arxiv.org/abs/2207.13190v1,"Julia Berezutskaya, Anne-Lise Saive, Karim Jerbi, Marcel van Gerven",arxiv.org,q-bio.NC
Consistency of Regions of Interest as nodes of functional brain networks   measured by fMRI,"The functional network approach, where fMRI BOLD time series are mapped to networks depicting functional relationships between brain areas, has opened new insights into the function of the human brain. In this approach, the choice of network nodes is of crucial importance. One option is to consider fMRI voxels as nodes. This results in a large number of nodes, making network analysis and interpretation of results challenging. A common alternative is to use pre-defined clusters of anatomically close voxels, Regions of Interest (ROIs). This approach assumes that voxels within ROIs are functionally similar. Because these two approaches result in different network structures, it is crucial to understand what happens to network connectivity when moving from the voxel level to the ROI level. We show that the consistency of ROIs, defined as the mean Pearson correlation coefficient between the time series of their voxels, varies widely in resting-state experimental data. Therefore the assumption of similar voxel dynamics within each ROI does not generally hold. Further, the time series of low-consistency ROIs may be highly correlated, resulting in spurious links in ROI-level networks. Based on these results, we recommend that averaging BOLD signals over anatomically defined ROIs should be carefully considered.",2017-04-25,http://arxiv.org/abs/1704.07635v1,"Onerva Korhonen, Heini Saarimäki, Enrico Glerean, Mikko Sams, Jari Saramäki",arxiv.org,q-bio.NC
Application of the hierarchical bootstrap to multi-level data in   neuroscience,"A common feature in many neuroscience datasets is the presence of hierarchical data structures, most commonly recording the activity of multiple neurons in multiple animals across multiple trials. Accordingly, the measurements constituting the dataset are not independent, even though the traditional statistical analyses often applied in such cases (e.g., Students t-test) treat them as such. The hierarchical bootstrap has been shown to be an effective tool to accurately analyze such data and while it has been used extensively in the statistical literature, its use is not widespread in neuroscience - despite the ubiquity of hierarchical datasets. In this paper, we illustrate the intuitiveness and utility of this approach to analyze hierarchically nested datasets. We use simulated neural data to show that traditional statistical tests can result in a false positive rate of over 45%, even if the Type-I error rate is set at 5%. While summarizing data across non-independent points (or lower levels) can potentially fix this problem, this approach greatly reduces the statistical power of the analysis. The hierarchical bootstrap, when applied sequentially over the levels of the hierarchical structure, keeps the Type-I error rate within the intended bound and retains more statistical power than summarizing methods. We conclude by demonstrating the effectiveness of the method in two real-world examples, first analyzing singing data in male Bengalese finches (Lonchura striata var. domestica) and second quantifying changes in behavior under optogenetic control in flies (Drosophila melanogaster).",2020-07-15,http://arxiv.org/abs/2007.07797v2,"Varun Saravanan, Gordon J Berman, Samuel J Sober",arxiv.org,q-bio.NC
Stochastic Models of Neural Synaptic Plasticity,"In neuroscience, learning and memory are usually associated to long-term changes of neuronal connectivity. In this context, synaptic plasticity refers to the set of mechanisms driving the dynamics of neuronal connections, called {\em synapses} and represented by a scalar value, the synaptic weight. Spike-Timing Dependent Plasticity (STDP) is a biologically-based model representing the time evolution of the synaptic weight as a functional of the past spiking activity of adjacent neurons.   If numerous models of neuronal cells have been proposed in the mathematical literature, few of them include a variable for the time-varying strength of the connection. A new, general, mathematical framework is introduced to study synaptic plasticity associated to different STDP rules. The system composed of two neurons connected by a single synapse is investigated and a stochastic process describing its dynamical behavior is presented and analyzed. The notion of plasticity kernel is introduced as a key component of plastic neural networks models, generalizing a notion used for pair-based models. We show that a large number of STDP rules from neuroscience and physics can be represented by this formalism. Several aspects of these models are discussed and compared to canonical models of computational neuroscience. An important sub-class of plasticity kernels with a Markovian formulation is also defined and investigated. In these models, the time evolution of cellular processes such as the neuronal membrane potential and the concentrations of chemical components created/suppressed by spiking activity has the Markov property.",2020-10-16,http://arxiv.org/abs/2010.08195v2,"Philippe Robert, Gaetan Vignoud",arxiv.org,"math.PR, q-bio.NC"
NeuroGen: activation optimized image synthesis for discovery   neuroscience,"Functional MRI (fMRI) is a powerful technique that has allowed us to characterize visual cortex responses to stimuli, yet such experiments are by nature constructed based on a priori hypotheses, limited to the set of images presented to the individual while they are in the scanner, are subject to noise in the observed brain responses, and may vary widely across individuals. In this work, we propose a novel computational strategy, which we call NeuroGen, to overcome these limitations and develop a powerful tool for human vision neuroscience discovery. NeuroGen combines an fMRI-trained neural encoding model of human vision with a deep generative network to synthesize images predicted to achieve a target pattern of macro-scale brain activation. We demonstrate that the reduction of noise that the encoding model provides, coupled with the generative network's ability to produce images of high fidelity, results in a robust discovery architecture for visual neuroscience. By using only a small number of synthetic images created by NeuroGen, we demonstrate that we can detect and amplify differences in regional and individual human brain response patterns to visual stimuli. We then verify that these discoveries are reflected in the several thousand observed image responses measured with fMRI. We further demonstrate that NeuroGen can create synthetic images predicted to achieve regional response patterns not achievable by the best-matching natural images. The NeuroGen framework extends the utility of brain encoding models and opens up a new avenue for exploring, and possibly precisely controlling, the human visual system.",2021-05-15,http://arxiv.org/abs/2105.07140v1,"Zijin Gu, Keith W. Jamison, Meenakshi Khosla, Emily J. Allen, Yihan Wu, Thomas Naselaris, Kendrick Kay, Mert R. Sabuncu, Amy Kuceyeski",arxiv.org,"q-bio.NC, cs.CV, q-bio.QM"
A brain basis of dynamical intelligence for AI and computational   neuroscience,"The deep neural nets of modern artificial intelligence (AI) have not achieved defining features of biological intelligence, including abstraction, causal learning, and energy-efficiency. While scaling to larger models has delivered performance improvements for current applications, more brain-like capacities may demand new theories, models, and methods for designing artificial learning systems. Here, we argue that this opportunity to reassess insights from the brain should stimulate cooperation between AI research and theory-driven computational neuroscience (CN). To motivate a brain basis of neural computation, we present a dynamical view of intelligence from which we elaborate concepts of sparsity in network structure, temporal dynamics, and interactive learning. In particular, we suggest that temporal dynamics, as expressed through neural synchrony, nested oscillations, and flexible sequences, provide a rich computational layer for reading and updating hierarchical models distributed in long-term memory networks. Moreover, embracing agent-centered paradigms in AI and CN will accelerate our understanding of the complex dynamics and behaviors that build useful world models. A convergence of AI/CN theories and objectives will reveal dynamical principles of intelligence for brains and engineered learning systems. This article was inspired by our symposium on dynamical neuroscience and machine learning at the 6th Annual US/NIH BRAIN Initiative Investigators Meeting.",2021-05-15,http://arxiv.org/abs/2105.07284v2,"Joseph D. Monaco, Kanaka Rajan, Grace M. Hwang",arxiv.org,"q-bio.NC, cs.AI"
Immersive virtual reality methods in cognitive neuroscience and   neuropsychology: Meeting the criteria of the National Academy of   Neuropsychology and American Academy of Clinical Neuropsychology,"Clinical tools involving immersive virtual reality (VR) may bring several advantages to cognitive neuroscience and neuropsychology. However, there are some technical and methodological pitfalls. The American Academy of Clinical Neuropsychology (AACN) and the National Academy of Neuropsychology (NAN) raised 8 key issues pertaining to Computerized Neuropsychological Assessment Devices. These issues pertain to: (1) the safety and effectivity; (2) the identity of the end-user; (3) the technical hardware and software features; (4) privacy and data security; (5) the psychometric properties; (6) examinee issues; (7) the use of reporting services; and (8) the reliability of the responses and results. The VR Everyday Assessment Lab (VR-EAL) is the first immersive VR neuropsychological battery with enhanced ecological validity for the assessment of everyday cognitive functions by offering a pleasant testing experience without inducing cybersickness. The VR-EAL meets the criteria of the NAN and AACN, addresses the methodological pitfalls, and brings advantages for neuropsychological testing. However, there are still shortcomings of the VR-EAL, which should be addressed. Future iterations should strive to improve the embodiment illusion in VR-EAL and the creation of an open access VR software library should be attempted. The discussed studies demonstrate the utility of VR methods in cognitive neuroscience and neuropsychology.",2021-05-25,http://arxiv.org/abs/2105.11909v2,"Panagiotis Kourtesis, Sarah E. MacPherson",arxiv.org,"cs.HC, cs.CY, B.8; C.4; D.0; J.4; J.3; K.4.0"
Object Based Attention Through Internal Gating,"Object-based attention is a key component of the visual system, relevant for perception, learning, and memory. Neurons tuned to features of attended objects tend to be more active than those associated with non-attended objects. There is a rich set of models of this phenomenon in computational neuroscience. However, there is currently a divide between models that successfully match physiological data but can only deal with extremely simple problems and models of attention used in computer vision. For example, attention in the brain is known to depend on top-down processing, whereas self-attention in deep learning does not. Here, we propose an artificial neural network model of object-based attention that captures the way in which attention is both top-down and recurrent. Our attention model works well both on simple test stimuli, such as those using images of handwritten digits, and on more complex stimuli, such as natural images drawn from the COCO dataset. We find that our model replicates a range of findings from neuroscience, including attention-invariant tuning, inhibition of return, and attention-mediated scaling of activity. Understanding object based attention is both computationally interesting and a key problem for computational neuroscience.",2021-06-08,http://arxiv.org/abs/2106.04540v1,"Jordan Lei, Ari S. Benjamin, Konrad P. Kording",arxiv.org,"q-bio.NC, cs.AI, cs.CV, cs.LG, cs.NE"
Can AI detect pain and express pain empathy? A review from emotion   recognition and a human-centered AI perspective,"Sensory and emotional experiences such as pain and empathy are essential for mental and physical health. Cognitive neuroscience has been working on revealing mechanisms underlying pain and empathy. Furthermore, as trending research areas, computational pain recognition and empathic artificial intelligence (AI) show progress and promise for healthcare or human-computer interaction. Although AI research has recently made it increasingly possible to create artificial systems with affective processing, most cognitive neuroscience and AI research do not jointly address the issues of empathy in AI and cognitive neuroscience. The main aim of this paper is to introduce key advances, cognitive challenges and technical barriers in computational pain recognition and the implementation of artificial empathy. Our discussion covers the following topics: How can AI recognize pain from unimodal and multimodal information? Is it crucial for AI to be empathic? What are the benefits and challenges of empathic AI? Despite some consensus on the importance of AI, including empathic recognition and responses, we also highlight future challenges for artificial empathy and possible paths from interdisciplinary perspectives. Furthermore, we discuss challenges for responsible evaluation of cognitive methods and computational techniques and show approaches to future work to contribute to affective assistants capable of empathy.",2021-10-08,http://arxiv.org/abs/2110.04249v2,"Siqi Cao, Di Fu, Xu Yang, Stefan Wermter, Xun Liu, Haiyan Wu",arxiv.org,"cs.AI, cs.HC"
Cognition of time and thinkings beyond,"A pervasive research protocol of cognitive neuroscience is to train subjects to perform deliberately designed experiments and record brain activity simultaneously, aiming to understand the brain mechanism underlying cognition. However, how the results of this protocol can be applied in technology is seldom discussed. Here, I review the studies on time processing of the brain as examples of this protocol, as well as two main application areas of neuroscience (neuroengineering and brain-inspired artificial intelligence). Time processing is an indispensable dimension of cognition; time is also an indispensable dimension of any real-world signal to be processed in technology. So one may expect that the studies of time processing in cognition profoundly influence brain-related technology. Surprisingly, I found that the results from cognitive studies on timing processing are hardly helpful in solving practical problems. This awkward situation may be due to the lack of generalizability of the results of cognitive studies, which are under well-controlled laboratory conditions, to real-life situations. This lack of generalizability may be rooted in the fundamental unknowability of the world (including cognition). Overall, this paper questions and criticizes the usefulness and prospect of the above-mentioned research protocol of cognitive neuroscience. I then give three suggestions for future research. First, to improve the generalizability of research, it is better to study brain activity under real-life conditions instead of in well-controlled laboratory experiments. Second, to overcome the unknowability of the world, we can engineer an easily accessible surrogate of the object under investigation, so that we can predict the behavior of the object by experimenting on the surrogate. Third, I call for technology-oriented research, with the aim of technology creation instead of knowledge discovery.",2023-03-08,http://arxiv.org/abs/2303.06076v1,Zedong Bi,arxiv.org,q-bio.NC
Control of synaptic plasticity via the fusion of reinforcement learning   and unsupervised learning in neural networks,"The brain can learn to execute a wide variety of tasks quickly and efficiently. Nevertheless, most of the mechanisms that enable us to learn are unclear or incredibly complicated. Recently, considerable efforts have been made in neuroscience and artificial intelligence to understand and model the structure and mechanisms behind the amazing learning capability of the brain. However, in the current understanding of cognitive neuroscience, it is widely accepted that synaptic plasticity plays an essential role in our amazing learning capability. This mechanism is also known as the Credit Assignment Problem (CAP) and is a fundamental challenge in neuroscience and Artificial Intelligence (AI). The observations of neuroscientists clearly confirm the role of two important mechanisms including the error feedback system and unsupervised learning in synaptic plasticity. With this inspiration, a new learning rule is proposed via the fusion of reinforcement learning (RL) and unsupervised learning (UL). In the proposed computational model, the nonlinear optimal control theory is used to resemble the error feedback loop systems and project the output error to neurons membrane potential (neurons state), and an unsupervised learning rule based on neurons membrane potential or neurons activity are utilized to simulate synaptic plasticity dynamics to ensure that the output error is minimized.",2023-03-26,http://arxiv.org/abs/2303.14705v1,Mohammad Modiri,arxiv.org,"cs.NE, cs.AI, cs.LG, cs.RO, cs.SY, eess.SY"
Neuroscience needs Network Science,"The brain is a complex system comprising a myriad of interacting elements, posing significant challenges in understanding its structure, function, and dynamics. Network science has emerged as a powerful tool for studying such intricate systems, offering a framework for integrating multiscale data and complexity. Here, we discuss the application of network science in the study of the brain, addressing topics such as network models and metrics, the connectome, and the role of dynamics in neural networks. We explore the challenges and opportunities in integrating multiple data streams for understanding the neural transitions from development to healthy function to disease, and discuss the potential for collaboration between network science and neuroscience communities. We underscore the importance of fostering interdisciplinary opportunities through funding initiatives, workshops, and conferences, as well as supporting students and postdoctoral fellows with interests in both disciplines. By uniting the network science and neuroscience communities, we can develop novel network-based methods tailored to neural circuits, paving the way towards a deeper understanding of the brain and its functions.",2023-05-10,http://arxiv.org/abs/2305.06160v2,"Dániel L Barabási, Ginestra Bianconi, Ed Bullmore, Mark Burgess, SueYeon Chung, Tina Eliassi-Rad, Dileep George, István A. Kovács, Hernán Makse, Christos Papadimitriou, Thomas E. Nichols, Olaf Sporns, Kim Stachenfeld, Zoltán Toroczkai, Emma K. Towlson, Anthony M Zador, Hongkui Zeng, Albert-László Barabási, Amy Bernard, György Buzsáki",arxiv.org,q-bio.NC
Explaining V1 Properties with a Biologically Constrained Deep Learning   Architecture,"Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties. Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1. The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence.",2023-05-18,http://arxiv.org/abs/2305.11275v2,"Galen Pogoncheff, Jacob Granley, Michael Beyeler",arxiv.org,"q-bio.NC, cs.AI, eess.IV"
Theoretical foundations of studying criticality in the brain,"Criticality is hypothesized as a physical mechanism underlying efficient transitions between cortical states and remarkable information processing capacities in the brain. While considerable evidence generally supports this hypothesis, non-negligible controversies persist regarding the ubiquity of criticality in neural dynamics and its role in information processing. Validity issues frequently arise during identifying potential brain criticality from empirical data. Moreover, the functional benefits implied by brain criticality are frequently misconceived or unduly generalized. These problems stem from the non-triviality and immaturity of the physical theories that analytically derive brain criticality and the statistic techniques that estimate brain criticality from empirical data. To help solve these problems, we present a systematic review and reformulate the foundations of studying brain criticality, i.e., ordinary criticality (OC), quasi-criticality (qC), self-organized criticality (SOC), and self-organized quasi-criticality (SOqC), using the terminology of neuroscience. We offer accessible explanations of the physical theories and statistic techniques of brain criticality, providing step-by-step derivations to characterize neural dynamics as a physical system with avalanches. We summarize error-prone details and existing limitations in brain criticality analysis and suggest possible solutions. Moreover, we present a forward-looking perspective on how optimizing the foundations of studying brain criticality can deepen our understanding of various neuroscience questions.",2023-06-09,http://arxiv.org/abs/2306.05635v1,"Yang Tian, Zeren Tan, Hedong Hou, Guoqi Li, Aohua Cheng, Yike Qiu, Kangyu Weng, Chun Chen, Pei Sun",arxiv.org,"q-bio.NC, cond-mat.dis-nn, cond-mat.other, cond-mat.stat-mech, physics.bio-ph"
Return to Lacan: an approach to digital twin mind with free energy   principle,"Free energy principle (FEP) is a burgeoning theory in theoretical neuroscience that provides a universal law for modelling living systems of any scale. Expecting a digital twin mind from this first principle, we propose a macro-level interpretation that bridge neuroscience and psychoanalysis through the lens of computational Lacanian psychoanalysis. In this article, we claim three fundamental parallels between FEP and Lacanian psychoanalysis, and suggest a FEP approach to formalizing Lacan's theory. Sharing the non-linear temporal structure that combines prediction and retrospection (logical time), both of two theories focus on epistemological questions that how systems represented themselves and external world, and those elements failed to be represented (lacks and free energy) significantly influence the systems' subsequent states. Additionally, the fundamental hypothesis of FEP that the precise state of environment is always concealed, accounts for object petit a, the core concept in Lacan's theory. With neuropsychoanalytic mapping from three orders (the Real, the Symbolic, and the Imaginary, RSI) onto brain regions, we propose a brain-wide FEP model for a minimal definition of Lacanian mind - composite state of RSI that is perturbated by desire running over the logical time. The FEP-RSI model involves three FEP units connected by respective free energy with a natural compliance with logical time, mimicking core dynamics of Lacanian mind. The biological plausibility of current model is considered from perspectives of cognitive neuroscience. In conclusion, the FEP-RSI model encapsulates a unified framework for digital twin modeling at the macro level.",2023-09-13,http://arxiv.org/abs/2309.06707v2,"Lingyu Li, Chunbo Li",arxiv.org,q-bio.NC
A Goal-Driven Approach to Systems Neuroscience,"Humans and animals exhibit a range of interesting behaviors in dynamic environments, and it is unclear how our brains actively reformat this dense sensory information to enable these behaviors. Experimental neuroscience is undergoing a revolution in its ability to record and manipulate hundreds to thousands of neurons while an animal is performing a complex behavior. As these paradigms enable unprecedented access to the brain, a natural question that arises is how to distill these data into interpretable insights about how neural circuits give rise to intelligent behaviors. The classical approach in systems neuroscience has been to ascribe well-defined operations to individual neurons and provide a description of how these operations combine to produce a circuit-level theory of neural computations. While this approach has had some success for small-scale recordings with simple stimuli, designed to probe a particular circuit computation, often times these ultimately lead to disparate descriptions of the same system across stimuli. Perhaps more strikingly, many response profiles of neurons are difficult to succinctly describe in words, suggesting that new approaches are needed in light of these experimental observations. In this thesis, we offer a different definition of interpretability that we show has promise in yielding unified structural and functional models of neural circuits, and describes the evolutionary constraints that give rise to the response properties of the neural population, including those that have previously been difficult to describe individually. We demonstrate the utility of this framework across multiple brain areas and species to study the roles of recurrent processing in the primate ventral visual pathway; mouse visual processing; heterogeneity in rodent medial entorhinal cortex; and facilitating biological learning.",2023-11-05,http://arxiv.org/abs/2311.02704v1,Aran Nayebi,arxiv.org,"q-bio.NC, cs.LG"
Clarifying the conceptual dimensions of representation in neuroscience,"Despite the centrality of the notion of representation to its explanations, neuroscience lacks a unified framework for the concepts used to characterize representation, leading to disparate use of terminology and measures associated with representation. To offer clarification, we propose a core set of conceptual dimensions that characterize representations in neuroscience. These dimensions describe relations between a neural response, features that may be represented, and downstream effects of the neural response. A neural response may be shown to be sensitive and specific to a feature, invariant to other features, and functional, which means that it is used downstream in the brain. We use information-theoretic measures to introduce these conceptual dimensions unambiguously and explain how data analysis methods such as correlational analyses, decoding and encoding models, representational similarity analysis, and tests of statistical dependence or adaptation relate to our framework. We consider several canonical examples, including the representation of orientation, numerosity, and spatial location, which illustrate how the evidence put forth in support or criticism of representational conclusions is systematized by our framework. By offering a unified conceptual framework we hope to aid the comparison and integration of results across studies and research groups and to help determine when evidence for a representational conclusion is strong.",2024-03-21,http://arxiv.org/abs/2403.14046v2,"Stephan Pohl, Edgar Y. Walker, David L. Barack, Jennifer Lee, Rachel N. Denison, Ned Block, Florent Meyniel, Wei Ji Ma",arxiv.org,q-bio.NC
covSTATIS: a multi-table technique for network neuroscience,"Similarity analyses between multiple correlation or covariance tables constitute the cornerstone of network neuroscience. Here, we introduce covSTATIS, a versatile, linear, unsupervised multi-table method designed to identify structured patterns in multi-table data, and allow for the simultaneous extraction and interpretation of both individual and group-level features. With covSTATIS, multiple similarity tables can now be easily integrated, without requiring a priori data simplification, complex black-box implementations, user-dependent specifications, or supervised frameworks. Applications of covSTATIS, a tutorial with Open Data and source code are provided. CovSTATIS offers a promising avenue for advancing the theoretical and analytic landscape of network neuroscience.",2024-03-21,http://arxiv.org/abs/2403.14481v2,"Giulia Baracchini, Ju-Chi Yu, Jenny Rieck, Derek Beaton, Vincent Guillemot, Cheryl Grady, Herve Abdi, R. Nathan Spreng",arxiv.org,q-bio.QM
Cycling on the Freeway: The Perilous State of Open Source Neuroscience   Software,"Most scientists need software to perform their research (Barker et al., 2020; Carver et al., 2022; Hettrick, 2014; Hettrick et al., 2014; Switters and Osimo, 2019), and neuroscientists are no exception. Whether we work with reaction times, electrophysiological signals, or magnetic resonance imaging data, we rely on software to acquire, analyze, and statistically evaluate the raw data we obtain - or to generate such data if we work with simulations. In recent years there has been a shift toward relying on free, open-source scientific software (FOSSS) for neuroscience data analysis (Poldrack et al., 2019), in line with the broader open science movement in academia (McKiernan et al., 2016) and wider industry trends (Eghbal, 2016). Importantly, FOSSS is typically developed by working scientists (not professional software developers) which sets up a precarious situation given the nature of the typical academic workplace (wherein academics, especially in their early careers, are on short and fixed term contracts). In this paper, we will argue that the existing ecosystem of neuroscientific open source software is brittle, and discuss why and how the neuroscience community needs to come together to ensure a healthy growth of our software landscape to the benefit of all.",2024-03-28,http://arxiv.org/abs/2403.19394v1,"Britta U. Westner, Daniel R. McCloy, Eric Larson, Alexandre Gramfort, Daniel S. Katz, Arfon M. Smith, invited co-signees",arxiv.org,"cs.CY, q-bio.OT"
Listen to the Waves: Using a Neuronal Model of the Human Auditory System   to Predict Ocean Waves,"Artificial neural networks (ANNs) have evolved from the 1940s primitive models of brain function to become tools for artificial intelligence. They comprise many units, artificial neurons, interlinked through weighted connections. ANNs are trained to perform tasks through learning rules that modify the connection weights. With these rules being in the focus of research, ANNs have become a branch of machine learning developing independently from neuroscience. Although likely required for the development of truly intelligent machines, the integration of neuroscience into ANNs has remained a neglected proposition.   Here, we demonstrate that designing an ANN along biological principles results in drastically improved task performance. As a challenging real-world problem, we choose real-time ocean-wave prediction which is essential for various maritime operations. Motivated by the similarity of ocean waves measured at a single location to sound waves arriving at the eardrum, we redesign an echo state network to resemble the brain's auditory system. This yields a powerful predictive tool which is computationally lean, robust with respect to network parameters, and works efficiently across a wide range of sea states. Our results demonstrate the advantages of integrating neuroscience with machine learning and offer a tool for use in the production of green energy from ocean waves.",2024-04-15,http://arxiv.org/abs/2404.09510v1,"Artur Matysiak, Volker Roeber, Henrik Kalisch, Reinhard König, Patrick J. C. May",arxiv.org,"eess.SP, cs.LG, cs.NE, q-bio.NC"
Biophysical effects and neuromodulatory dose of transcranial ultrasonic   stimulation,"Transcranial ultrasonic stimulation (TUS) has the potential to usher in a new era for human neuroscience by allowing spatially precise and high-resolution non-invasive targeting of both deep and superficial brain regions. Currently, fundamental research on the mechanisms of interaction between ultrasound and neural tissues is progressing in parallel with application-focused research. However, a major hurdle in the wider use of TUS is the selection of optimal parameters to enable safe and effective neuromodulation in humans. In this paper, we will discuss the major factors that determine both the safety and efficacy of TUS. We will discuss the thermal and mechanical biophysical effects of ultrasound, which underlie its biological effects, in the context of their relationships with tunable parameters. Based on this knowledge of biophysical effects, and drawing on concepts from radiotherapy, we propose a framework for conceptualising TUS dose.",2024-06-28,http://arxiv.org/abs/2406.19869v3,"Tulika Nandi, Benjamin R. Kop, Kasra Naftchi-Ardebili, Charlotte J. Stagg, Kim Butts Pauly, Lennart Verhagen",arxiv.org,"physics.bio-ph, physics.med-ph"
Expressivity of Neural Networks with Random Weights and Learned Biases,"Landmark universal function approximation results for neural networks with trained weights and biases provided the impetus for the ubiquitous use of neural networks as learning models in neuroscience and Artificial Intelligence (AI). Recent work has extended these results to networks in which a smaller subset of weights (e.g., output weights) are tuned, leaving other parameters random. However, it remains an open question whether universal approximation holds when only biases are learned, despite evidence from neuroscience and AI that biases significantly shape neural responses. The current paper answers this question. We provide theoretical and numerical evidence demonstrating that feedforward neural networks with fixed random weights can approximate any continuous function on compact sets. We further show an analogous result for the approximation of dynamical systems with recurrent neural networks. Our findings are relevant to neuroscience, where they demonstrate the potential for behaviourally relevant changes in dynamics without modifying synaptic weights, as well as for AI, where they shed light on recent fine-tuning methods for large language models, like bias and prefix-based approaches.",2024-07-01,http://arxiv.org/abs/2407.00957v3,"Ezekiel Williams, Alexandre Payeur, Avery Hee-Woon Ryoo, Thomas Jiralerspong, Matthew G. Perich, Luca Mazzucato, Guillaume Lajoie",arxiv.org,"cs.NE, q-bio.NC, stat.ML"
"ODIN: Open Data In Neurophysiology: Advancements, Solutions & Challenges","Across the life sciences, an ongoing effort over the last 50 years has made data and methods more reproducible and transparent. This openness has led to transformative insights and vastly accelerated scientific progress. For example, structural biology and genomics have undertaken systematic collection and publication of protein sequences and structures over the past half-century, and these data have led to scientific breakthroughs that were unthinkable when data collection first began. We believe that neuroscience is poised to follow the same path, and that principles of open data and open science will transform our understanding of the nervous system in ways that are impossible to predict at the moment. To this end, new social structures along with active and open scientific communities are essential to facilitate and expand the still limited adoption of open science practices in our field. Unified by shared values of openness, we set out to organize a symposium for Open Data in Neuroscience (ODIN) to strengthen our community and facilitate transformative neuroscience research at large. In this report, we share what we learned during this first ODIN event. We also lay out plans for how to grow this movement, document emerging conversations, and propose a path toward a better and more transparent science of tomorrow.",2024-07-01,http://arxiv.org/abs/2407.00976v2,"Colleen J. Gillon, Cody Baker, Ryan Ly, Edoardo Balzani, Bingni W. Brunton, Manuel Schottdorf, Satrajit Ghosh, Nima Dehghani",arxiv.org,q-bio.NC
Topological Representational Similarity Analysis in Brains and Beyond,"Understanding how the brain represents and processes information is crucial for advancing neuroscience and artificial intelligence. Representational similarity analysis (RSA) has been instrumental in characterizing neural representations, but traditional RSA relies solely on geometric properties, overlooking crucial topological information. This thesis introduces Topological RSA (tRSA), a novel framework combining geometric and topological properties of neural representations.   tRSA applies nonlinear monotonic transforms to representational dissimilarities, emphasizing local topology while retaining intermediate-scale geometry. The resulting geo-topological matrices enable model comparisons robust to noise and individual idiosyncrasies. This thesis introduces several key methodological advances: (1) Topological RSA (tRSA) for identifying computational signatures and testing topological hypotheses; (2) Adaptive Geo-Topological Dependence Measure (AGTDM) for detecting complex multivariate relationships; (3) Procrustes-aligned Multidimensional Scaling (pMDS) for revealing neural computation stages; (4) Temporal Topological Data Analysis (tTDA) for uncovering developmental trajectories; and (5) Single-cell Topological Simplicial Analysis (scTSA) for characterizing cell population complexity.   Through analyses of neural recordings, biological data, and neural network simulations, this thesis demonstrates the power and versatility of these methods in understanding brains, computational models, and complex biological systems. They not only offer robust approaches for adjudicating among competing models but also reveal novel theoretical insights into the nature of neural computation. This work lays the foundation for future investigations at the intersection of topology, neuroscience, and time series analysis, paving the way for more nuanced understanding of brain function and dysfunction.",2024-08-21,http://arxiv.org/abs/2408.11948v1,Baihan Lin,arxiv.org,"q-bio.NC, cs.LG, math.GT"
Pennsieve: A Collaborative Platform for Translational Neuroscience and   Beyond,"The exponential growth of neuroscientific data necessitates platforms that facilitate data management and multidisciplinary collaboration. In this paper, we introduce Pennsieve - an open-source, cloud-based scientific data management platform built to meet these needs. Pennsieve supports complex multimodal datasets and provides tools for data visualization and analyses. It takes a comprehensive approach to data integration, enabling researchers to define custom metadata schemas and utilize advanced tools to filter and query their data. Pennsieve's modular architecture allows external applications to extend its capabilities, and collaborative workspaces with peer-reviewed data publishing mechanisms promote high-quality datasets optimized for downstream analysis, both in the cloud and on-premises.   Pennsieve forms the core for major neuroscience research programs including NIH SPARC Initiative, NIH HEAL Initiative's PRECISION Human Pain Network, and NIH HEAL RE-JOIN Initiative. It serves more than 80 research groups worldwide, along with several large-scale, inter-institutional projects at clinical sites through the University of Pennsylvania. Underpinning the SPARC.Science, Epilepsy.Science, and Pennsieve Discover portals, Pennsieve stores over 125 TB of scientific data, with 35 TB of data publicly available across more than 350 high-impact datasets. It adheres to the findable, accessible, interoperable, and reusable (FAIR) principles of data sharing and is recognized as one of the NIH-approved Data Repositories. By facilitating scientific data management, discovery, and analysis, Pennsieve fosters a robust and collaborative research ecosystem for neuroscience and beyond.",2024-09-16,http://arxiv.org/abs/2409.10509v2,"Zack Goldblum, Zhongchuan Xu, Haoer Shi, Patryk Orzechowski, Jamaal Spence, Kathryn A Davis, Brian Litt, Nishant Sinha, Joost Wagenaar",arxiv.org,"cs.CY, cs.DB, cs.DL, cs.ET, H.2.4; H.3; J.3"
Object segmentation from common fate: Motion energy processing enables   human-like zero-shot generalization to random dot stimuli,"Humans excel at detecting and segmenting moving objects according to the Gestalt principle of ""common fate"". Remarkably, previous works have shown that human perception generalizes this principle in a zero-shot fashion to unseen textures or random dots. In this work, we seek to better understand the computational basis for this capability by evaluating a broad range of optical flow models and a neuroscience inspired motion energy model for zero-shot figure-ground segmentation of random dot stimuli. Specifically, we use the extensively validated motion energy model proposed by Simoncelli and Heeger in 1998 which is fitted to neural recordings in cortex area MT. We find that a cross section of 40 deep optical flow models trained on different datasets struggle to estimate motion patterns in random dot videos, resulting in poor figure-ground segmentation performance. Conversely, the neuroscience-inspired model significantly outperforms all optical flow models on this task. For a direct comparison to human perception, we conduct a psychophysical study using a shape identification task as a proxy to measure human segmentation performance. All state-of-the-art optical flow models fall short of human performance, but only the motion energy model matches human capability. This neuroscience-inspired model successfully addresses the lack of human-like zero-shot generalization to random dot stimuli in current computer vision models, and thus establishes a compelling link between the Gestalt psychology of human object perception and cortical motion processing in the brain.   Code, models and datasets are available at https://github.com/mtangemann/motion_energy_segmentation",2024-11-03,http://arxiv.org/abs/2411.01505v1,"Matthias Tangemann, Matthias Kümmerer, Matthias Bethge",arxiv.org,cs.CV
The tardigrade as an emerging model organism for systems neuroscience,"We present the case for developing the tardigrade (Hypsibius exemplaris) into a model organism for systems neuroscience. These microscopic, transparent animals (~300-500 microns) are among the smallest known to possess both limbs (eight) and eyes (two), with a nervous system of only a few hundred neurons organized into a multi-lobed brain, ventral nerve cord, and a series of ganglia along the body. Despite their neuroanatomical simplicity, tardigrades exhibit complex behaviors, including multi-limbed walking gaits, individual limb grasping, phototaxis, and transitions between active and dormant states. These behaviors position tardigrades as a uniquely powerful system for addressing certain fundamental questions in systems neuroscience, such as: How do nervous systems coordinate multi-limbed behaviors? How are top-down and bottom-up motor control systems integrated? How is stereovision-guided navigation implemented? What mechanisms underlie neural resilience and recovery during environmental stress? We review current knowledge of tardigrade neuroanatomy, behavior, and genomics, and we identify opportunities and challenges for leveraging their unique biology. We propose developing essential neuroscientific tools for tardigrades, including genetic engineering and live neuroimaging, alongside behavioral assays linking neural activity to outputs. Leveraging their evolutionary ties to Caenorhabditis elegans and Drosophila melanogaster, we can adapt existing toolkits to accelerate tardigrade research - providing a bridge between simpler invertebrate systems and more complex neural architectures.",2025-01-11,http://arxiv.org/abs/2501.06606v1,"Ana M. Lyons, Saul Kato",arxiv.org,q-bio.NC
Data mining the functional architecture of the brain's circuitry,"The brain is a highly complex organ consisting of a myriad of subsystems that flexibly interact and adapt over time and context to enable perception, cognition, and behavior. Understanding the multi-scale nature of the brain, i.e., how circuit- and moleclular-level interactions build up the fundamental components of brain function, holds incredible potential for developing interventions for neurodegenerative and psychiatric diseases, as well as open new understanding into our very nature. Historically technological limitations have forced systems neuroscience to be local in anatomy (localized, small neural populations in single brain areas), in behavior (studying single tasks), in time (focusing on specific stages of learning or development), and in modality (focusing on imaging single biological quantities). New developments in neural recording technology and behavioral monitoring now provide the data needed to break free of local neuroscience to global neuroscience: i.e., understanding how the brain's many subsystem interact, adapt, and change across the multitude of behaviors animals and humans must perform to thrive. Specifically, while we have much knowledge of the anatomical architecture of the brain (i.e., the hardware), we finally are approaching the data needed to find the functional architecture and discover the fundamental properties of the software that runs on the hardware. We must take this opportunity to bridge between the vast amounts of data to discover this functional architecture which will face numerous challenges from low-level data alignment up to high level questions of interpretable mathematical models of behavior that can synthesize the myriad of datasets together.",2025-01-16,http://arxiv.org/abs/2501.09684v1,Adam S. Charles,arxiv.org,"q-bio.NC, stat.AP"
Personalized Artificial General Intelligence (AGI) via   Neuroscience-Inspired Continuous Learning Systems,"Artificial Intelligence has made remarkable advancements in recent years, primarily driven by increasingly large deep learning models. However, achieving true Artificial General Intelligence (AGI) demands fundamentally new architectures rather than merely scaling up existing models. Current approaches largely depend on expanding model parameters, which improves task-specific performance but falls short in enabling continuous, adaptable, and generalized learning. Achieving AGI capable of continuous learning and personalization on resource-constrained edge devices is an even bigger challenge.   This paper reviews the state of continual learning and neuroscience-inspired AI, and proposes a novel architecture for Personalized AGI that integrates brain-like learning mechanisms for edge deployment. We review literature on continuous lifelong learning, catastrophic forgetting, and edge AI, and discuss key neuroscience principles of human learning, including Synaptic Pruning, Hebbian plasticity, Sparse Coding, and Dual Memory Systems, as inspirations for AI systems. Building on these insights, we outline an AI architecture that features complementary fast-and-slow learning modules, synaptic self-optimization, and memory-efficient model updates to support on-device lifelong adaptation.   Conceptual diagrams of the proposed architecture and learning processes are provided. We address challenges such as catastrophic forgetting, memory efficiency, and system scalability, and present application scenarios for mobile AI assistants and embodied AI systems like humanoid robots. We conclude with key takeaways and future research directions toward truly continual, personalized AGI on the edge. While the architecture is theoretical, it synthesizes diverse findings and offers a roadmap for future implementation.",2025-04-27,http://arxiv.org/abs/2504.20109v1,"Rajeev Gupta, Suhani Gupta, Ronak Parikh, Divya Gupta, Amir Javaheri, Jairaj Singh Shaktawat",arxiv.org,"cs.AI, cs.LG"
Sequential Sparsening by Successive Adaptation in Neural Populations,"In the principal cells of the insect mushroom body, the Kenyon cells (KC), olfactory information is represented by a spatially and temporally sparse code. Each odor stimulus will activate only a small portion of neurons and each stimulus leads to only a short phasic response following stimulus onset irrespective of the actual duration of a constant stimulus. The mechanisms responsible for the sparse code in the KCs are yet unresolved.   Here, we explore the role of the neuron-intrinsic mechanism of spike-frequency adaptation (SFA) in producing temporally sparse responses to sensory stimulation in higher processing stages. Our single neuron model is defined through a conductance-based integrate-and-fire neuron with spike-frequency adaptation [1]. We study a fully connected feed-forward network architecture in coarse analogy to the insect olfactory pathway. A first layer of ten neurons represents the projection neurons (PNs) of the antenna lobe. All PNs receive a step-like input from the olfactory receptor neurons, which was realized by independent Poisson processes. The second layer represents 100 KCs which converge onto ten neurons in the output layer which represents the population of mushroom body extrinsic neurons (ENs).   Our simulation result matches with the experimental observations. In particular, intracellular recordings of PNs show a clear phasic-tonic response that outlasts the stimulus [2] while extracellular recordings from KCs in the locust express sharp transient responses [3]. We conclude that the neuron-intrinsic mechanism is can explain a progressive temporal response sparsening in the insect olfactory system. Further experimental work is needed to test this hypothesis empirically.   [1] Muller et. al., Neural Comput, 19(11):2958-3010, 2007. [2] Assisi et. al., Nat Neurosci, 10(9):1176-1184, 2007. [3] Krofczik et. al. Front. Comput. Neurosci., 2(9), 2009.",2010-07-14,http://arxiv.org/abs/1007.2345v1,"Farzad Farkhooi, Eilif Muller, Martin P. Nawrot",arxiv.org,"physics.bio-ph, q-bio.NC"
Information theoretic interpretation of frequency domain connectivity   measures,"To provide adequate multivariate measures of information flow between neural structures, modified expressions of Partial Directed Coherence (PDC) and Directed Transfer Function (DTF), two popular multivariate connectivity measures employed in neuroscience, are introduced and their formal relationship to mutual information rates are proved.",2010-12-02,http://arxiv.org/abs/1012.0353v1,"Daniel Yasumasa Takahashi, Luiz Antonio Baccalá, Koichi Sameshima",arxiv.org,"math.ST, q-bio.NC, stat.TH"
Approximation by Spline Curves: towards an Application to Cognitive   Neuroscience,"We present a procedure to approximate a plane contour by piecewise polynomial functions, depending on various parameters, such as degree, number of local patches, selection of knots. This procedure aims to be adopted to study how information about shape is represented.",2015-07-14,http://arxiv.org/abs/1507.03865v1,"Maria-Laura Torrente, Stefano Anzellotti, Chiara Finocchiaro, Claudio Fontanari",arxiv.org,math.NA
Engineering an Anthropocene Citizenship Framework,"This article presents an Anthropocene citizen-cantered framework by incorporating the neuroscience of sustainability related stressors, the biology of collaboration in multi-agent ecosystems such as urban systems, and by emphasising on the importance of harnessing the collective intelligence of the crowd in addressing wicked challenges of sustainable development. The Anthropocene citizenship framework aims to transcend the cognitive model of global citizenship and sustainability to a dynamic, resilient and thriving mental model of collective cooperation.",2015-07-25,http://arxiv.org/abs/1508.03525v1,Shima Beigi,arxiv.org,"physics.soc-ph, physics.bio-ph"
Differential flatness for neuroscience population dynamics -- A   preliminary study,"The present document is devoted to structural properties of neural population dynamics and especially their differential flatness. Several applications of differential flatness in the present context can be envisioned, among which: trajectory tracking, feedforward to feedback switching, cyclic character, positivity and boundedness.",2016-12-10,http://arxiv.org/abs/1612.03314v1,Hugues Mounier,arxiv.org,"cs.SY, q-bio.NC"
Polynomials under Ornstein-Uhlenbeck noise and an application to   inference in stochastic Hodgkin-Huxley systems,"We discuss estimation problems where a polynomial is observed under Ornstein Uhlenbeck noise over a long time interval. We prove local asymptotic normality (LAN) and specify asymptotically efficient estimators. We apply this to the following problem: feeding noise into the classical (deterministic) Hodgkin Huxley model of neuroscience, we are interested in asymptotically efficient estimation of the parameters of the noise process.",2020-03-30,http://arxiv.org/abs/2003.13531v1,Reinhard Höpfner,arxiv.org,"math.PR, math.ST, stat.TH, 62F12, 60J60"
Measuring Causality: The Science of Cause and Effect,"Determining and measuring cause-effect relationships is fundamental to most scientific studies of natural phenomena. The notion of causation is distinctly different from correlation which only looks at association of trends or patterns in measurements. In this article, we review different notions of causality and focus especially on measuring causality from time series data. Causality testing finds numerous applications in diverse disciplines such as neuroscience, econometrics, climatology, physics and artificial intelligence.",2019-10-19,http://arxiv.org/abs/1910.08750v1,"Aditi Kathpalia, Nithin Nagaraj",arxiv.org,"stat.ME, stat.AP"
XOR at a Single Vertex -- Artificial Dendrites,"New to neuroscience with implications for AI, the exclusive OR, or any other Boolean gate may be biologically accomplished within a single region where active dendrites merge. This is demonstrated below using dynamic circuit analysis. Medical knowledge aside, this observation points to the possibility of specially coated conductors to accomplish artificial dendrites.",2010-04-13,http://arxiv.org/abs/1004.2280v2,John Robert Burger,arxiv.org,"cs.NE, q-bio.NC, B.6.1; C.1.3; I.2.0"
Weak entropy solutions of nonlinear reaction-hyperbolic systems for   axonal transport,"This paper is concerned with a class of nonlinear reaction-hyperbolic systems as models for axonal transport in neuroscience. We show the global existence of entropy-satisfying BV-solutions to the initial-value problems by using hyperbolic-type methods. Moreover, we rigorously justify the limit as the biochemical processes are much faster than the transport ones.",2010-06-21,http://arxiv.org/abs/1006.3994v1,"Hao Yan, Wen-An Yong",arxiv.org,math.AP
An embedded system for real-time feedback neuroscience experiments,"A complete data acquisition and signal output control system for synchronous stimuli generation, geared towards in vivo neuroscience experiments, was developed using the Terasic DE2i-150 board. All emotions and thoughts are an emergent property of the chemical and electrical activity of neurons. Most of these cells are regarded as excitable cells (spiking neurons), which produce temporally localized electric patterns (spikes). Researchers usually consider that only the instant of occurrence (timestamp) of these spikes encodes information. Registering neural activity evoked by stimuli demands timing determinism and data storage capabilities that cannot be met without dedicated hardware and a hard real-time operational system (RTOS). Indeed, research in neuroscience usually requires dedicated electronic instrumentation for studies in neural coding, brain machine interfaces and closed loop in vivo or in vitro experiments. We developed a complete embedded system solution consisting of a hardware/software co-design with the Intel Atom processor running a free RTOS and a FPGA communicating via a PCIe-to-Avalon bridge. Our system is capable of registering input event timestamps with 1{\mu}s precision and digitally generating stimuli output in hard real-time. The whole system is controlled by a Linux-based Graphical User Interface (GUI). Collected results are simultaneously saved in a local file and broadcasted wirelessly to mobile device web-browsers in an user-friendly graphic format, enhanced by HTML5 technology. The developed system is low-cost and highly configurable, enabling various neuroscience experimental setups, while the commercial off-the-shelf systems have low availability and are less flexible to adapt to specific experimental configurations.",2015-04-03,http://arxiv.org/abs/1504.00932v1,"Lirio Onofre Baptista de Almeida, Paulo Matias, Rafael Tuma Guariento",arxiv.org,"q-bio.QM, cs.OH"
Phase-of-firing code,Definition. The phase-of-firing code is a neural coding scheme whereby neurons encode information using the time at which they fire spikes within a cycle of the ongoing oscillatory pattern of network activity. This coding scheme may allow neurons to use their temporal pattern of spikes to encode information that is not encoded in their firing rate.,2015-04-15,http://arxiv.org/abs/1504.03954v1,"Anna Cattani, Gaute T. Einevoll, Stefano Panzeri",arxiv.org,q-bio.NC
The Making of a Creative Worldview,"Research at the interface between cognitive psychology, neuroscience, and the science of complex, dynamical systems, is piecing together an understanding of the creative process, including how it works, how it can be fostered, and the developmental antecedents and personality traits of particularly creative people. This chapter examines the workings of creative minds, those with the potential to significantly impact the evolution of human culture.",2018-11-27,http://arxiv.org/abs/1811.11236v1,Liane Gabora,arxiv.org,q-bio.NC
A Computational Model for Machine Thinking,A machine thinking model is proposed in this report based on recent advances of computer vision and the recent results of neuroscience devoted to brain understanding. We deliver the result of machine thinking in the form of sentences of natural-language or drawn sketches either informative or decisional. This result is obtained from a reasoning performed on new acquired data and memorized data.,2022-01-20,http://arxiv.org/abs/2201.08122v1,Slimane Larabi,arxiv.org,cs.CV
"RaspberryPi to measure EEG, ECG, EMG and EOG with Shield PiEEG","In this paper presented hardware and software for shield PiEEG for reading signals through the families of single-board computers - RaspberryPi, OrangePi, BananaPi, etc. For the most part, the paper provides technical information on how to implement this device. This device is designed to be familiar with neuroscience and is one of the easiest ways to get started with EEG measurements.",2023-01-01,http://arxiv.org/abs/2301.03459v1,Ildar Rakhmatulin,arxiv.org,"eess.SP, cs.HC"
Inference of Abstraction for a Unified Account of Symbolic Reasoning   from Data,"Inspired by empirical work in neuroscience for Bayesian approaches to brain function, we give a unified probabilistic account of various types of symbolic reasoning from data. We characterise them in terms of formal logic using the classical consequence relation, an empirical consequence relation, maximal consistent sets, maximal possible sets and maximum likelihood estimation. The theory gives new insights into reasoning towards human-like machine intelligence.",2024-02-13,http://arxiv.org/abs/2402.08646v1,Hiroyuki Kido,arxiv.org,cs.AI
Large language models surpass human experts in predicting neuroscience   results,"Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. To evaluate this possibility, we created BrainBench, a forward-looking benchmark for predicting neuroscience results. We find that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries. Our approach is not neuroscience-specific and is transferable to other knowledge-intensive endeavors.",2024-03-04,http://arxiv.org/abs/2403.03230v4,"Xiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin K. Nejad, Felipe Yáñez, Bati Yilmaz, Kangjoo Lee, Alexandra O. Cohen, Valentina Borghesani, Anton Pashkov, Daniele Marinazzo, Jonathan Nicholas, Alessandro Salatiello, Ilia Sucholutsky, Pasquale Minervini, Sepehr Razavi, Roberta Rocca, Elkhan Yusifov, Tereza Okalova, Nianlong Gu, Martin Ferianc, Mikail Khona, Kaustubh R. Patil, Pui-Shee Lee, Rui Mata, Nicholas E. Myers, Jennifer K Bizley, Sebastian Musslick, Isil Poyraz Bilgin, Guiomar Niso, Justin M. Ales, Michael Gaebler, N Apurva Ratan Murty, Leyla Loued-Khenissi, Anna Behler, Chloe M. Hall, Jessica Dafflon, Sherry Dongqi Bao, Bradley C. Love",arxiv.org,"q-bio.NC, cs.AI"
Statistics of Extremes for Neuroscience,"This chapter illustrates how tools from univariate and multivariate statistics of extremes can complement classical methods used to study brain signals and enhance the understanding of brain activity and connectivity during specific cognitive tasks or abnormal episodes, such as an epileptic seizure.",2024-04-14,http://arxiv.org/abs/2404.09157v1,"Paolo V. Redondo, Matheus B. Guerrero, Raphaël Huser, Hernando Ombao",arxiv.org,"stat.AP, stat.ME"
On logic and generative AI,"A hundred years ago, logic was almost synonymous with foundational studies. The ongoing AI revolution raises many deep foundational problems involving neuroscience, philosophy, computer science, and logic. The goal of the following dialog is to provoke young logicians with a taste for foundations to notice the foundational problems raised by the AI revolution.",2024-09-22,http://arxiv.org/abs/2409.14465v1,"Yuri Gurevich, Andreas Blass",arxiv.org,"cs.AI, cs.LO"
Vector Symbolic Architectures answer Jackendoff's challenges for   cognitive neuroscience,Jackendoff (2002) posed four challenges that linguistic combinatoriality and rules of language present to theories of brain function. The essence of these problems is the question of how to neurally instantiate the rapid construction and transformation of the compositional structures that are typically taken to be the domain of symbolic processing. He contended that typical connectionist approaches fail to meet these challenges and that the dialogue between linguistic theory and cognitive neuroscience will be relatively unproductive until the importance of these problems is widely recognised and the challenges answered by some technical innovation in connectionist modelling. This paper claims that a little-known family of connectionist models (Vector Symbolic Architectures) are able to meet Jackendoff's challenges.,2004-12-13,http://arxiv.org/abs/cs/0412059v1,Ross W. Gayler,arxiv.org,"cs.NE, cs.AI, I.5.1; I.2.0, I.2.6"
Measuring spike train synchrony,"Estimating the degree of synchrony or reliability between two or more spike trains is a frequent task in both experimental and computational neuroscience. In recent years, many different methods have been proposed that typically compare the timing of spikes on a certain time scale to be fixed beforehand. Here, we propose the ISI-distance, a simple complementary approach that extracts information from the interspike intervals by evaluating the ratio of the instantaneous frequencies. The method is parameter free, time scale independent and easy to visualize as illustrated by an application to real neuronal spike trains obtained in vitro from rat slices. In a comparison with existing approaches on spike trains extracted from a simulated Hindemarsh-Rose network, the ISI-distance performs as well as the best time-scale-optimized measure based on spike timing.",2007-01-23,http://arxiv.org/abs/physics/0701261v3,"Thomas Kreuz, Julie S. Haas, Alice Morelli, Henry D. I. Abarbanel, Antonio Politi",arxiv.org,"physics.bio-ph, physics.data-an, q-bio.NC"
Nonsmooth Formulation of the Support Vector Machine for a Neural   Decoding Problem,"This paper formulates a generalized classification algorithm with an application to classifying (or `decoding') neural activity in the brain. Medical doctors and researchers have long been interested in how brain activity correlates to body movement. Experiments have been conducted on patients whom are unable to move, in order to gain insight as to how thinking about movements might generate discernable neural activity. Researchers are tasked with determining which neurons are responsible for different imagined movements and how the firing behavior changes, given neural firing data. For instance, imagined movements may include wrist flexion, elbow extension, or closing the hand. This is just one of many applications to data classification. Though this article deals with an application in neuroscience, the generalized algorithm proposed in this article has applications in scientific areas ranging from neuroscience to acoustic and medical imaging.",2010-12-05,http://arxiv.org/abs/1012.0958v1,"Cary Humber, Kazufumi Ito, Chad Bouton",arxiv.org,"math.OC, math.NA, math.ST, stat.TH, 65K10, 49N45"
Emergence of Intrinsic Representations of Images by Feedforward and   Feedback Processes and Bioluminescent Photons in Early Retinotopic Areas,"Recently, we put forwarded a redox molecular hypothesis involving the natural biophysical substrate of visual perception and imagery. Here, we explicitly propose that the feedback and feedforward iterative operation processes can be interpreted in terms of a homunculus looking at the biophysical picture in our brain during visual imagery. We further propose that the brain can use both picture-like and language-like representation processes. In our interpretation, visualization (imagery) is a special kind of representation i.e., visual imagery requires a peculiar inherent biophysical (picture-like) mechanism. We also conjecture that the evolution of higher levels of complexity made the biophysical picture representation of the external visual world possible by controlled redox and bioluminescent nonlinear (iterative) biochemical reactions in the V1 and V2 areas during visual imagery. Our proposal deals only with the primary level of visual representation (i.e. perceived ""scene"").",2010-12-16,http://arxiv.org/abs/1012.3618v1,"I. Bokkon, V. Salari, J. Tuszynski",arxiv.org,q-bio.NC
Gain control in molecular information processing: Lessons from   neuroscience,"Statistical properties of environments experienced by biological signaling systems in the real world change, which necessitate adaptive responses to achieve high fidelity information transmission. One form of such adaptive response is gain control. Here we argue that a certain simple mechanism of gain control, understood well in the context of systems neuroscience, also works for molecular signaling. The mechanism allows to transmit more than one bit (on or off) of information about the signal independently of the signal variance. It does not require additional molecular circuitry beyond that already present in many molecular systems, and, in particular, it does not depend on existence of feedback loops. The mechanism provides a potential explanation for abundance of ultrasensitive response curves in biological regulatory networks.",2011-08-03,http://arxiv.org/abs/1108.0736v1,Ilya Nemenman,arxiv.org,q-bio.MN
One dimensional Fokker-Planck reduced dynamics of decision making models   in Computational Neuroscience,"We study a Fokker-Planck equation modelling the firing rates of two interacting populations of neurons. This model arises in computational neuroscience when considering, for example, bistable visual perception problems and is based on a stochastic Wilson-Cowan system of differential equations. In a previous work, the slow-fast behavior of the solution of the Fokker-Planck equation has been highlighted. Our aim is to demonstrate that the complexity of the model can be drastically reduced using this slow-fast structure. In fact, we can derive a one-dimensional Fokker-Planck equation that describes the evolution of the solution along the so-called slow manifold. This permits to have a direct efficient determination of the equilibrium state and its effective potential, and thus to investigate its dependencies with respect to various parameters of the model. It also allows to obtain information about the time escaping behavior. The results obtained for the reduced 1D equation are validated with those of the original 2D equation both for equilibrium and transient behavior.",2011-12-16,http://arxiv.org/abs/1112.3794v1,"José Antonio Carrillo, Stéphane Cordier, Simona Mancini",arxiv.org,math.AP
The neuron's response at extended timescales,"Many systems are modulated by unknown slow processes. This hinders analysis in highly non-linear systems, such as excitable systems. We show that for such systems, if the input matches the sparse `spiky' nature of the output, the spiking input-output relation can be derived. We use this relation to reproduce and interpret the irregular and complex 1/f response observed in isolated neurons stimulated over days. We decompose the neuronal response into contributions from its long history of internal noise and its short (few minutes) history of inputs, quantifying memory, noise and stability.",2013-01-11,http://arxiv.org/abs/1301.2631v2,"Daniel Soudry, Ron Meir",arxiv.org,"q-bio.NC, q-bio.QM"
A Brief History of Excitable Map-Based Neurons and Neural Networks,"This review gives a short historical account of the excitable maps approach for modeling neurons and neuronal networks. Some early models, due to Pasemann (1993), Chialvo (1995) and Kinouchi and Tragtenberg (1996), are compared with more recent proposals by Rulkov (2002) and Izhikevich (2003). We also review map-based schemes for electrical and chemical synapses and some recent findings as critical avalanches in map-based neural networks. We conclude with suggestions for further work in this area like more efficient maps, compartmental modeling and close dynamical comparison with conductance-based models.",2013-03-01,http://arxiv.org/abs/1303.0256v1,"M. Girardi-Schappo, M. H. R. Tragtenberg, O. Kinouchi",arxiv.org,"q-bio.NC, cond-mat.dis-nn, nlin.CD"
"Sequential sampling models in computational psychiatry: Bayesian   parameter estimation, model selection and classification","Current psychiatric research is in crisis. In this review I will describe the causes of this crisis and highlight recent efforts to overcome current challenges. One particularly promising approach is the emerging field of computational psychiatry. By using methods and insights from computational cognitive neuroscience, computational psychiatry might enable us to move from a symptom-based description of mental illness to descriptors based on objective computational multidimensional functional variables. To exemplify this I will survey recent efforts towards this goal. I will then describe a set of methods that together form a toolbox of cognitive models to aid this research program. At the core of this toolbox are sequential sampling models which have been used to explain diverse cognitive neuroscience phenomena but have so far seen little adoption in psychiatric research. I will then describe how these models can be fitted to subject data and highlight how hierarchical Bayesian estimation provides a rich framework with many desirable properties and benefits compared to traditional optimization-based approaches. Finally, non-parametric Bayesian methods provide general solutions to the problem of classifying mental illness within this framework.",2013-03-22,http://arxiv.org/abs/1303.5616v1,Thomas V. Wiecki,arxiv.org,q-bio.NC
Replicating and Applying a Neuro-Cognitive Experimental Technique in HCI   Research,"In cognitive neuroscience the sense of agency is defined as the as the experience of controlling ones own actions and, through this control, affecting the external world. At CHI 2012 I presented a paper entitled I did that! Measuring Users Experience of Agency in their own Actions [1]. This extended abstract draws heavily on that paper, which described an implicit measure called intentional binding. This measure, developed by researchers in cognitive neuroscience, has been shown to provide a robust implicit measure for the sense of agency. My interest in intentional binding stemmed from prior HCI literature, (e.g. the work of Shneiderman) which emphasises the importance of the sense of control in human-computer interactions. The key question behind the CHI 2012 paper was: can we apply intention binding to provide an implicit measure for the experience of control in human-computer interactions? In investigating this question, replication was a key element of the experimental process.",2013-07-11,http://arxiv.org/abs/1307.3166v1,David Coyle,arxiv.org,"cs.HC, H.5.2"
On the Technology Prospects and Investment Opportunities for Scalable   Neuroscience,"Two major initiatives to accelerate research in the brain sciences have focused attention on developing a new generation of scientific instruments for neuroscience. These instruments will be used to record static (structural) and dynamic (behavioral) information at unprecedented spatial and temporal resolution and report out that information in a form suitable for computational analysis. We distinguish between recording - taking measurements of individual cells and the extracellular matrix - and reporting - transcoding, packaging and transmitting the resulting information for subsequent analysis - as these represent very different challenges as we scale the relevant technologies to support simultaneously tracking the many neurons that comprise neural circuits of interest. We investigate a diverse set of technologies with the purpose of anticipating their development over the span of the next 10 years and categorizing their impact in terms of short-term [1-2 years], medium-term [2-5 years] and longer-term [5-10 years] deliverables.",2013-07-27,http://arxiv.org/abs/1307.7302v1,"Thomas Dean, Biafra Ahanonu, Mainak Chowdhury, Anjali Datta, Andre Esteva, Daniel Eth, Nobie Redmon, Oleg Rumyantsev, Ysis Tarter",arxiv.org,q-bio.NC
Topological determinants of self-sustained activity in a simple model of   excitable dynamics on graphs,"Models of simple excitable dynamics on graphs are an efficient framework for studying the interplay between network topology and dynamics. This subject is a topic of practical relevance to diverse fields, ranging from neuroscience to engineering. Here we analyze how a single excitation propagates through a random network as a function of the excitation threshold, that is, the relative amount of activity in the neighborhood required for an excitation of a node. Using numerical simulations and analytical considerations, we can understand the onset of sustained activity as an interplay between topological cycle statistics and path statistics. Our findings are interpreted in the context of the theory of network reverberations in neural systems, which is a question of long-standing interest in computational neuroscience.",2014-03-24,http://arxiv.org/abs/1403.6174v2,"C. Fretter, A. Lesne, C. C. Hilgetag, M. -Th. Hütt",arxiv.org,"q-bio.NC, nlin.AO, physics.soc-ph, q-bio.MN"
How to test cognitive theory with fMRI,"The objective of this chapter is to provide a guide to using functional magnetic resonance imaging (fMRI) to inform cognitive theory. This is, of course, a daunting task, as the premise itself - that fMRI data can inform cognitive theory - is still actively debated. Below, we touch on this debate as a means of framing our guide. In particular, we argue that cognitive theories can be constrained by neuroscientific data, including that offered by fMRI, but to do so requires embellishing the cognitive theory so that it can make predictions for neuroscience; much the same as how testing a cognitive theory using behavior requires embellishing that theory to make experimentally realizable behavioral predictions (i.e., the process of generating operational definitions). Moreover, recent years have seen the development of several new approaches that allow fMRI to better test neurally-embellished models. Along with a review of several ways of testing neurally-embellished cognitive theory using fMRI, we also consider the inferential challenges that can accompany these approaches. Readers of this chapter should gain an understanding of both of the potential power and the challenges associated with fMRI as a cognitive neuroscience methodology.",2014-04-10,http://arxiv.org/abs/1404.2917v2,"Christopher H. Chatham, David Badre",arxiv.org,q-bio.NC
Probabilistic inference in discrete spaces can be implemented into   networks of LIF neurons,"The means by which cortical neural networks are able to efficiently solve inference problems remains an open question in computational neuroscience. Recently, abstract models of Bayesian computation in neural circuits have been proposed, but they lack a mechanistic interpretation at the single-cell level. In this article, we describe a complete theoretical framework for building networks of leaky integrate-and-fire neurons that can sample from arbitrary probability distributions over binary random variables. We test our framework for a model inference task based on a psychophysical phenomenon (the Knill-Kersten optical illusion) and further assess its performance when applied to randomly generated distributions. As the local computations performed by the network strongly depend on the interaction between neurons, we compare several types of couplings mediated by either single synapses or interneuron chains. Due to its robustness to substrate imperfections such as parameter noise and background noise correlations, our model is particularly interesting for implementation on novel, neuro-inspired computing architectures, which can thereby serve as a fast, low-power substrate for solving real-world inference problems.",2014-10-20,http://arxiv.org/abs/1410.5212v2,"Dimitri Probst, Mihai A. Petrovici, Ilja Bytschok, Johannes Bill, Dejan Pecevski, Johannes Schemmel, Karlheinz Meier",arxiv.org,q-bio.NC
Multivariate wavelet Whittle estimation in long-range dependence,"Multivariate processes with long-range dependent properties are found in a large number of applications including finance, geophysics and neuroscience. For real data applications, the correlation between time series is crucial. Usual estimations of correlation can be highly biased due to phase-shifts caused by the differences in the properties of autocorrelation in the processes. To address this issue, we introduce a semiparametric estimation of multivariate long-range dependent processes. The parameters of interest in the model are the vector of the long-range dependence parameters and the long-run covariance matrix, also called functional connectivity in neuroscience. This matrix characterizes coupling between time series. The proposed multivariate wavelet-based Whittle estimation is shown to be consistent for the estimation of both the long-range dependence and the covariance matrix and to encompass both stationary and nonstationary processes. A simulation study and a real data example are presented to illustrate the finite sample behaviour.",2014-12-01,http://arxiv.org/abs/1412.0391v2,"Sophie Achard, Irène Gannaz",arxiv.org,"math.ST, stat.TH, 60G22, 62M10, 62M15, 62H20, 92C55"
Estimating Information-Theoretic Quantities,"Information theory is a practical and theoretical framework developed for the study of communication over noisy channels. Its probabilistic basis and capacity to relate statistical structure to function make it ideally suited for studying information flow in the nervous system. It has a number of useful properties: it is a general measure sensitive to any relationship, not only linear effects; it has meaningful units which in many cases allow direct comparison between different experiments; and it can be used to study how much information can be gained by observing neural responses in single trials, rather than in averages over multiple trials. A variety of information theoretic quantities are in common use in neuroscience - (see entry ""Summary of Information-Theoretic Quantities""). Estimating these quantities in an accurate and unbiased way from real neurophysiological data frequently presents challenges, which are explained in this entry.",2015-01-08,http://arxiv.org/abs/1501.01863v1,"Robin A. A. Ince, Simon R. Schultz, Stefano Panzeri",arxiv.org,q-bio.NC
Quantifying Neural Efficiency and Capacity: A Differential Equation   Interpretation of Polynomial Contrasts,"Task based neuroimaging tools for the study of cognitive neuroscience provide insight into understanding how the brain responds to increasing cognitive demand. Theoretical models of neural-cognitive relationships have been developed based on observations of linear and non-linear increases in brain activity. Neural efficiency and capacity are two parameters of current theoretical models. These two theoretical parameters describe the rate of increase of brain activity and the upper limits of the increases, respectively. The current work demonstrates that a quadratic model of increasing brain activity in response to the n-back task is a solution to a differential equation model. This reinterpretation of a standard approach to analyzing a common cognitive task provides a wealth of new insight. The results include brain wide measures of neural efficiency and capacity. The quantification of neural-cognitive relationships provides evidence to support current cognitive neuroscience theories. In addition, the methods provide a framework for understanding the neural mechanisms of working memory. This allows estimation of the effects of experimental manipulations within a conceptual research framework. The proposed methods were applied to twenty-one healthy young adults while engaging in four levels of the n-back task. All methods are easily applicable using standard current software packages for neuroimaging.",2016-06-20,http://arxiv.org/abs/1606.06249v1,"Jason Steffener, Karen Li, Syrina Alain, Johannes Frasnelli",arxiv.org,"q-bio.QM, q-bio.NC"
Analytical modelling of temperature effects on synapses,"It was previously reported, that temperature may significantly influence neural dynamics on different levels of brain modelling. Due to this fact, while creating the model in computational neuroscience we would like to make it scalable for wide-range of various brain temperatures. However currently, because of a lack of experimental data and an absence of analytical model describing temperature influence on synapses, it is not possible to include temperature effects on multi-neuron modelling level. In this paper, we propose first step to deal with this problem: new analytical model of AMPA-type synaptic conductance, which is able to include temperature effects in low-frequency stimulations. It was constructed on basis of Markov model description of AMPA receptor kinetics and few simplifications motivated both experimentally and from Monte Carlo simulation of synaptic transmission. The model may be used for efficient and accurate implementation of temperature effects on AMPA receptor conductance in large scale neural network simulations. This in fact, opens wide-range of new possibilities for researching an influence of temperature on brain functioning.",2016-10-03,http://arxiv.org/abs/1610.00611v1,"Dominik S. Kufel, Grzegorz M. Wojcik",arxiv.org,"q-bio.NC, physics.bio-ph"
Mind Control as a Guide for the Mind,"The human brain is a complex network that supports mental function. The nascent field of network neuroscience applies tools from mathematics to neuroimaging data in the hopes of shedding light on cognitive function. A critical question arising from these empirical studies is how to modulate a human brain network to treat cognitive deficits or enhance mental abilities. While historically a number of tools have been employed to modulate mental states (such as cognitive behavioral therapy and brain stimulation), theoretical frameworks to guide these interventions - and to optimize them for clinical use - are fundamentally lacking. One promising and as-yet underexplored approach lies in a sub-discipline of engineering known as network control theory. Here, we posit that network control fundamentally relates to mind control, and that this relationship highlights important areas for future empirical research and opportunities to translate knowledge in practical domains. We clarify the conceptual intersection between neuroanatomy, cognition, and control engineering in the context of network neuroscience. Finally, we discuss the challenges, ethics, and promises of mind control.",2016-10-13,http://arxiv.org/abs/1610.04134v2,"John D. Medaglia, Perry Zurn, Walter Sinnott-Armstrong, Danielle S. Bassett",arxiv.org,q-bio.NC
Free-Endpoint Optimal Control of Inhomogeneous Bilinear Ensemble Systems,"Optimal control of bilinear systems has been a well-studied subject in the areas of mathematical and computational optimal control. However, effective methods for solving emerging optimal control problems involving an ensemble of deterministic or stochastic bilinear systems are underdeveloped. These burgeoning problems arise in diverse applications from quantum control and molecular imaging to neuroscience. In this work, we develop an iterative method to find optimal controls for an inhomogeneous bilinear ensemble system with free-endpoint conditions. The central idea is to represent the bilinear ensemble system at each iteration as a time-varying linear ensemble system, and then solve it in an iterative manner. We analyze convergence of the iterative procedure and discuss optimality of the convergent solutions. The method is directly applicable to solve the same class of optimal control problems involving a stochastic bilinear ensemble system driven by independent additive noise processes. We demonstrate the robustness and applicability of the developed iterative method through practical control designs in neuroscience and quantum control.",2016-12-01,http://arxiv.org/abs/1612.00090v1,"Shuo Wang, Jr-Shin Li",arxiv.org,"math.OC, 93A15"
A self-calibrating method for heavy tailed data modelling. Application   in neuroscience and finance,"Modelling non-homogeneous and multi-component data is a problem that challenges scientific researchers in several fields. In general, it is not possible to find a simple and closed form probabilistic model to describe such data. That is why one often resorts to non-parametric approaches. However, when the multiple components are separable, parametric modelling becomes again tractable. In this study, we propose a self-calibrating method to model multi-component data that exhibit heavy tails. We introduce a three-component hybrid distribution: a Gaussian distribution is linked to a Generalized Pareto one via an exponential distribution that bridges the gap between mean and tail behaviors. An unsupervised algorithm is then developed for estimating the parameters of this model. We study analytically and numerically its convergence. The effectiveness of the self-calibrating method is tested on simulated data, before applying it to real data from neuroscience and finance, respectively. A comparison with other standard Extreme Value Theory approaches confirms the relevance and the practical advantage of this new method.",2016-12-12,http://arxiv.org/abs/1612.03974v2,"Nehla Debbabi, Marie Kratz, Mamadou Mboup",arxiv.org,"stat.ME, math.NA, 60G70, 62E20, 62F35, 62P05, 62P10, 65D15, 68W40"
Emerging Frontiers of Neuroengineering: A Network Science of Brain   Connectivity,"Neuroengineering is faced with unique challenges in repairing or replacing complex neural systems that are composed of many interacting parts. These interactions form intricate patterns over large spatiotemporal scales, and produce emergent behaviors that are difficult to predict from individual elements. Network science provides a particularly appropriate framework in which to study and intervene in such systems, by treating neural elements (cells, volumes) as nodes in a graph and neural interactions (synapses, white matter tracts) as edges in that graph. Here, we review the emerging discipline of network neuroscience, which uses and develops tools from graph theory to better understand and manipulate neural systems, from micro- to macroscales. We present examples of how human brain imaging data is being modeled with network analysis and underscore potential pitfalls. We then highlight current computational and theoretical frontiers, and emphasize their utility in informing diagnosis and monitoring, brain-machine interfaces, and brain stimulation. A flexible and rapidly evolving enterprise, network neuroscience provides a set of powerful approaches and fundamental insights critical to the neuroengineer's toolkit.",2016-12-23,http://arxiv.org/abs/1612.08059v1,"Danielle S. Bassett, Ankit N. Khambhati, Scott T. Grafton",arxiv.org,"q-bio.NC, physics.bio-ph"
Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive   Direct Search,"Computational models in fields such as computational neuroscience are often evaluated via stochastic simulation or numerical approximation. Fitting these models implies a difficult optimization problem over complex, possibly noisy parameter landscapes. Bayesian optimization (BO) has been successfully applied to solving expensive black-box problems in engineering and machine learning. Here we explore whether BO can be applied as a general tool for model fitting. First, we present a novel hybrid BO algorithm, Bayesian adaptive direct search (BADS), that achieves competitive performance with an affordable computational overhead for the running time of typical models. We then perform an extensive benchmark of BADS vs. many common and state-of-the-art nonconvex, derivative-free optimizers, on a set of model-fitting problems with real data and models from six studies in behavioral, cognitive, and computational neuroscience. With default settings, BADS consistently finds comparable or better solutions than other methods, including `vanilla' BO, showing great promise for advanced BO techniques, and BADS in particular, as a general model-fitting tool.",2017-05-11,http://arxiv.org/abs/1705.04405v2,"Luigi Acerbi, Wei Ji Ma",arxiv.org,"stat.ML, q-bio.NC, q-bio.QM"
A Useful Motif for Flexible Task Learning in an Embodied Two-Dimensional   Visual Environment,"Animals (especially humans) have an amazing ability to learn new tasks quickly, and switch between them flexibly. How brains support this ability is largely unknown, both neuroscientifically and algorithmically. One reasonable supposition is that modules drawing on an underlying general-purpose sensory representation are dynamically allocated on a per-task basis. Recent results from neuroscience and artificial intelligence suggest the role of the general purpose visual representation may be played by a deep convolutional neural network, and give some clues how task modules based on such a representation might be discovered and constructed. In this work, we investigate module architectures in an embodied two-dimensional touchscreen environment, in which an agent's learning must occur via interactions with an environment that emits images and rewards, and accepts touches as input. This environment is designed to capture the physical structure of the task environments that are commonly deployed in visual neuroscience and psychophysics. We show that in this context, very simple changes in the nonlinear activations used by such a module can significantly influence how fast it is at learning visual tasks and how suitable it is for switching to new tasks.",2017-06-22,http://arxiv.org/abs/1706.07147v1,"Kevin T. Feigelis, Daniel L. K. Yamins",arxiv.org,"cs.LG, cs.AI, q-bio.NC, stat.ML"
Bayesian Brain meets Bayesian Recommender - Towards Systems with Empathy   for the Human Nature,"In this paper we consider the modern theory of the Bayesian brain from cognitive neurosciences in the light of recommender systems and expose potentials for our community. In particular, we elaborate on noisy user feedback and the thus resulting multicomponent user models, which have indeed a biological origin. In real user experiments we observe the impact of both factors directly in a repeated rating task along with recommendation. As a consequence, this contribution supports the plausibility of contemporary theories of mind in the context of recommender systems and can be understood as a solicitation to integrate ideas of cognitive neurosciences into our systems in order to further improve the prediction of human behaviour.",2017-06-26,http://arxiv.org/abs/1706.08319v2,"Kevin Jasberg, Sergej Sizov",arxiv.org,cs.HC
Polarization of Neural Rings,"The ""neural code"" is the way the brain characterizes, stores, and processes information. Unraveling the neural code is a key goal of mathematical neuroscience. Topology, coding theory, and, recently, commutative algebra are some the mathematical areas that are involved in analyzing these codes. Neural rings and ideals are algebraic objects that create a bridge between mathematical neuroscience and commutative algebra. A neural ideal is an ideal in a polynomial ring that encodes the combinatorial firing data of a neural code. Using some algebraic techniques one hopes to understand more about the structure of a neural code via neural rings and ideals. In this paper, we introduce an operation, called ""polarization,"" that allows us to relate neural ideals with squarefree monomial ideals, which are very well studied and known for their nice behavior in commutative algebra.",2017-06-26,http://arxiv.org/abs/1706.08559v1,"Sema Gunturkun, Jack Jeffries, Jeffrey Sun",arxiv.org,math.AC
Closing the loop between neural network simulators and the OpenAI Gym,"Since the enormous breakthroughs in machine learning over the last decade, functional neural network models are of growing interest for many researchers in the field of computational neuroscience. One major branch of research is concerned with biologically plausible implementations of reinforcement learning, with a variety of different models developed over the recent years. However, most studies in this area are conducted with custom simulation scripts and manually implemented tasks. This makes it hard for other researchers to reproduce and build upon previous work and nearly impossible to compare the performance of different learning architectures. In this work, we present a novel approach to solve this problem, connecting benchmark tools from the field of machine learning and state-of-the-art neural network simulators from computational neuroscience. This toolchain enables researchers in both fields to make use of well-tested high-performance simulation software supporting biologically plausible neuron, synapse and network models and allows them to evaluate and compare their approach on the basis of standardized environments of varying complexity. We demonstrate the functionality of the toolchain by implementing a neuronal actor-critic architecture for reinforcement learning in the NEST simulator and successfully training it on two different environments from the OpenAI Gym.",2017-09-17,http://arxiv.org/abs/1709.05650v1,"Jakob Jordan, Philipp Weidel, Abigail Morrison",arxiv.org,q-bio.NC
3D Based Landmark Tracker Using Superpixels Based Segmentation for   Neuroscience and Biomechanics Studies,"Examining locomotion has improved our basic understanding of motor control and aided in treating motor impairment. Mice and rats are premier models of human disease and increasingly the model systems of choice for basic neuroscience. High frame rates (250 Hz) are needed to quantify the kinematics of these running rodents. Manual tracking, especially for multiple markers, becomes time-consuming and impossible for large sample sizes. Therefore, the need for automatic segmentation of these markers has grown in recent years. Here, we address this need by presenting a method to segment the markers using the SLIC superpixel method. The 2D coordinates on the image plane are projected to a 3D domain using direct linear transform (DLT) and a 3D Kalman filter has been used to predict the position of markers based on the speed and position of markers from the previous frames. Finally, a probabilistic function is used to find the best match among superpixels. The method is evaluated for different difficulties for tracking of the markers and it achieves 95% correct labeling of markers.",2017-11-23,http://arxiv.org/abs/1711.08785v1,"Omid Haji Maghsoudi, Andrew Spence",arxiv.org,cs.CV
Static and dynamic measures of human brain connectivity predict   complementary aspects of human cognitive performance,"In cognitive network neuroscience, the connectivity and community structure of the brain network is related to cognition. Much of this research has focused on two measures of connectivity - modularity and flexibility - which frequently have been examined in isolation. By using resting state fMRI data from 52 young adults, we investigate the relationship between modularity, flexibility and performance on cognitive tasks. We show that flexibility and modularity are highly negatively correlated. However, we also demonstrate that flexibility and modularity make unique contributions to explain task performance, with modularity predicting performance for simple tasks and flexibility predicting performance on complex tasks that require cognitive control and executive functioning. The theory and results presented here allow for stronger links between measures of brain network connectivity and cognitive processes.",2017-11-27,http://arxiv.org/abs/1711.09841v1,"Aurora I. Ramos-Nuñez, Simon Fischer-Baum, Randi Martin, Qiuhai Yue, Fengdan Ye, Michael W. Deem",arxiv.org,q-bio.NC
Decoding the circuitry of consciousness: from local microcircuits to   brain-scale networks,"Identifying the physiological processes underlying the emergence and maintenance of consciousness is one of the most fundamental problems of neuroscience, with implications ranging from fundamental neuroscience to the treatment of patients with disorders of consciousness (DOC). One major challenge is to understand how cortical circuits at drastically different spatial scales, from local networks to brain-scale networks, operate in concert to enable consciousness, and how those processes are impaired in DOC patients. In this review, we attempt to relate available neurophysiological and clinical data with existing theoretical models of consciousness, while linking the micro- and macro-circuit levels. First, we address the relationships between awareness and wakefulness on the one hand, and cortico-cortical, and thalamo-cortical connectivity on the other hand. Second, we discuss the role of three main types of GABAergic interneurons in specific circuits responsible for the dynamical re-organization of functional networks. Third, we explore advances in the functional role of nested oscillations for neural synchronization and communication, emphasizing the importance of the balance between local (high-frequency) and distant (low-frequency) activity for efficient information processing. The clinical implications of these theoretical considerations are presented. We propose that such cellular-scale mechanisms could extend current theories of consciousness.",2019-07-26,http://arxiv.org/abs/1907.11570v1,"Julien Modolo, Mahmoud Hassan, Fabrice Wendling, Pascal Benquet",arxiv.org,q-bio.NC
The many faces of deep learning,"Deep learning has sparked a network of mutual interactions between different disciplines and AI. Naturally, each discipline focuses and interprets the workings of deep learning in different ways. This diversity of perspectives on deep learning, from neuroscience to statistical physics, is a rich source of inspiration that fuels novel developments in the theory and applications of machine learning. In this perspective, we collect and synthesize different intuitions scattered across several communities as for how deep learning works. In particular, we will briefly discuss the different perspectives that disciplines across mathematics, physics, computation, and neuroscience take on how deep learning does its tricks. Our discussion on each perspective is necessarily shallow due to the multiple views that had to be covered. The deepness in this case should come from putting all these faces of deep learning together in the reader's mind, so that one can look at the same problem from different angles.",2019-08-25,http://arxiv.org/abs/1908.10206v1,Raul Vicente,arxiv.org,"cs.LG, physics.data-an, q-bio.NC, stat.ML"
Network of Evolvable Neural Units: Evolving to Learn at a Synaptic Level,"Although Deep Neural Networks have seen great success in recent years through various changes in overall architectures and optimization strategies, their fundamental underlying design remains largely unchanged. Computational neuroscience on the other hand provides more biologically realistic models of neural processing mechanisms, but they are still high level abstractions of the actual experimentally observed behaviour. Here a model is proposed that bridges Neuroscience, Machine Learning and Evolutionary Algorithms to evolve individual soma and synaptic compartment models of neurons in a scalable manner. Instead of attempting to manually derive models for all the observed complexity and diversity in neural processing, we propose an Evolvable Neural Unit (ENU) that can approximate the function of each individual neuron and synapse. We demonstrate that this type of unit can be evolved to mimic Integrate-And-Fire neurons and synaptic Spike-Timing-Dependent Plasticity. Additionally, by constructing a new type of neural network where each synapse and neuron is such an evolvable neural unit, we show it is possible to evolve an agent capable of learning to solve a T-maze environment task. This network independently discovers spiking dynamics and reinforcement type learning rules, opening up a new path towards biologically inspired artificial intelligence.",2019-12-16,http://arxiv.org/abs/1912.07589v1,"Paul Bertens, Seong-Whan Lee",arxiv.org,"cs.LG, cs.NE, stat.ML"
A theoretical connection between the noisy leaky integrate-and-fire and   escape rate models: the non-autonomous case,"One of the most important challenges in mathematical neuroscience is to properly illustrate the stochastic nature of neurons. Among different approaches, the noisy leaky integrate-and-fire and the escape rate models are probably the most popular. These two models are usually chosen to express different noise action over the neural cell. In this paper we investigate the link between the two formalisms in the case of a neuron subject to a time dependent input. To this aim, we introduce a new general stochastic framework. As we shall prove, our general framework entails the two already existing ones. Our result has theoretical implications since it offers a general view upon the two stochastic processes mostly used in neuroscience, upon the way they can be linked, and explain their observed statistical similarity.",2017-02-05,http://arxiv.org/abs/1702.01391v1,"Grégory Dumont, Jacques Henry, Carmen Oana Tarniceriu",arxiv.org,"math.AP, q-bio.NC"
Simple Cortex: A Model of Cells in the Sensory Nervous System,"Neuroscience research has produced many theories and computational neural models of sensory nervous systems. Notwithstanding many different perspectives towards developing intelligent machines, artificial intelligence has ultimately been influenced by neuroscience. Therefore, this paper provides an introduction to biologically inspired machine intelligence by exploring the basic principles of sensation and perception as well as the structure and behavior of biological sensory nervous systems like the neocortex. Concepts like spike timing, synaptic plasticity, inhibition, neural structure, and neural behavior are applied to a new model, Simple Cortex (SC). A software implementation of SC has been built and demonstrates fast observation, learning, and prediction of spatio-temporal sensory-motor patterns and sequences. Finally, this paper suggests future areas of improvement and growth for Simple Cortex and other related machine intelligence models.",2017-10-03,http://arxiv.org/abs/1710.01347v1,David Di Giorgio,arxiv.org,"cs.AI, cs.NE"
Brain experiments imply adaptation mechanisms which outperform common AI   learning algorithms,"Attempting to imitate the brain functionalities, researchers have bridged between neuroscience and artificial intelligence for decades; however, experimental neuroscience has not directly advanced the field of machine learning. Here, using neuronal cultures, we demonstrate that increased training frequency accelerates the neuronal adaptation processes. This mechanism was implemented on artificial neural networks, where a local learning step-size increases for coherent consecutive learning steps and tested on a simple dataset of handwritten digits, MNIST. Based on our online learning results with a few handwriting examples, success rates for brain-inspired algorithms substantially outperform the commonly used machine learning algorithms. We speculate this emerging bridge from slow brain function to machine learning will promote ultrafast decision making under limited examples, which is the reality in many aspects of human activity, robotic control, and network optimization.",2020-04-23,http://arxiv.org/abs/2005.04106v1,"Shira Sardi, Roni Vardi, Yuval Meir, Yael Tugendhaft, Shiri Hodassman, Amir Goldental, Ido Kanter",arxiv.org,"q-bio.NC, physics.bio-ph"
Deep learning approaches for neural decoding: from CNNs to LSTMs and   spikes to fMRI,"Decoding behavior, perception, or cognitive state directly from neural signals has applications in brain-computer interface research as well as implications for systems neuroscience. In the last decade, deep learning has become the state-of-the-art method in many machine learning tasks ranging from speech recognition to image segmentation. The success of deep networks in other domains has led to a new wave of applications in neuroscience. In this article, we review deep learning approaches to neural decoding. We describe the architectures used for extracting useful features from neural recording modalities ranging from spikes to EEG. Furthermore, we explore how deep learning has been leveraged to predict common outputs including movement, speech, and vision, with a focus on how pretrained deep networks can be incorporated as priors for complex decoding targets like acoustic speech or images. Deep learning has been shown to be a useful tool for improving the accuracy and flexibility of neural decoding across a wide range of tasks, and we point out areas for future scientific development.",2020-05-19,http://arxiv.org/abs/2005.09687v1,"Jesse A. Livezey, Joshua I. Glaser",arxiv.org,"q-bio.NC, cs.LG"
Complexity for deep neural networks and other characteristics of deep   feature representations,"We define a notion of complexity, which quantifies the nonlinearity of the computation of a neural network, as well as a complementary measure of the effective dimension of feature representations. We investigate these observables both for trained networks for various datasets as well as explore their dynamics during training, uncovering in particular power law scaling. These observables can be understood in a dual way as uncovering hidden internal structure of the datasets themselves as a function of scale or depth. The entropic character of the proposed notion of complexity should allow to transfer modes of analysis from neuroscience and statistical physics to the domain of artificial neural networks. The introduced observables can be applied without any change to the analysis of biological neuronal systems.",2020-06-08,http://arxiv.org/abs/2006.04791v2,"Romuald A. Janik, Przemek Witaszczyk",arxiv.org,"cs.LG, cond-mat.dis-nn, cs.NE, hep-th, stat.ML"
Explainable Classification of Brain Networks via Contrast Subgraphs,"Mining human-brain networks to discover patterns that can be used to discriminate between healthy individuals and patients affected by some neurological disorder, is a fundamental task in neuroscience. Learning simple and interpretable models is as important as mere classification accuracy. In this paper we introduce a novel approach for classifying brain networks based on extracting contrast subgraphs, i.e., a set of vertices whose induced subgraphs are dense in one class of graphs and sparse in the other. We formally define the problem and present an algorithmic solution for extracting contrast subgraphs. We then apply our method to a brain-network dataset consisting of children affected by Autism Spectrum Disorder and children Typically Developed. Our analysis confirms the interestingness of the discovered patterns, which match background knowledge in the neuroscience literature. Further analysis on other classification tasks confirm the simplicity, soundness, and high explainability of our proposal, which also exhibits superior classification accuracy, to more complex state-of-the-art methods.",2020-06-09,http://arxiv.org/abs/2006.05176v1,"Tommaso Lanciano, Francesco Bonchi, Aristides Gionis",arxiv.org,"cs.SI, q-bio.NC"
"Exploration, inference and prediction in neuroscience and biomedicine","The last decades saw dramatic progress in brain research. These advances were often buttressed by probing single variables to make circumscribed discoveries, typically through null hypothesis significance testing. New ways for generating massive data fueled tension between the traditional methodology, used to infer statistically relevant effects in carefully-chosen variables, and pattern-learning algorithms, used to identify predictive signatures by searching through abundant information. In this article, we detail the antagonistic philosophies behind two quantitative approaches: certifying robust effects in understandable variables, and evaluating how accurately a built model can forecast future outcomes. We discourage choosing analysis tools via categories like 'statistics' or 'machine learning'. Rather, to establish reproducible knowledge about the brain, we advocate prioritizing tools in view of the core motivation of each quantitative analysis: aiming towards mechanistic insight, or optimizing predictive accuracy.",2019-02-21,http://arxiv.org/abs/1903.10310v1,"Danilo Bzdok, John Ioannidis",arxiv.org,"q-bio.NC, stat.AP"
Locomotion and gesture tracking in mice and small animals for   neurosceince applications: A survey,"Neuroscience has traditionally relied on manually observing lab animals in controlled environments. Researchers usually record animals behaving in free or restrained manner and then annotate the data manually. The manual annotation is not desirable for three reasons; one, it is time consuming, two, it is prone to human errors and three, no two human annotators will 100\% agree on annotation, so it is not reproducible. Consequently, automated annotation of such data has gained traction because it is efficient and replicable. Usually, the automatic annotation of neuroscience data relies on computer vision and machine leaning techniques. In this article, we have covered most of the approaches taken by researchers for locomotion and gesture tracking of lab animals. We have divided these papers in categories based upon the hardware they use and the software approach they take. We also have summarized their strengths and weaknesses.",2019-03-25,http://arxiv.org/abs/1903.10422v1,"Waseem Abbas, David Masip Rodo",arxiv.org,"cs.CV, 68Txx"
Deep neuroethology of a virtual rodent,"Parallel developments in neuroscience and deep learning have led to mutually productive exchanges, pushing our understanding of real and artificial neural networks in sensory and cognitive systems. However, this interaction between fields is less developed in the study of motor control. In this work, we develop a virtual rodent as a platform for the grounded study of motor activity in artificial models of embodied control. We then use this platform to study motor activity across contexts by training a model to solve four complex tasks. Using methods familiar to neuroscientists, we describe the behavioral representations and algorithms employed by different layers of the network using a neuroethological approach to characterize motor activity relative to the rodent's behavior and goals. We find that the model uses two classes of representations which respectively encode the task-specific behavioral strategies and task-invariant behavioral kinematics. These representations are reflected in the sequential activity and population dynamics of neural subpopulations. Overall, the virtual rodent facilitates grounded collaborations between deep reinforcement learning and motor neuroscience.",2019-11-21,http://arxiv.org/abs/1911.09451v1,"Josh Merel, Diego Aldarondo, Jesse Marshall, Yuval Tassa, Greg Wayne, Bence Ölveczky",arxiv.org,q-bio.NC
Neuron-Glial Interactions,"Although lagging behind classical computational neuroscience, theoretical and computational approaches are beginning to emerge to characterize different aspects of neuron-glial interactions. This chapter aims to provide essential knowledge on neuron-glial interactions in the mammalian brain, leveraging on computational studies that focus on structure (anatomy) and function (physiology) of such interactions in the healthy brain. Although our understanding of the need of neuron-glial interactions in the brain is still at its infancy, being mostly based on predictions that await for experimental validation, simple general modeling arguments borrowed from control theory are introduced to support the importance of including such interactions in traditional neuron-based modeling paradigms.",2020-01-19,http://arxiv.org/abs/2001.06881v1,Maurizio De Pittà,arxiv.org,q-bio.NC
"Convolutional Neural Networks as a Model of the Visual System: Past,   Present, and Future","Convolutional neural networks (CNNs) were inspired by early findings in the study of biological vision. They have since become successful tools in computer vision and state-of-the-art models of both neural activity and behavior on visual tasks. This review highlights what, in the context of CNNs, it means to be a good model in computational neuroscience and the various ways models can provide insight. Specifically, it covers the origins of CNNs and the methods by which we validate them as models of biological vision. It then goes on to elaborate on what we can learn about biological vision by understanding and experimenting on CNNs and discusses emerging opportunities for the use of CNNS in vision research beyond basic object recognition.",2020-01-20,http://arxiv.org/abs/2001.07092v2,Grace W. Lindsay,arxiv.org,"q-bio.NC, cs.CV, cs.NE"
"Rejoinder to ""Equi-energy sampler with applications in statistical   inference and statistical mechanics""","Rejoinder to ``Equi-energy sampler with applications in statistical inference and statistical mechanics'' by Kou, Zhou and Wong [math.ST/0507080]",2006-11-08,http://arxiv.org/abs/math/0611224v1,"S. C. Kou, Qing Zhou, Wing H. Wong",arxiv.org,"math.ST, stat.TH"
Upward and downward statistical continuities,"A real valued function $f$ defined on a subset $E$ of $\textbf{R}$, the set of real numbers, is statistically upward continuous if it preserves statistically upward half quasi-Cauchy sequences, is statistically downward continuous if it preserves statistically downward half quasi-Cauchy sequences; and a subset $E$ of $\textbf{R}$, is statistically upward compact if any sequence of points in $E$ has a statistically upward half quasi-Cauchy subsequence, is statistically downward compact if any sequence of points in $E$ has a statistically downward half quasi-Cauchy subsequence where a sequence $(x_{n})$ of points in $\textbf{R}$ is called statistically upward half quasi-Cauchy if \[ \lim_{n\rightarrow\infty}\frac{1}{n}|\{k\leq n: x_{k}-x_{k+1}\geq \varepsilon\}|=0 \] is statistically downward half quasi-Cauchy if \[ \lim_{n\rightarrow\infty}\frac{1}{n}|\{k\leq n: x_{k+1}-x_{k}\geq \varepsilon\}|=0 \] for every $\varepsilon>0$. We investigate statistically upward continuity, statistically downward continuity, statistically upward half compactness, statistically downward half compactness and prove interesting theorems. It turns out that uniform limit of a sequence of statistically upward continuous functions is statistically upward continuous, and uniform limit of a sequence of statistically downward continuous functions is statistically downward continuous.",2013-07-09,http://arxiv.org/abs/1307.2418v1,Huseyin Cakalli,arxiv.org,"math.GM, 26A15, 40A05, 40A30"
Applications of Information Theory: Statistics and Statistical Mechanics,The method of optimizing entropy is used to (i) conduct Asymptotic Hypothesis Testing and (ii) determine the particle distribution for which Entropy is maximized. This paper focuses on two related applications of Information Theory: Statistics and Statistical Mechanics.,2016-03-05,http://arxiv.org/abs/1603.02589v1,Khizar Qureshi,arxiv.org,"math.ST, stat.TH"
Order statistics on the spacings between order statistics for the   uniform distribution,Closed-form expressions for the distributions of the order statistics on the spacings between order statistics for the uniform distribution are obtained. This generalizes a result by Fisher concerning tests of significance in the harmonic analysis of a series.,2019-09-13,http://arxiv.org/abs/1909.06406v1,Iosif Pinelis,arxiv.org,"math.ST, stat.TH, 62E15, 62F03"
Statistical Inference: The Big Picture,"Statistics has moved beyond the frequentist-Bayesian controversies of the past. Where does this leave our ability to interpret results? I suggest that a philosophy compatible with statistical practice, labeled here statistical pragmatism, serves as a foundation for inference. Statistical pragmatism is inclusive and emphasizes the assumptions that connect statistical models with observed data. I argue that introductory courses often mischaracterize the process of statistical inference and I propose an alternative ""big picture"" depiction.",2011-06-15,http://arxiv.org/abs/1106.2895v2,Robert E. Kass,arxiv.org,"stat.OT, stat.ME"
On a geometric mean and power-law statistical distributions,For a large class of statistical systems a geometric mean value of the observables is constrained. These observables are characterized by a power-law statistical distribution.,2005-07-18,http://arxiv.org/abs/cond-mat/0507414v1,A. Rostovtsev,arxiv.org,cond-mat.stat-mech
Fisher's combined probability test for high-dimensional covariance   matrices,"Testing large covariance matrices is of fundamental importance in statistical analysis with high-dimensional data. In the past decade, three types of test statistics have been studied in the literature: quadratic form statistics, maximum form statistics, and their weighted combination. It is known that quadratic form statistics would suffer from low power against sparse alternatives and maximum form statistics would suffer from low power against dense alternatives. The weighted combination methods were introduced to enhance the power of quadratic form statistics or maximum form statistics when the weights are appropriately chosen. In this paper, we provide a new perspective to exploit the full potential of quadratic form statistics and maximum form statistics for testing high-dimensional covariance matrices. We propose a scale-invariant power enhancement test based on Fisher's method to combine the p-values of quadratic form statistics and maximum form statistics. After carefully studying the asymptotic joint distribution of quadratic form statistics and maximum form statistics, we prove that the proposed combination method retains the correct asymptotic size and boosts the power against more general alternatives. Moreover, we demonstrate the finite-sample performance in simulation studies and a real application.",2020-05-31,http://arxiv.org/abs/2006.00426v1,"Xiufan Yu, Danning Li, Lingzhou Xue",arxiv.org,"math.ST, stat.AP, stat.ME, stat.ML, stat.TH, 62H12, 60F05"
Copula representations and order statistics for conditionally   independent random variables,The copula representations for conditionally independent random variables and the distribution properties of order statistics of these random variables are studied.,2011-07-16,http://arxiv.org/abs/1107.3200v1,Ismihan Bairamov,arxiv.org,"math.ST, stat.TH"
A simple regression equivalence of Pillai's trace statistic,Derived here is a single regression coefficient equivalent to Pillai's trace statistic in multivariate analysis of variance.,2015-04-22,http://arxiv.org/abs/1504.06006v1,"Xia Shen, Zheng Ning, Yudi Pawitan",arxiv.org,"math.ST, stat.TH"
One Hundred Probability and Statistics Inequalities,"Herein we present one hundred inequalities culled from various corners of the probability, statistics, and combinatorics literature. We welcome new suggestions.",2021-02-14,http://arxiv.org/abs/2102.07234v1,CNP Slagle,arxiv.org,"math.ST, stat.TH"
"Bayesian nonparametric statistics, St-Flour lecture notes","These are lecture notes of the 51st Saint-Flour summer school, July 2023, on the topic of Bayesian nonparametric statistics",2024-02-26,http://arxiv.org/abs/2402.16422v1,Ismaël Castillo,arxiv.org,"math.ST, stat.TH"
On statistical independence and density independence,The object of observation in present paper is statistical independence of real sequences and its description as independence with re spect to certain class of densities.,2024-11-04,http://arxiv.org/abs/2411.02357v1,Milan Pasteka,arxiv.org,"math.ST, math.NT, stat.TH"
Discussion: The Dantzig selector: Statistical estimation when $p$ is   much larger than $n$,"Discussion of ""The Dantzig selector: Statistical estimation when $p$ is much larger than $n$"" [math/0506081]",2008-03-21,http://arxiv.org/abs/0803.3124v1,Peter J. Bickel,arxiv.org,"math.ST, stat.TH"
Discussion: The Dantzig selector: Statistical estimation when $p$ is   much larger than $n$,Discussion of ``The Dantzig selector: Statistical estimation when $p$ is much larger than $n$'' [math/0506081],2008-03-21,http://arxiv.org/abs/0803.3126v1,"Bradley Efron, Trevor Hastie, Robert Tibshirani",arxiv.org,"math.ST, stat.TH"
Discussion: The Dantzig selector: Statistical estimation when $p$ is   much larger than $n$,Discussion of ``The Dantzig selector: Statistical estimation when $p$ is much larger than $n$'' [math/0506081],2008-03-21,http://arxiv.org/abs/0803.3127v1,"T. Tony Cai, Jinchi Lv",arxiv.org,"math.ST, stat.TH"
Discussion: The Dantzig selector: statistical estimation when $p$ is   much larger than $n$,Discussion of ``The Dantzig selector: Statistical estimation when $p$ is much larger than $n$'' [math/0506081],2008-03-21,http://arxiv.org/abs/0803.3130v1,Ya'acov Ritov,arxiv.org,"math.ST, stat.TH"
Discussion: The Dantzig selector: Statistical estimation when $p$ is   much larger than $n$,Discussion of ``The Dantzig selector: Statistical estimation when $p$ is much larger than $n$'' [math/0506081],2008-03-21,http://arxiv.org/abs/0803.3135v1,"Michael P. Friedlander, Michael A. Saunders",arxiv.org,"math.ST, stat.TH"
Rejoinder: The Dantzig selector: Statistical estimation when $p$ is much   larger than $n$,Rejoinder to ``The Dantzig selector: Statistical estimation when $p$ is much larger than $n$'' [math/0506081],2008-03-21,http://arxiv.org/abs/0803.3136v1,"Emmanuel Candès, Terence Tao",arxiv.org,"math.ST, stat.TH"
Open statistical issues in particle physics,"Many statistical issues arise in the analysis of Particle Physics experiments. We give a brief introduction to Particle Physics, before describing the techniques used by Particle Physicists for dealing with statistical problems, and also some of the open statistical questions.",2008-11-11,http://arxiv.org/abs/0811.1663v1,Louis Lyons,arxiv.org,stat.AP
Multivariate Statistical Analysis: A Geometric Perspective,"A new, coordinate-free (geometric) approach to multivariate statistical analysis. General multivariate linear models and linear hypotheses are defined in geometric form. A method of constructing statistical criteria is defined for linear hypotheses. As a result, multivariate statistical analysis is developed in full analogy to classical statistical analysis. This approach is based on tensor products and modules over the ring of square matrices, supplied with an inner product.",2009-02-03,http://arxiv.org/abs/0902.0408v1,Yuri N. Tyurin,arxiv.org,"math.ST, stat.TH, 62H15, 62H12 (Primary) 62J05, 62J12 (Secondary)"
Deferred statistical order convergence in Riesz spaces,"Some types of statistical convergence such as statistical order and deferred statistical convergences have been studied and investigated in Riesz spaces, recently. In this paper, we introduce the concept of deferred statistical convergence in Riesz spaces with order convergence. Moreover, we give some relations between deferred statistical order convergence and other kinds of statistical convergences.",2022-09-28,http://arxiv.org/abs/2209.14381v1,"Mehmet Küçükaslan, Abdullah Aydın",arxiv.org,math.FA
Introduction,"The Statistics Consortium at the University of Maryland, College Park, hosted a two-day workshop on Bayesian Methods that Frequentists Should Know during April 30--May 1, 2008. The event was co-sponsored by the Institute of Mathematical Statistics (IMS), Office of Research and Methodology, National Center for Health Statistics, Survey Research Methods Section (SRMS) of the American Statistical Association, and Washington Statistical Society. The workshop was intended to bring out the positive features of Bayesian statistics in solving real-life problems, including complex problems in sample surveys and production of high-quality official statistics.",2011-09-29,http://arxiv.org/abs/1109.6405v1,"P. Lahiri, Eric Slud",arxiv.org,stat.ME
On the geometry of lightlike submanifolds of indefinite statistical   manifolds,"We study lightlike submanifolds of indefinite statistical manifolds. Contrary to the classical theory of submanifolds of statistical manifolds, lightlike submanifolds of indefinite statistical manifolds need not to be statistical submanifold. Therefore we obtain some conditions for a lightlike submanifold of indefinite statistical manifolds to be a lightlike statistical submanifold. We derive the expression of statistical sectional curvature and finally obtain some conditions for the induced statistical Ricci tensor on a lightlike submanifold of indefinite statistical manifolds to be symmetric.",2019-03-18,http://arxiv.org/abs/1903.07387v1,"Varun Jain, Amrinder Pal Singh, Rakesh Kumar",arxiv.org,"math.DG, 53B05, 53B30, 53C40"
Foundations of Structural Statistics: Statistical Manifolds,"Upon a consistent topological statistical theory the application of structural statistics requires a quantification of the proximity structure of model spaces. An important tool to study these structures are Pseudo-Riemannian metrices, which in the category of statistical models are induced by statistical divergences. The present article extends the notation of topological statistical models by a differential structure to statistical manifolds and introduces the differential geometric foundations to study distribution families by their differential-, Riemannian- and symplectic geometry.",2020-02-18,http://arxiv.org/abs/2002.07424v2,Patrick Michl,arxiv.org,"math.ST, cs.IT, math.IT, stat.TH, 62A01"
Comment: Causal Inference in the Medical Area,Comment on Causal Inference in the Medical Area [math.ST/0612783],2006-12-27,http://arxiv.org/abs/math/0612785v1,Edward L. Korn,arxiv.org,"math.ST, stat.TH"
"Comment on ""Support Vector Machines with Applications""",Comment on ``Support Vector Machines with Applications'' [math.ST/0612817],2006-12-28,http://arxiv.org/abs/math/0612820v1,"Olivier Bousquet, Bernhard Schölkopf",arxiv.org,"math.ST, stat.TH"
"Comment on ""Support Vector Machines with Applications""","Comment on ""Support Vector Machines with Applications"" [math.ST/0612817]",2006-12-28,http://arxiv.org/abs/math/0612822v1,Grace Wahba,arxiv.org,"math.ST, stat.TH"
"Comment on ""Support Vector Machines with Applications""",Comment on [math.ST/0612817],2006-12-28,http://arxiv.org/abs/math/0612824v1,"Trevor Hastie, Ji Zhu",arxiv.org,"math.ST, stat.TH"
Discussion: Conditional growth charts,Discussion of Conditional growth charts [math.ST/0702634],2007-02-22,http://arxiv.org/abs/math/0702636v1,"Raymond J. Carroll, David Ruppert",arxiv.org,"math.ST, stat.TH"
Discussion: Conditional growth charts,Discussion of Conditional growth charts [math.ST/0702634],2007-02-22,http://arxiv.org/abs/math/0702640v1,Anneli Pere,arxiv.org,"math.ST, stat.TH"
Discussion: Conditional growth charts,Discussion of Conditional growth charts [math.ST/0702634],2007-02-22,http://arxiv.org/abs/math/0702641v1,"Matias Salibian-Barrera, Ruben H. Zamar",arxiv.org,"math.ST, stat.TH"
Discussion: Conditional growth charts,Discussion of Conditional growth charts [math.ST/0702634],2007-02-22,http://arxiv.org/abs/math/0702642v1,Mary Lou Thompson,arxiv.org,"math.ST, stat.TH"
Rejoinder: Conditional Growth Charts,Rejoinder: Conditional Growth Charts [math.ST/0702634],2007-02-22,http://arxiv.org/abs/math/0702643v1,"Ying Wei, Xuming He",arxiv.org,"math.ST, stat.TH"
Discussion of: Statistical analysis of an archeological find,Discussion of ``Statistical analysis of an archeological find'' by Andrey Feuerverger [arXiv:0804.0079],2008-04-01,http://arxiv.org/abs/0804.0088v1,Camil Fuchs,arxiv.org,stat.AP
Rejoinder of: Statistical analysis of an archeological find,Rejoinder of ``Statistical analysis of an archeological find'' [arXiv:0804.0079],2008-04-01,http://arxiv.org/abs/0804.0103v1,Andrey Feuerverger,arxiv.org,stat.AP
Comment: Citation Statistics,"Comment on ""Citation Statistics"" [arXiv:0910.3529]",2009-10-19,http://arxiv.org/abs/0910.3543v1,"David Spiegelhalter, Harvey Goldstein",arxiv.org,"stat.ME, cs.DL, physics.soc-ph"
Comment: Citation Statistics,"Comment on ""Citation Statistics"" [arXiv:0910.3529]",2009-10-19,http://arxiv.org/abs/0910.3546v1,Peter Gavin Hall,arxiv.org,"stat.ME, cs.DL, physics.soc-ph"
Rejoinder: Citation Statistics,"Rejoinder to ""Citation Statistics"" [arXiv:0910.3529]",2009-10-19,http://arxiv.org/abs/0910.3548v1,"Robert Adler, John Ewing, Peter Taylor",arxiv.org,"stat.ME, cs.DL, physics.soc-ph"
Ternary algebras with braided statistics,"Algebraic relations that characterize quantum statistics (Bose-Einstein statistic, Fermi-Dirac statistic, supersymmetry, parastatistic, anyonic statistic, ...) are reformulated herein in terms of a new algebraic structure, which we call para-algebra.",2010-03-06,http://arxiv.org/abs/1003.1353v1,Azzouz Zinoun,arxiv.org,"math.SP, math-ph, math.MP"
Comment: The Need for Syncretism in Applied Statistics,"Comment on ""The Need for Syncretism in Applied Statistics"" [arXiv:1012.1161]",2010-12-07,http://arxiv.org/abs/1012.1414v1,Sander Greenland,arxiv.org,stat.ME
Bayesian Statistical Pragmatism,"Discussion of ""Statistical Inference: The Big Picture"" by R. E. Kass [arXiv:1106.2895]",2011-06-16,http://arxiv.org/abs/1106.3220v1,Andrew Gelman,arxiv.org,stat.ME
"Discussion of ""Statistical Inference: The Big Picture"" by R. E. Kass","Discussion of ""Statistical Inference: The Big Picture"" by R. E. Kass [arXiv:1106.2895]",2011-06-17,http://arxiv.org/abs/1106.3400v1,Robert McCulloch,arxiv.org,stat.ME
"Discussion of ""Statistical Inference: The Big Picture"" by R. E. Kass","Discussion of ""Statistical Inference: The Big Picture"" by R. E. Kass [arXiv:1106.2895]",2011-06-17,http://arxiv.org/abs/1106.3406v1,Hal Stern,arxiv.org,stat.ME
"Statistical Curse of the Second Half Rank, Eulerian numbers and Stirling   numbers",I describe the occurence of Eulerian numbers and Stirling numbers of the second kind in the combinatorics of the Statistical Curse of the Second Half Rank problem.,2013-10-14,http://arxiv.org/abs/1310.5743v1,Stephane Ouvry,arxiv.org,"math.ST, cond-mat.stat-mech, math.HO, stat.TH"
Information Geometry and Statistical Manifold,"We review basic notions in the field of information geometry such as Fisher metric on statistical manifold, $\alpha$-connection and corresponding curvature following Amari's work . We show application of information geometry to asymptotic statistical inference.",2014-10-09,http://arxiv.org/abs/1410.3369v1,Mashbat Suzuki,arxiv.org,"math.ST, math.DG, math.PR, stat.TH"
Multivariate Order Statistics: The Intermediate Case,"Asymptotic normality of intermediate order statistics taken from univariate iid random variables is well-known. We generalize this result to random vectors in arbitrary dimension, where the order statistics are taken componentwise.",2016-07-20,http://arxiv.org/abs/1607.05896v1,"Michael Falk, Florian Wisheckel",arxiv.org,"math.ST, stat.TH, 62G30 (Primary), 62H10 (Secondary)"
Comment: Classifier Technology and the Illusion of Progress,Comment on Classifier Technology and the Illusion of Progress [math.ST/0606441],2006-06-19,http://arxiv.org/abs/math/0606447v1,Jerome H. Friedman,arxiv.org,"math.ST, stat.TH"
Comment: Classifier Technology and the Illusion of Progress,Comment on Classifier Technology and the Illusion of Progress [math.ST/0606441],2006-06-19,http://arxiv.org/abs/math/0606457v1,Robert A. Stine,arxiv.org,"math.ST, stat.TH"
Rejoinder: Classifier Technology and the Illusion of Progress,Rejoinder: Classifier Technology and the Illusion of Progress [math.ST/0606441],2006-06-19,http://arxiv.org/abs/math/0606461v1,David J. Hand,arxiv.org,"math.ST, stat.TH"
Anti-Invariant Holomorphic Statistical Submersions,"Our purpose in this article is to study anti-invariant statistical submersions from holomorphic statistical manifolds. Firstly we introduce holomorphic statistical submersions satisfying the certain condition, after we give anti-invariant statistical submersions satisfying the certain condition. And we supported our results with examples.",2022-09-08,http://arxiv.org/abs/2209.03627v1,"Sema Kazan, Kazuhiko Takano",arxiv.org,math.DG
On the Powers of Some New Chi-Square Type Statistics,"In this paper, four new Chi-Square type statistics are presented for testing the hypothesis of a uniform null versus specified trend alternatives. The powers of these test statistics are compared with the powers of the statistics considered by Steele and Chaseling [8]. The four test statistics are shown to have superior or equivalent powers to the powers of the test statistics considered by the authors for certain trend alternatives and for certain conditions placed on the cell count.",2010-07-29,http://arxiv.org/abs/1007.5107v1,Clement Ampadu,arxiv.org,"math.ST, stat.TH, Primary 62G30, Secondary 62G10"
Statistical quasi Cauchyness in two normed spaces,"A function $f$ defined on a subset $E$ of a two normed space $X$ is statistically ward continuous if it preserves statistically quasi-Cauchy sequences of points in $E$ where a sequence $(x_n)$ is statistically quasi-Cauchy if $(\Delta x_{n})$ is a statistically null sequence. A subset $E$ of $X$ is statistically ward compact if any sequence of points in $E$ has a statistically quasi-Cauchy subsequence. In this paper, new kinds of continuities are investigated in two normed spaces. It turns out that uniform limit of statistically ward continuous functions is again statistically ward continuous.",2014-02-13,http://arxiv.org/abs/1402.3291v1,"Huseyin Cakalli, Sibel Ersan",arxiv.org,"math.FA, 40A05, 40A35, 26A15, 40A30"
Foundations of Structural Statistics: Topological Statistical Theory,"Topological statistical theory provides the foundation for a modern mathematical reformulation of classical statistical theory: Structural Statistics emphasizes the structural assumptions that accompany distribution families and the set of structure preserving transformations between them, given by their statistical morphisms. The resulting language is designed to integrate complicated structured model spaces like deep-learning models and to close the gap to topology and differential geometry. To preserve the compatibility to classical statistics the language comprises corresponding concepts for standard information criteria like sufficiency and completeness.",2019-12-21,http://arxiv.org/abs/1912.10266v3,Patrick Michl,arxiv.org,"math.ST, cs.LG, stat.TH, 62A01"
Stratified Permutational Berry--Esseen Bounds and Their Applications to   Statistics,"The stratified linear permutation statistic arises in various statistics problems, including stratified and post-stratified survey sampling, stratified and post-stratified experiments, conditional permutation tests, etc. Although we can derive the Berry--Esseen bounds for the stratified linear permutation statistic based on existing bounds for the non-stratified statistics, those bounds are not sharp, and moreover, this strategy does not work in general settings with heterogeneous strata with varying sizes. We first use Stein's method to obtain a unified stratified permutational Berry--Esseen bound that can accommodate heterogeneous strata. We then apply the bound to various statistics problems, leading to stronger theoretical quantifications and thereby facilitating statistical inference in those problems.",2025-03-18,http://arxiv.org/abs/2503.13986v1,"Pengfei Tian, Fan Yang, Peng Ding",arxiv.org,"math.ST, stat.TH, 62E17, 60F05 (Primary) 62K10 (Secondary)"
Statistical duality of the Laplace distribution,The statistical duality of distributions is a powerful tool for statistical inferences. In the paper the statistical duality of Laplace distribution is discussed. As shown the confidence density of the parameter of this distribution is uniquely determined.,2005-07-21,http://arxiv.org/abs/math/0507452v1,"E. A. Barkova, S. I. Bityukov, V. A. Taperechkina",arxiv.org,"math.ST, math.PR, stat.TH"
Correction. Efficient parameter estimation for self-similar processes,Correction to The Annals of Statistics (1989) 17 1749--1766 [URL: http://links.jstor.org/sici?sici=0090-5364%28198912%2917%3A4%3C1749%3AEPEFSP%3E 2.0.CO%3B2-9],2006-07-04,http://arxiv.org/abs/math/0607078v1,Rainer Dahlhaus,arxiv.org,"math.ST, stat.TH"
"Discussion: A tale of three cousins: Lasso, L2Boosting and Dantzig",Discussion of ``The Dantzig selector: Statistical estimation when $p$ is much larger than $n$'' by Emmanuel Candes and Terence Tao [math/0506081],2008-03-21,http://arxiv.org/abs/0803.3134v1,"N. Meinshausen, G. Rocha, B. Yu",arxiv.org,"math.ST, stat.TH"
Correction. Strong invariance principles for sequential Bahadur--Kiefer   and Vervaat error processes of long-range dependent sequences,"Correction to The Annals of Statistics (2006) 34, 1013--1044 [URL: http://projecteuclid.org/euclid.aos/1151418250]",2008-03-28,http://arxiv.org/abs/0803.4118v1,"Miklós Csörgõ, Barbara Szyszkowicz, Lihong Wang",arxiv.org,"math.ST, stat.TH"
Some Probabilistic and Statistical Properties of a Random Coefficient   Autoregressive Model,A statistical inference for random coefficient first-order autoregressive model $[RCAR(1)]$ was investigated by P.M. ROBINSON (1978) in which the coefficients varying over individuals. In this paper we attempt to generalize this result to random coefficient autoregressive model of order $p$ $[RCAR(p)]$. The stationarity condition will derived for this model.,2008-11-12,http://arxiv.org/abs/0811.1846v1,"A. Bouchemella, A. Bibi",arxiv.org,"math.ST, stat.TH"
"On some problems in the article ""Efficient Likelihood Estimation in   State Space Models"" by Cheng-Der Fuh","On some problems in the article ""Efficient Likelihood Estimation in State Space Models"" by Cheng-Der Fuh [Ann. Statist. 34 (2006) 2026--2068] [arXiv:math/0611376]",2010-02-26,http://arxiv.org/abs/1002.4959v1,Jens Ledet Jensen,arxiv.org,"math.ST, stat.TH"
Generalized statistical mechanics for superstatistical systems,"Mesoscopic systems in a slowly fluctuating environment are often well described by superstatistical models. We develop a generalized statistical mechanics formalism for superstatistical systems, by mapping the superstatistical complex system onto a system of ordinary statistical mechanics with modified energy levels. We also briefly review recent examples of applications of the superstatistics concept for three very different subject areas, namely train delay statistics, turbulent tracer dynamics, and cancer survival statistics.",2010-07-06,http://arxiv.org/abs/1007.0903v1,Christian Beck,arxiv.org,cond-mat.stat-mech
Remembering Erich Lehmann,"In this paper I shall try to sketch some typical aspects of Erich Lehmann's contributions to statistics through his research, his teaching, his service to the profession and his personality.",2012-02-23,http://arxiv.org/abs/1202.5098v1,Willem R. van Zwet,arxiv.org,"math.HO, math.ST, stat.TH"
Correction on Moments of minors of Wishart matrices,"Correction on Moments of minors of Wishart matrices by M. Drton and A. Goia (Ann. Statist. 36 (2008) 2261-2283), arXiv:math/0604488",2012-08-16,http://arxiv.org/abs/1208.3360v1,"Mathias Drton, Aldo Goia",arxiv.org,"math.ST, stat.TH"
Asymptotic Formula for a General Double-Bounded Custom-Sided Likelihood   Based Test Statistic,"This paper presents the asymptotic distributions of a general likelihood-based test statistic, derived using results of Wilks and Wald. The general form of the test statistic incorporates the test statistics and associated asymptotic formulae previously derived by Cowan, Cranmer, Gross and Vitells, which are seen to be special cases of the likelihood-based test statistic described here.",2013-02-12,http://arxiv.org/abs/1302.2799v1,Will Buttinger,arxiv.org,"physics.data-an, stat.ME"
Continuous-time statistics and generalized relaxation equations,"Using two simple examples, the continuous-time random walk as well as a two state Markov chain, the relation between generalized anomalous relaxation equations and semi-Markov processes is illustrated. This relation is then used to discuss continuous-time random statistics in a general setting, for statistics of convolution-type. Two examples are presented in some detail: the sum statistic and the maximum statistic.",2017-07-27,http://arxiv.org/abs/1707.08927v1,Enrico Scalas,arxiv.org,"math.PR, cond-mat.stat-mech, math-ph, math.MP"
Width-$k$ Generalizations of Classical Permutation Statistics,"We introduce new natural generalizations of the classical descent and inversion statistics for permutations, called width-$k$ descents and width-$k$ inversions. These variations induce generalizations of the excedance and major statistics, providing a framework in which the most well-known equidistributivity results for classical statistics are paralleled. We explore additional relationships among the statistics providing specific formulas in certain special cases. Moreover, we explore the behavior of these width-$k$ statistics in the context of pattern avoidance.",2017-01-17,http://arxiv.org/abs/1701.04788v1,Robert Davis,arxiv.org,"math.CO, 05"
Statistical Inference with Data Augmentation and Parameter Expansion,"Statistical pragmatism embraces all efficient methods in statistical inference. Augmentation of the collected data is used herein to obtain representative population information from a large class of non-representative population's units. Parameter expansion of a probability model is shown to reduce the upper bound on the sum of error probabilities for a test of simple hypotheses, and a measure, R, is proposed for the effect of activating additional component(s) in the sufficient statistic.",2015-12-02,http://arxiv.org/abs/1512.00847v1,Yannis G. Yatracos,arxiv.org,"math.ST, stat.TH"
A Bayesian Redesign of the First Probability/Statistics Course,"The traditional calculus-based introduction to statistical inference consists of a semester of probability followed by a semester of frequentist inference. Cobb (2015) challenges the statistical education community to rethink the undergraduate statistics curriculum. In particular, he suggests that we should focus on two goals: making fundamental concepts accessible and minimizing prerequisites to research. Using five underlying principles of Cobb, we describe a new calculus-based introduction to statistics based on simulation-based Bayesian computation.",2020-07-08,http://arxiv.org/abs/2007.04180v1,Jim Albert,arxiv.org,stat.OT
Statistically unbounded p-convergence in lattice-normed Riesz spaces,"The statistically unbounded $p$-convergence is an abstraction of the statistical order, unbounded order, and $p$-convergences. We investigate the concept of the statistically unbounded convergence on lattice-normed Riesz spaces with respect to statistical p-decreasing sequences. Also, we get some relations between this concept and the other kinds of statistical convergences on Riesz spaces.",2022-04-26,http://arxiv.org/abs/2204.12321v2,Abdullah Aydın,arxiv.org,"math.FA, 46A40, 46E30, 40A05, 46B42"
On the Tightness of the Laplace Approximation for Statistical Inference,Laplace's method is used to approximate intractable integrals in a statistical problems. The relative error rate of the approximation is not worse than $O_p(n^{-1})$. We provide the first statistical lower bounds showing that the $n^{-1}$ rate is tight.,2022-10-17,http://arxiv.org/abs/2210.09442v2,"Blair Bilodeau, Yanbo Tang, Alex Stringer",arxiv.org,"math.ST, stat.TH"
Higher criticism: $p$-values and criticism,"This paper compares the higher criticism statistic (Donoho and Jin [Ann. Statist. 32 (2004) 962-994]), a modification of the higher criticism statistic also suggested by Donoho and Jin, and two statistics of the Berk-Jones [Z. Wahrsch. Verw. Gebiete 47 (1979) 47-59] type. New approximations to the significance levels of the statistics are derived, and their accuracy is studied by simulations. By numerical examples it is shown that over a broad range of sample sizes the Berk-Jones statistics have a better power function than the higher criticism statistics to detect sparse mixtures. The applications suggested by Meinshausen and Rice [Ann. Statist. 34 (2006) 373-393], to find lower confidence bounds for the number of false hypotheses, and by Jeng, Cai and Li [Biometrika 100 (2013) 157-172], to detect copy number variants, are also studied.",2014-11-05,http://arxiv.org/abs/1411.1437v3,"Jian Li, David Siegmund",arxiv.org,"math.ST, stat.ME, stat.TH"
A New Method for Derivation of Statistical Weight of the Gentile   Statistics,"We present a new method for obtaining the statistical weight of the Gentile Statistics. In a recent paper, Perez and Tun presented an ap- proximate combinatoric and an exact recursive formula for the statistical weight of Gentile Statistics, beginning from bosonic and fermionic cases, respectively [1]. In this paper, we obtain two exact, one combinatoric and one recursive, formulae for the statistical weight of Gentile Statistics, by an another approach. The combinatoric formula is valid only for special cases, whereas recursive formula is valid for all possible cases. Moreover, for a given q-maximum number of particles that can occupy a level for Gentile statistics-the recursive formula we have derived gives the result much faster than the recursive formula presented in [1], when one uses a computer program. Moreover we obtained the statistical weight for the distribution proposed by Dai and Xie in Ref. [2]. Keywords: Fractional statistics, Gentile distribution, Statistical Weight",2015-11-25,http://arxiv.org/abs/1511.08051v1,"Sevilay Selvi, Haydar Uncu",arxiv.org,cond-mat.stat-mech
Study of invariance of nonextensive statistics under the uniform energy   spectrum translation,"The general formalisms of the $q$-dual statistics, the Boltzmann-Gibbs statistics, and three versions of the Tsallis statistics known as Tsallis-1, Tsallis-2, and Tsallis-3 statistics have been considered in the canonical ensemble. We have rigorously proved that the probability distribution of the Tsallis-1 statistics is invariant under the uniform energy spectrum translation at a fixed temperature. This invariance demonstrates that the formalism of the Tsallis-1 statistics is consistent with the fundamentals of the equilibrium statistical mechanics. The same results we have obtained for the probability distributions of the Tsallis-3 statistics, Boltzmann-Gibbs statistics, and $q$-dual statistics. However, we have found that the probability distribution of the Tsallis-2 statistics, the expectation values of which are not consistent with the normalization condition of probabilities, is indeed not invariant under the overall shift in energy as expected.",2021-08-11,http://arxiv.org/abs/2108.05702v2,A. S. Parvan,arxiv.org,"cond-mat.stat-mech, hep-ph, nucl-th"
Statistical properties and statistical interaction for particles with   spin: Hubbard model in one dimension and statistical spin liquid,"We derive the statistical distribution functions for the Hubbard chain with infinite Coulomb repulsion among particles and for the statistical spin liquid with an arbitrary magnitude of the local interaction in momentum space. Haldane's statistical interaction is derived from an exact solution for each of the two models. In the case of the Hubbard chain the charge (holon) and the spin (spinon) excitations decouple completely and are shown to behave statistically as fermions and bosons, respectively. In both cases the statistical interaction must contain several components, a rule for the particles with the internal symmetry.",1994-06-23,http://arxiv.org/abs/cond-mat/9406095v2,"Krzysztof Byczuk, Jozef Spalek",arxiv.org,cond-mat
Normal approximation for nonlinear statistics using a concentration   inequality approach,"Let $T$ be a general sampling statistic that can be written as a linear statistic plus an error term. Uniform and non-uniform Berry--Esseen type bounds for $T$ are obtained. The bounds are the best possible for many known statistics. Applications to U-statistics, multisample U-statistics, L-statistics, random sums and functions of nonlinear statistics are discussed.",2007-08-31,http://arxiv.org/abs/0708.4272v1,"Louis H. Y. Chen, Qi-Man Shao",arxiv.org,"math.ST, stat.TH"
The emergence of French statistics. How mathematics entered the world of   statistics in France during the 1920s,"This paper concerns the emergence of modern mathematical statistics in France after the First World War. Emile Borel's achievements are presented, and especially his creation of two institutions where mathematical statistics was developed: the {\it Statistical Institute of Paris University}, (ISUP) in 1922 and above all the {\it Henri Poincar\'e Institute} (IHP) in 1928. At the IHP, a new journal {\it Annales de l'Institut Henri Poincar\'e} was created in 1931. We discuss the first papers in that journal dealing with mathematical statistics.",2009-06-23,http://arxiv.org/abs/0906.4205v2,"Rémi Catellier, Laurent Mazliak",arxiv.org,"math.HO, math.ST, stat.TH"
Stochastic simulations for the time evolution of systems which obey   generalized statistics: Fractional exclusion statistics and Gentile's   statistics,"We present a stochastic method for the simulation of the time evolution in systems which obey generalized statistics, namely fractional exclusion statistics and Gentile's statistics. The transition rates are derived in the framework of canonical ensembles. This approach introduces a tool for describing interacting fermionic and bosonic systems in non-equilibrium as ideal FES systems, in a computationally efficient manner. The two types of statistics are analyzed comparatively, indicating their intrinsic thermodynamic differences and revealing key aspects related to the species size.",2010-07-26,http://arxiv.org/abs/1007.4491v1,"George Alexandru Nemnes, Dragos-Victor Anghel",arxiv.org,"cond-mat.stat-mech, cond-mat.quant-gas, physics.comp-ph"
Nonequilibrium statistical operator method in the Renyi statistics,The generalization of the Zubarev nonequilibrium statistical operator method for the case of Renyi statistics is proposed when the relevant statistical operator (or distribution function) is obtained based on the principle of maximum for the Renyi entropy. The nonequilibrium statistical operator and corresponding generalized transport equations for the reduced-description parameters are obtained. A consistent description of kinetic and hydrodynamic processes in the system of interacting particles is considered as an example.,2010-08-29,http://arxiv.org/abs/1008.4907v1,"B. Markiv, R. Tokarchuk, P. Kostrobij, M. Tokarchuk",arxiv.org,cond-mat.stat-mech
Distribution of a Non-parametric Wavelet-based Statistic for Functional   Data,"Mathematical formulations and proofs for a wavelet based statistic employed in functional data analysis is elaborately discussed in this report. The propositions and derivations discussed here apply to a wavelet based statistic with hard thresholding. The proposed analytic distribution is made feasible only due to the assumption of normality. Since the statistic is developed for applications in high dimensional data analysis, the assumption holds true in most practical situations. In the future, the work here could be extended to address data that are non-Gaussian. Aside from establishing a rigorous mathematical foundation for the distribution of the statistic, the report also explores a few approximations for the proposed statistic.",2014-08-11,http://arxiv.org/abs/1408.2581v1,"Senthil B. Girimurugan, Eric Chicken",arxiv.org,"math.ST, stat.TH, 62E15, 62E17, 62E20, 62G08"
The standard map: From Boltzmann-Gibbs statistics to Tsallis statistics,"As well known, Boltzmann-Gibbs statistics is the correct way of thermostatistically approaching ergodic systems. On the other hand, nontrivial ergodicity breakdown and strong correlations typically drag the system into out-of-equilibrium states where Boltzmann-Gibbs statistics fails. For a wide class of such systems, it has been shown in recent years that the correct approach is to use Tsallis statistics instead. Here we show how the dynamics of the paradigmatic conservative (area-preserving) standard map exhibits, in an exceptionally clear manner, the crossing from one statistics to the other. Our results unambiguously illustrate the domains of validity of both Boltzmann-Gibbs and Tsallis statistics.",2015-01-11,http://arxiv.org/abs/1501.02459v1,"Ugur Tirnakli, Ernesto P. Borges",arxiv.org,cond-mat.stat-mech
Immersions into Statistical Manifolds,This paper studies the geometry of immersions into statistical manifolds. A necessary and sufficient condition is obtained for statistical manifold structures to be dual to each other for a non-degenerate equiaffine immersion. Then we obtain conditions for realizing an n-dimensional statistical manifold in an (n+1)-dimensional statistical manifold and its converse. Centro-affine immersion of codimension two into a dually flat statistical manifold is defined. Also we have shown that statistical manifold realized in a dually flat statistical manifold of codimension two is conformally-projectively flat.,2018-03-07,http://arxiv.org/abs/1803.02538v3,"T V Mahesh, K S Subrahamanian Moosath",arxiv.org,"math.DG, 53A15, 53C42"
On Model Selection with Summary Statistics,"Recently, many authors have cast doubts on the validity of ABC model choice. It has been shown that the use of sufficient statistic in ABC model selection leads, apart from few exceptional cases in which the sufficient statistic is also cross-model sufficient, to unreliable results. In a single model context and given a sufficient summary statistic, we show that it is possible to fully recover the posterior normalising constant, without using the likelihood function. The idea can be applied, in an approximate way, to more realistic scenarios in which the sufficient statistic is not unavailable but a ""good"" summary statistic for estimation is available.",2018-03-28,http://arxiv.org/abs/1803.10749v2,Erlis Ruli,arxiv.org,"stat.CO, math.ST, stat.TH"
A Goodness-of-Fit Test for Statistical Models,"Statistical modeling plays a fundamental role in understanding the underlying mechanism of massive data (statistical inference) and predicting the future (statistical prediction). Although all models are wrong, researchers try their best to make some of them be useful. The question here is how can we measure the usefulness of a statistical model for the data in hand? This is key to statistical prediction. The important statistical problem of testing whether the observations follow the proposed statistical model has only attracted relatively few attentions. In this paper, we proposed a new framework for this problem through building its connection with two-sample distribution comparison. The proposed method can be applied to evaluate a wide range of models. Examples are given to show the performance of the proposed method.",2020-06-16,http://arxiv.org/abs/2006.08864v1,Hangjin Jiang,arxiv.org,"stat.ME, stat.AP"
Intermediate symmetric construction of transformation between anyon and   Gentile statistics,"Gentile statistics describes fractional statistical systems in the occupation number representation. Anyon statistics researches those systems in the winding number representation. Both of them are intermediate statistics between Bose-Einstein and Fermi-Dirac statistics. The second quantization of Gentile statistics shows a lot of advantages. According to the symmetry requirement of the wave function, we give the general construction of transformation between anyon and Gentile statistics. In other words, we introduce the second quantization form of anyons in a easy way. Basic relations of second quantization operators, the coherent state and Berry phase are also discussed.",2020-03-10,http://arxiv.org/abs/2003.06235v1,Yao Shen,arxiv.org,quant-ph
Robust test statistics for the two-way MANOVA based on the minimum   covariance determinant estimator,"Robust test statistics for the two-way MANOVA based on the minimum covariance determinant (MCD) estimator are proposed as alternatives to the classical Wilks' Lambda test statistics which are well known to be very sensitive to outliers as they are based on classical normal theory estimates of generalized variances. The classical Wilks' Lambda statistics are robustified by replacing the classical estimates by highly robust and efficient reweighted MCD estimates. Further, Monte Carlo simulations are used to evaluate the performance of the new test statistics under various designs by investigating their finite sample accuracy, power, and robustness against outliers. Finally, these robust test statistics are applied to a real data example.",2018-06-11,http://arxiv.org/abs/1806.04106v1,Bernhard Spangl,arxiv.org,"math.ST, stat.TH"
Direct statistical simulation of the Lorenz63 system,"We use direct statistical simulation (DSS) to find the low-order statistics of the well-known dynamical system, the Lorenz63 model. Instead of accumulating statistics from numerical simulation of the dynamical systems, we solve the equations of motion for the statistics themselves after closing them by making several different choices for the truncation. Fixed points of the statistics are obtained either by time evolving, or by iterative methods. Statistics so obtained are compared to those found by the traditional approach.",2021-08-28,http://arxiv.org/abs/2109.02482v2,"Kuan Li, J. B. Marston, Saloni Saxena, Steven M. Tobias",arxiv.org,"cond-mat.stat-mech, nlin.CD, physics.flu-dyn"
Statistical reform and the replication crisis,"The replication crisis has prompted many to call for statistical reform within the psychological sciences. Here we examine issues within Frequentist statistics that may have led to the replication crisis, and we examine the alternative---Bayesian statistics---that many have suggested as a replacement. The Frequentist approach and the Bayesian approach offer radically different perspectives on evidence and inference with the Frequentist approach prioritising error control and the Bayesian approach offering a formal method for quantifying the relative strength of evidence for hypotheses. We suggest that rather than mere statistical reform, what is needed is a better understanding of the different modes of statistical inference and a better understanding of how statistical inference relates to scientific inference.",2018-11-05,http://arxiv.org/abs/1811.01821v1,"Lincoln J Colling, Denes Szucs",arxiv.org,"stat.ME, stat.OT"
On Some Mathematics Related to the Interpolating Statistics,"Motivated by fractional quantum Hall effects, we introduce a universal space of statistics interpolating Bose-Einstein statistics and Fermi-Dirac statistics. We connect the interpolating statistics to umbral calculus and use it as a bridge to study the interpolation statistics by the principle maximum entropy by deformed entropy functions. On the one hand this connection makes it possible to relate fractional quantum Hall effects to many different mathematical objects, including formal group laws, complex bordism theory, complex genera, operads, counting trees, spectral curves in Eynard-Orantin topological recursions, etc. On the other hand, this also suggests to reexamine umbral calculus from the point of view of quantum mechanics and statistical mechanics.",2021-08-24,http://arxiv.org/abs/2108.10514v1,Jian Zhou,arxiv.org,"math-ph, cond-mat.stat-mech, math.MP"
Six Statistical Senses,"This article proposes a set of categories, each one representing a particular distillation of important statistical ideas. Each category is labeled a ""sense"" because we think of these as essential in helping every statistical mind connect in constructive and insightful ways with statistical theory, methodologies, and computation, toward the ultimate goal of building statistical phronesis. The illustration of each sense with statistical principles and methods provides a sensical tour of the conceptual landscape of statistics, as a leading discipline in the data science ecosystem.",2022-04-11,http://arxiv.org/abs/2204.05313v2,"Radu V. Craiu, Ruobin Gong, Xiao-Li Meng",arxiv.org,"stat.OT, 62A01, 62-02"
Statistical $p$-convergence in lattice-normed Riesz spaces,"A sequence $(x_n)$ in a lattice-normed space $(X,p,E)$ is statistical $p$-convergent to $x\in X$ if there exists a statistical $p$-decreasing sequence $q\stpd 0$ with an index set $K$ such that $\delta(K)=1$ and $p(x_{n_k}-x)\leq q_{n_k}$ for every $n_k\in K$. This convergence has been investigated recently for $(X,p,E)=(E,|\cdot|,E)$ under the name of statistical order convergence and under the name of statistical multiplicative order convergence, and also, for taking $E$ as a locally solid Riesz space under the names statistically unbounded $\tau$-convergence and statistically multiplicative convergence. In this paper, we study the general properties of statistical $p$-convergence.",2022-04-22,http://arxiv.org/abs/2204.10499v1,"Abdullah Aydın, Reha Yapalı, Erdal Korkmaz",arxiv.org,math.FA
Statistical discrimination and statistical informativeness,"We study the link between Phelps-Aigner-Cain-type statistical discrimination and familiar notions of statistical informativeness. Our central insight is that Blackwell's Theorem, suitably relabeled, characterizes statistical discrimination in terms of statistical informativeness. This delivers one-half of Chambers and Echenique's (2021) characterization of statistical discrimination as a corollary, and suggests a different interpretation of it: that discrimination is inevitable. In addition, Blackwell's Theorem delivers a number of finer-grained insights into the nature of statistical discrimination. We argue that the discrimination-informativeness link is quite general, illustrating with an informativeness characterization of a different type of discrimination.",2022-05-14,http://arxiv.org/abs/2205.07128v2,"Matteo Escudé, Paula Onuchic, Ludvig Sinander, Quitzé Valenzuela-Stookey",arxiv.org,econ.TH
Generalized Rényi statistics,"In R\'enyi's representation for exponential order statistics, we replace the iid exponential sequence with any iid sequence, and call the resulting order statistic generalized R\'enyi statistic. We prove that by randomly reordering the variables in the generalized R\'enyi statistic, we obtain in the limit a sequence of iid exponentials. This result allows us to propose a new model for heavy-tailed data. Although the new model is very close to the classical iid framework, we establish that the Hill estimator is weakly consistent and asymptotically normal without any further assumptions on the underlying distribution or on the number of upper order statistics used in the estimator.",2024-04-04,http://arxiv.org/abs/2404.03548v2,"Péter Kevei, László Viharos",arxiv.org,"math.ST, math.PR, stat.TH, 60F05, 62G32"
Generalized Fractional Statistics,"We link, by means of a semiclassical approach, the fractional statistics of particles obeying the Haldane exclusion principle to the Tsallis statistics and derive a generalized quantum entropy and its associated statistics.",1996-03-19,http://arxiv.org/abs/hep-th/9603132v1,"G. Kaniadakis, A. Lavagno, P. Quarati",arxiv.org,hep-th
Goodness-of-fit tests via phi-divergences,"A unified family of goodness-of-fit tests based on $\phi$-divergences is introduced and studied. The new family of test statistics $S_n(s)$ includes both the supremum version of the Anderson--Darling statistic and the test statistic of Berk and Jones [Z. Wahrsch. Verw. Gebiete 47 (1979) 47--59] as special cases ($s=2$ and $s=1$, resp.). We also introduce integral versions of the new statistics. We show that the asymptotic null distribution theory of Berk and Jones [Z. Wahrsch. Verw. Gebiete 47 (1979) 47--59] and Wellner and Koltchinskii [High Dimensional Probability III (2003) 321--332. Birkh\""{a}user, Basel] for the Berk--Jones statistic applies to the whole family of statistics $S_n(s)$ with $s\in[-1,2]$. On the side of power behavior, we study the test statistics under fixed alternatives and give extensions of the ``Poisson boundary'' phenomena noted by Berk and Jones for their statistic. We also extend the results of Donoho and Jin [Ann. Statist. 32 (2004) 962--994] by showing that all our new tests for $s\in[-1,2]$ have the same ``optimal detection boundary'' for normal shift mixture alternatives as Tukey's ``higher-criticism'' statistic and the Berk--Jones statistic.",2006-03-10,http://arxiv.org/abs/math/0603238v2,"Leah Jager, Jon A. Wellner",arxiv.org,"math.ST, stat.TH, 62G10, 62G20 (Primary) 62G30 (Secondary)"
Notes on statistical separation of classes of events,This short note contains some definitions and formulas about the power of an observable in statistically separating different classes of events.,2006-11-22,http://arxiv.org/abs/physics/0611219v1,Giovanni Punzi,arxiv.org,"physics.data-an, hep-ex"
Theoretical construction of 1D anyon models,One-dimensional anyon models are renewedly constructed by using path integral formalism. A statistical interaction term is introduced to realize the anyonic exchange statistics. The quantum mechanics formulation of statistical transmutation is presented.,2007-12-08,http://arxiv.org/abs/0712.1264v1,"Ren-Gui Zhu, An-Min Wang",arxiv.org,cond-mat.stat-mech
On the trasductive arguments in statistics,"The paper argues that a part of the current statistical discussion is not based on the standard firm foundations of the field. Among the examples we consider are prediction into the future, semi-supervised classification, and causality inference based on observational data.",2010-03-07,http://arxiv.org/abs/1003.1513v1,Ya'acov Ritov,arxiv.org,"math.ST, stat.TH"
Causality and Statistical Learning,"We review some approaches and philosophies of causal inference coming from sociology, economics, computer science, cognitive science, and statistics",2010-03-12,http://arxiv.org/abs/1003.2619v1,Andrew Gelman,arxiv.org,"math.ST, stat.ME, stat.TH"
Efron's curvature of the structural gradient model,"The structural gradient model is a multivariate statistical model in order to extract various interactions of given data set. In this note, we show that Efron's statistical curvature of the structural gradient model is less than that of a competitive mixture model under a null hypothesis.",2010-09-22,http://arxiv.org/abs/1009.4281v1,Tomonari SEI,arxiv.org,"math.ST, stat.TH"
Bayesian Statistics Then and Now,"Discussion of ""The Future of Indirect Evidence"" by Bradley Efron [arXiv:1012.1161]",2010-12-07,http://arxiv.org/abs/1012.1423v1,Andrew Gelman,arxiv.org,stat.ME
Comment: Quantifying Information Loss in Survival Studies,"Comment on ""Quantifying the Fraction of Missing Information for Hypothesis Testing in Statistical and Genetic Studies"" [arXiv:1102.2774]",2011-02-15,http://arxiv.org/abs/1102.2982v1,Hani Doss,arxiv.org,stat.ME
Rejoinder,"Rejoinder of ""Statistical Inference: The Big Picture"" by R. E. Kass [arXiv:1106.2895]",2011-06-17,http://arxiv.org/abs/1106.3410v1,Robert E. Kass,arxiv.org,stat.ME
Rejoinder,"Rejoinder of ""Calibrated Bayes, for Statistics in General, and Missing Data in Particular"" by R. Little [arXiv:1108.1917]",2011-08-17,http://arxiv.org/abs/1108.3466v1,Roderick Little,arxiv.org,stat.ME
Nonparametric Inference for Max-Stable Dependence,"Discussion of ""Statistical Modeling of Spatial Extremes"" by A. C. Davison, S. A. Padoan and M. Ribatet [arXiv:1208.3378].",2012-08-17,http://arxiv.org/abs/1208.3571v1,Johan Segers,arxiv.org,stat.ME
CLT for linear spectral statistics of random matrix $S^{-1}T$,This paper proposes a CLT for linear spectral statistics of random matrix $S^{-1}T$ for a general non-negative definite and {\bf non-random} Hermitian matrix $T$.,2013-05-07,http://arxiv.org/abs/1305.1376v1,"Shurong Zheng, Zhidong Bai, Jianfeng Yao",arxiv.org,"math.ST, stat.TH"
Formula to evaluate a limit related to AR(k) model of Statistics,"Computing moments of various parameter estimators related to an autoregressive model of Statistics, one needs to evaluate several expressions of the type mentioned in the title of this article. We proceed to derive the corresponding formulas.",2015-06-09,http://arxiv.org/abs/1506.03131v1,"Yuhao Liu, Jan Vrbik",arxiv.org,"math.ST, stat.TH"
"Fibonacci Numbers, Statistical Convergence and Applications","The purpose of this paper is twofold. First, the definition of new statistical convergence with Fibonacci sequence is given and some fundamental properties of statistical convergence are examined. Second, approximation theory worked as a application of the statistical convergence.",2016-07-08,http://arxiv.org/abs/1607.02307v1,"Murat Kirisci, Ali Karaisa",arxiv.org,"math.FA, 11B39, 41A10, 41A25, 41A36, 40A30, 40G15"
A technical note on divergence of the Wald statistic,"The Wald test statistic has been shown to diverge (Dufour et al, 2013, 2017) under some conditions. This note links the divergence to eigenvalues of a polynomial matrix and establishes the divergence rate.",2019-06-13,http://arxiv.org/abs/1906.05951v1,"Jean-Marie Dufour, Eric Renault, Victoria Zinde-Walsh",arxiv.org,"math.ST, stat.TH, 62F03"
Comparisons of Order Statistics from Some Heterogeneous Discrete   Distributions,"In this paper, we compare extreme order statistics through vector majorization arising from heterogeneous Poisson and geometric random variables. These comparisons are carried out with respect to usual stochastic ordering.",2021-03-01,http://arxiv.org/abs/2103.00763v1,"Shovan Chowdhury, Amarjit Kundu, Surja Kanta Mishra",arxiv.org,"math.ST, stat.TH"
Weyl-Mahonian Statistics for Weighted Flags of Type A-D,"We relate properties of weighted flags (or multiflags) of type AD to statistics of the corresponding Weyl groups. For type A, we recover the Mahonian statistics on symmetric groups. Finally, we sketch briefly an easy extension incorporating statistics for so-called Euler-polynomials.",2018-11-12,http://arxiv.org/abs/1811.04701v1,Roland Bacher,arxiv.org,"math.CO, math.RT"
The Best Bounds for Range Type Statistics,"In this paper, we obtain the upper and lower bounds for two inequalities related to the range statistics. The first one is concerning the one-variable case and the second one is about the bivariate case.",2022-07-04,http://arxiv.org/abs/2207.01365v1,"Tsung-Lin Cheng, Chin-Yuan Hu",arxiv.org,"math.PR, math.ST, stat.TH"
A Statistical Approach to Broken Stick Problems,"Let a stick be broken at random at n-1 points to form n pieces. We consider three problems on forming k-gons with k out of these n pieces, and show how a statistical approach, through a linear transformation of variables, yields simple solutions that also allow fast computation.",2022-07-16,http://arxiv.org/abs/2207.07879v1,Rahul Mukerjee,arxiv.org,"math.ST, stat.TH"
Enhanced Cauchy Schwarz inequality and some of its statistical   applications,We present a general refinement of the Cauchy-Schwarz inequality over complete inner product spaces and show that it can be of interest for some statistical applications. This generalizes and simplifies previous results on the same subject.,2024-03-20,http://arxiv.org/abs/2403.13964v2,Sergio Scarlatti,arxiv.org,"math.ST, stat.TH"
Equilibrium Statistical Mechanics,An introductory review of Classical Statistical Mechanics,2005-04-29,http://arxiv.org/abs/cond-mat/0504790v1,Giovanni Gallavotti,arxiv.org,cond-mat.stat-mech
Stationary nonequilibrium statistical mechanics,A brief review on the dynamical systems approach to nonequilibrium statistical mechanics and chaotic dynamics,2005-10-02,http://arxiv.org/abs/cond-mat/0510027v1,Giovanni Gallavotti,arxiv.org,cond-mat.stat-mech
"Generalized Fock Spaces, New Forms of Quantum Statistics and their   Algebras","We formulate a theory of generalized Fock spaces which underlies the different forms of quantum statistics such as ``infinite'', Bose-Einstein and Fermi-Dirac statistics. Single-indexed systems as well as multi-indexed systems that cannot be mapped into single-indexed systems are studied. Our theory is based on a three-tiered structure consisting of Fock space, statistics and algebra. This general formalism not only unifies the various forms of statistics and algebras, but also allows us to construct many new forms of quantum statistics as well as many algebras of creation and destruction operators. Some of these are : new algebras for infinite statistics, q-statistics and its many avatars, a consistent algebra for fractional statistics, null statistics or statistics of frozen order, ``doubly-infinite'' statistics, many representations of orthostatistics, Hubbard statistics and its variations.",1996-05-29,http://arxiv.org/abs/hep-th/9605204v1,"A. K. Mishra, G. Rajasekaran",arxiv.org,"hep-th, math.QA, q-alg"
Generalized Fock Spaces and New Forms of Quantum Statistics,"The recent discoveries of new forms of quantum statistics require a close look at the under-lying Fock space structure. This exercise becomes all the more important in order to provide a general classification scheme for various forms of statistics, and establish interconnections among them whenever it is possible. We formulate a theory of generalized Fock spaces, which has a three tired structure consisting of Fock space, statistics and algebra. This general formalism unifies various forms of statistics and algebras, which were earlier considered to describe different systems. Besides, the formalism allows us to construct many new kinds of quantum statistics and the associated algebras of creation and destruction operators. Some of these are: orthostatistics, null statistics or statistics of frozen order, quantum group based statistics and its many avatars, and `doubly-infinite' statistics. The emergence of new forms of quantum statistics for particles interacting with singular potential is also highlighted.",2001-05-01,http://arxiv.org/abs/hep-th/0105003v1,"A. K. Mishra, G. Rajasekaran",arxiv.org,"hep-th, math-ph, math.MP"
"Discussion of ""Least angle regression"" by Efron et al",Discussion of ``Least angle regression'' by Efron et al. [math.ST/0406456],2004-06-23,http://arxiv.org/abs/math/0406463v1,Hemant Ishwaran,arxiv.org,"math.ST, stat.TH"
"Discussion of ""Least angle regression"" by Efron et al",Discussion of ``Least angle regression'' by Efron et al. [math.ST/0406456],2004-06-23,http://arxiv.org/abs/math/0406467v1,Keith Knight,arxiv.org,"math.ST, stat.TH"
"Discussion of ""Least angle regression"" by Efron et al",Discussion of ``Least angle regression'' by Efron et al. [math.ST/0406456],2004-06-23,http://arxiv.org/abs/math/0406473v1,Sanford Weisberg,arxiv.org,"math.ST, stat.TH"
Deriving Landauer's erasure principle from statistical mechanics,"We present a concise derivation of Landauer's erasure principle from the postulates of statistical mechanics, along with a small number of additional but uncontroversial axioms.",2005-12-13,http://arxiv.org/abs/quant-ph/0512105v1,Kurt Jacobs,arxiv.org,"quant-ph, cond-mat.stat-mech"
Higher-order asymptotic normality of approximations to the modified   signed likelihood ratio statistic for regular models,"Approximations to the modified signed likelihood ratio statistic are asymptotically standard normal with error of order $n^{-1}$, where $n$ is the sample size. Proofs of this fact generally require that the sufficient statistic of the model be written as $(\hat{\theta},a)$, where $\hat{\theta}$ is the maximum likelihood estimator of the parameter $\theta$ of the model and $a$ is an ancillary statistic. This condition is very difficult or impossible to verify for many models. However, calculation of the statistics themselves does not require this condition. The goal of this paper is to provide conditions under which these statistics are asymptotically normally distributed to order $n^{-1}$ without making any assumption about the sufficient statistic of the model.",2007-11-22,http://arxiv.org/abs/0711.3598v1,"Heping He, Thomas A. Severini",arxiv.org,"math.ST, stat.TH, 62F05 (Primary) 62F03 (Secondary)"
Fuzzy Statistical Limits,"Statistical limits are defined relaxing conditions on conventional convergence. The main idea of the statistical convergence of a sequence l is that the majority of elements from l converge and we do not care what is going on with other elements. At the same time, it is known that sequences that come from real life sources, such as measurement and computation, do not allow, in a general case, to test whether they converge or statistically converge in the strict mathematical sense. To overcome these limitations, fuzzy convergence was introduced earlier in the context of neoclassical analysis and fuzzy statistical convergence is introduced and studied in this paper. We find relations between fuzzy statistical convergence of a sequence and fuzzy statistical convergence of its subsequences (Theorem 2.1), as well as between fuzzy statistical convergence of a sequence and conventional convergence of its subsequences (Theorem 2.2). It is demonstrated what operations with fuzzy statistical limits are induced by operations on sequences (Theorem 2.3) and how fuzzy statistical limits of different sequences influence one another (Theorem 2.4). In Section 3, relations between fuzzy statistical convergence and fuzzy convergence of statistical characteristics, such as the mean (average) and standard deviation, are studied (Theorems 3.1 and 3.2).",2008-03-27,http://arxiv.org/abs/0803.4019v1,"Mark Burgin, Oktay Duman",arxiv.org,"math.CA, math.GN, 40A05"
Detection of spatial clustering with average likelihood ratio test   statistics,"Generalized likelihood ratio (GLR) test statistics are often used in the detection of spatial clustering in case-control and case-population datasets to check for a significantly large proportion of cases within some scanning window. The traditional spatial scan test statistic takes the supremum GLR value over all windows, whereas the average likelihood ratio (ALR) test statistic that we consider here takes an average of the GLR values. Numerical experiments in the literature and in this paper show that the ALR test statistic has more power compared to the spatial scan statistic. We develop in this paper accurate tail probability approximations of the ALR test statistic that allow us to by-pass computer intensive Monte Carlo procedures to estimate $p$-values. In models that adjust for covariates, these Monte Carlo evaluations require an initial fitting of parameters that can result in very biased $p$-value estimates.",2009-11-19,http://arxiv.org/abs/0911.3769v1,Hock Peng Chan,arxiv.org,"math.ST, stat.TH, 60F10, 62G10 (Primary) 60G55 (Secondary)"
It is Time to Stop Teaching Frequentism to Non-statisticians,We should cease teaching frequentist statistics to undergraduates and switch to Bayes. Doing so will reduce the amount of confusion and over-certainty rife among users of statistics.,2012-01-12,http://arxiv.org/abs/1201.2590v1,William M. Briggs,arxiv.org,stat.OT
Note on the One and Two-Sided Z Tests,The one sided Z test of elementary statistics is more powerful than the two-sided test of the same size.,2012-02-03,http://arxiv.org/abs/1202.0696v1,Trrry R. McConnell,arxiv.org,"math.ST, stat.TH"
Statistical mechanics of interfaces,Mathematical aspects of the theory of interfaces in statistical mechanics are discussed.,2012-06-19,http://arxiv.org/abs/1206.4230v1,Salvador Miracle-Sole,arxiv.org,"cond-mat.stat-mech, 82B24, 82B20, 82B05"
Basic univariate and bivariate statistics for symbolic data: a critical   review,Some proofs of the problems of the basic statistics proposed for numeric symbolic data.,2013-12-08,http://arxiv.org/abs/1312.2248v1,Antonio Irpino,arxiv.org,"stat.ME, stat.OT"
Quantum Statistical Mechanics. III. Equilibrium Probability,Given are a first principles derivation and formulation of the probabilistic concepts that underly equilibrium quantum statistical mechanics. The transition to non-equilibrium probability is traversed briefly.,2014-04-10,http://arxiv.org/abs/1404.2683v1,Phil Attard,arxiv.org,"cond-mat.stat-mech, quant-ph"
Legendre transforms of the fundamental thermodynamic relation and   statistical ensembles,We show how the Legendre transforms of the fundamental thermodynamic relation can be used to introduce different statistical ensembles.,2014-10-15,http://arxiv.org/abs/1410.4026v1,S. Stepanow,arxiv.org,cond-mat.stat-mech
What is... a Markov basis?,This short piece defines a Markov basis. The aim is to introduce the statistical concept to mathematicians.,2019-07-16,http://arxiv.org/abs/1907.07320v1,Sonja Petrović,arxiv.org,"math.ST, math.AC, stat.ME, stat.TH"
New Interpretable Statistics for Large Scale Structure Analysis and   Generation,"We introduce Wavelet Phase Harmonics (WPH) statistics: interpretable low-dimensional statistics that describe 2D non-Gaussian fields. These statistics are built from WPH moments, which were recently introduced in the data science and machine learning community. We apply WPH statistics to projected 2D matter density fields from the Quijote N-body simulations of the large-scale structure of the Universe. By computing Fisher information matrices, we find that the WPH statistics place more stringent constraints on four of five cosmological parameters when compared to statistics based on the combination of the power spectrum and bispectrum. We also use the WPH statistics with a maximum entropy model to statistically generate new 2D density fields that accurately reproduce the probability density function, the mean and standard deviation of the power spectrum, the bispectrum, and Minkowski functionals of the input density fields. Although other methods are efficient for either parameter estimates or statistical syntheses of the large-scale structure, WPH statistics are the first statistics that achieve state-of-the-art results for both tasks as well as being interpretable.",2020-06-11,http://arxiv.org/abs/2006.06298v2,"E. Allys, T. Marchand, J. -F. Cardoso, F. Villaescusa-Navarro, S. Ho, S. Mallat",arxiv.org,astro-ph.CO
Multi-Statistic Approximate Bayesian Computation with Multi-Armed   Bandits,"Approximate Bayesian computation is an established and popular method for likelihood-free inference with applications in many disciplines. The effectiveness of the method depends critically on the availability of well performing summary statistics. Summary statistic selection relies heavily on domain knowledge and carefully engineered features, and can be a laborious time consuming process. Since the method is sensitive to data dimensionality, the process of selecting summary statistics must balance the need to include informative statistics and the dimensionality of the feature vector. This paper proposes to treat the problem of dynamically selecting an appropriate summary statistic from a given pool of candidate summary statistics as a multi-armed bandit problem. This allows approximate Bayesian computation rejection sampling to dynamically focus on a distribution over well performing summary statistics as opposed to a fixed set of statistics. The proposed method is unique in that it does not require any pre-processing and is scalable to a large number of candidate statistics. This enables efficient use of a large library of possible time series summary statistics without prior feature engineering. The proposed approach is compared to state-of-the-art methods for summary statistics selection using a challenging test problem from the systems biology literature.",2018-05-22,http://arxiv.org/abs/1805.08647v1,"Prashant Singh, Andreas Hellander",arxiv.org,"stat.ML, cs.LG"
Solitonic aspects of submanifolds in Kenmotsu statistical manifolds,"The differential geometry of Kenmotsu manifold is a valuable part of contact geometry with nice applications in other fields such as theoretical physics. In fact, its statistical counterpart, that is, Kenmotsu statistical manifold also has same importance as that of Kenmotsu manifold. Theoretical physicists have also been looking into the equation of Ricci soliton and Yamabe soliton in relation with Einstein manifolds, quasi-Einstein manifolds and string theory. In this research article, first we examine the statistical solitons and Yamabe soliton on Kenmotsu statistical manifolds with some related examples. Then we investigate some statistical curvature properties of Kenmotsu statistical manifolds. Also, we study the statistical solitons on submanifolds of Kenmotsu statistical manifold with concircular vector field. Furthermore, we discuss the behavior of almost quasi-Yamabe soliton on submanifolds of Kenmotsu statistical manifolds endowed with concircular vector field and concurrent vector filed. Finally, we furnish an example of $5$-dimensional Kenmotsu statistical manifolds admitting a statistical soliton and almost quasi-Yamabe soliton as well in the support of this study.",2019-05-30,http://arxiv.org/abs/1905.13569v5,"Mohd. Danish Siddiqi, Aliya Naaz Siddiqui",arxiv.org,"math.DG, 53C15, 53C25, 53C40"
Inclusion statistics and particle condensation in 2 dimensions,"We propose a new type of quantum statistics, which we call inclusion statistics, in which particles tend to coalesce more than ordinary bosons. Inclusion statistics is defined in analogy with exclusion statistics, in which statistical exclusion is stronger than in Fermi statistics, but now extrapolating beyond Bose statistics, resulting in statistical inclusion. A consequence of inclusion statistics is that the lowest space dimension in which particles can condense in the absence of potentials is $d=2$, unlike $d=3$ for the usual Bose-Einstein condensation. This reduction in the dimension happens for any inclusion stronger than bosons, and the critical temperature increases with stronger inclusion. Possible physical realizations of inclusion statistics involving attractive interactions between bosons may be experimentally achievable.",2023-03-15,http://arxiv.org/abs/2303.08835v1,"Stéphane Ouvry, Alexios P. Polychronakos",arxiv.org,"cond-mat.stat-mech, hep-th, math-ph, math.MP"
Statistical convergence in metric-like spaces,In this paper we introduce the notions of statistical convergence and statistical Cauchyness of sequences in a metric-like space. We study some basic properties of these notions,2024-07-01,http://arxiv.org/abs/2407.07117v1,"Prasanta Malik, Saikat Das",arxiv.org,math.GN
Applied Statistics in the Era of Artificial Intelligence: A Review and   Vision,"The advent of artificial intelligence (AI) technologies has significantly changed many domains, including applied statistics. This review and vision paper explores the evolving role of applied statistics in the AI era, drawing from our experiences in engineering statistics. We begin by outlining the fundamental concepts and historical developments in applied statistics and tracing the rise of AI technologies. Subsequently, we review traditional areas of applied statistics, using examples from engineering statistics to illustrate key points. We then explore emerging areas in applied statistics, driven by recent technological advancements, highlighting examples from our recent projects. The paper discusses the symbiotic relationship between AI and applied statistics, focusing on how statistical principles can be employed to study the properties of AI models and enhance AI systems. We also examine how AI can advance applied statistics in terms of modeling and analysis. In conclusion, we reflect on the future role of statisticians. Our paper aims to shed light on the transformative impact of AI on applied statistics and inspire further exploration in this dynamic field.",2024-12-13,http://arxiv.org/abs/2412.10331v1,"Jie Min, Xinyi Song, Simin Zheng, Caleb B. King, Xinwei Deng, Yili Hong",arxiv.org,"stat.AP, cs.SI"
"Exclusion Statistics: Low-Temperature Properties, Fluctuations, Duality,   Applications","We derive some physical properties of ideal assemblies of identical particles obeying generalized exclusion statistics. We discuss fluctuations, and in this connection point out a fundamental contrast to conventional quantum statistics. We demonstrate a duality relating the distribution of particles at statistics $g$ to the distribution of holes at statistics $1/g$. We suggest applications to Mott insulators.",1994-05-06,http://arxiv.org/abs/cond-mat/9405017v2,"Frank Wilczek, Chetan Nayak",arxiv.org,cond-mat
Exclusion Statistics in a trapped two-dimensional Bose gas,"We study the statistical mechanics of a two-dimensional gas with a repulsive delta function interaction, using a mean field approximation. By a direct counting of states we establish that this model obeys exclusion statistics and is equivalent to an ideal exclusion statistics gas.",2000-09-13,http://arxiv.org/abs/cond-mat/0009190v2,"T. H. Hansson, J. M. Leinaas, S. Viefers",arxiv.org,cond-mat.stat-mech
Generalized statistical mechanics and fully developed turbulence,"The statistical properties of fully developed hydrodynamic turbulence can be successfully described using methods from nonextensive statistical mechanics. The predicted probability densities and scaling exponents precisely coincide with what is measured in various turbulence experiments. As a dynamical basis for nonextensive behaviour we consider nonlinear Langevin equations with fluctuating friction forces, where Tsallis statistics can be rigorously proved.",2001-10-03,http://arxiv.org/abs/cond-mat/0110073v1,Christian Beck,arxiv.org,"cond-mat.stat-mech, physics.flu-dyn"
Generalized statistics in one dimension,"An exposition of the different definitions and approaches to quantum statistics is given, with emphasis in one-dimensional situations. Permutation statistics, scattering statistics and exclusion statistics are analyzed. The Calogero model, matrix model and spin chain models constitute specific realizations.",1999-02-23,http://arxiv.org/abs/hep-th/9902157v1,Alexios P. Polychronakos,arxiv.org,"hep-th, cond-mat, nlin.SI, quant-ph, solv-int"
Moment Inequalities for Symmetric Statistics,"In this paper, we prove analogues of Khintchine and Rosenthal's moment inequalities for symmetric statistics (U-statistics) of arbitrary order. An example that shows significance of each term in the analogues of Rosenthal's bounds for symmetric statistics is constructed as well.",2000-05-01,http://arxiv.org/abs/math/0005004v1,"R. Ibragimov, Sh. Sharakhmetov",arxiv.org,"math.PR, math.FA, Primary 60E15, 60F25, 60G50"
Teleportation into Quantum Statistics,"The paper is a tutorial introduction to quantum information theory, developing the basic model and emphasizing the role of statistics and probability.",2004-05-29,http://arxiv.org/abs/math/0405572v3,Richard D. Gill,arxiv.org,"math.ST, quant-ph, stat.TH, 62F12, 62P35"
New approaches to Bayesian consistency,"We use martingales to study Bayesian consistency. We derive sufficient conditions for both Hellinger and Kullback-Leibler consistency, which do not rely on the use of a sieve. Alternative sufficient conditions for Hellinger consistency are also found and demonstrated on examples.",2005-03-29,http://arxiv.org/abs/math/0503672v1,Stephen Walker,arxiv.org,"math.ST, stat.TH, 62G20 (Primary)"
"Discussion of ""Breakdown and groups"" by P. L. Davies and U. Gather",Discussion of ``Breakdown and groups'' by P. L. Davies and U. Gather [math.ST/0508497],2005-08-25,http://arxiv.org/abs/math/0508499v1,"Marc G. Genton, Andre Lucas",arxiv.org,"math.ST, stat.TH"
"Discussion of ""Breakdown and groups"" by P. L. Davies and U. Gather",Discussion of ``Breakdown and groups'' by P. L. Davies and U. Gather [math.ST/0508497],2005-08-25,http://arxiv.org/abs/math/0508500v1,Frank Hampel,arxiv.org,"math.ST, stat.TH"
"Discussion of ""Breakdown and groups"" by P. L. Davies and U. Gather",Discussion of ``Breakdown and groups'' by P. L. Davies and U. Gather [math.ST/0508497],2005-08-25,http://arxiv.org/abs/math/0508501v1,Xuming He,arxiv.org,"math.ST, stat.TH"
"Discussion of ""Breakdown and groups"" by P. L. Davies and U. Gather",Discussion of ``Breakdown and groups'' by P. L. Davies and U. Gather [math.ST/0508497],2005-08-25,http://arxiv.org/abs/math/0508502v1,Hannu Oja,arxiv.org,"math.ST, stat.TH"
"Discussion of ""Breakdown and groups"" by P. L. Davies and U. Gather",Discussion of ``Breakdown and groups'' by P. L. Davies and U. Gather [math.ST/0508497],2005-08-25,http://arxiv.org/abs/math/0508503v1,Peter J. Rousseeuw,arxiv.org,"math.ST, stat.TH"
"Discussion of ""Breakdown and groups"" by P. L. Davies and U. Gather",Discussion of ``Breakdown and groups'' by P. L. Davies and U. Gather [math.ST/0508497],2005-08-25,http://arxiv.org/abs/math/0508504v1,David E. Tyler,arxiv.org,"math.ST, stat.TH"
"Rejoinder to ""Breakdown and groups"" by P. L. Davies and U. Gather",Rejoinder to ``Breakdown and groups'' by P. L. Davies and U. Gather [math.ST/0508497],2005-08-25,http://arxiv.org/abs/math/0508505v1,"P. Laurie Davies, Ursula Gather",arxiv.org,"math.ST, stat.TH"
"Discussion of ""Analysis of variance--why it is more important than ever""   by A. Gelman",Discussion of ``Analysis of variance--why it is more important than ever'' by A. Gelman [math.ST/0504499],2005-08-26,http://arxiv.org/abs/math/0508526v1,Tue Tjur,arxiv.org,"math.ST, stat.TH"
"Discussion of ""Analysis of variance--why it is more important than ever""   by A. Gelman",Discussion of ``Analysis of variance--why it is more important than ever'' by A. Gelman [math.ST/0504499],2005-08-26,http://arxiv.org/abs/math/0508527v1,Peter McCullagh,arxiv.org,"math.ST, stat.TH"
"Discussion of ""Analysis of variance--why it is more important than ever""   by A. Gelman",Discussion of ``Analysis of variance--why it is more important than ever'' by A. Gelman [math.ST/0504499],2005-08-26,http://arxiv.org/abs/math/0508528v1,"Joop Hox, Herbert Hoijtink",arxiv.org,"math.ST, stat.TH"
"Discussion of ""Analysis of variance--why it is more important than ever""   by A. Gelman",Discussion of ``Analysis of variance--why it is more important than ever'' by A. Gelman [math.ST/0504499],2005-08-26,http://arxiv.org/abs/math/0508529v1,Alan M. Zaslavsky,arxiv.org,"math.ST, stat.TH"
"Rejoinder to ""Analysis of variance--why it is more important than ever""   by A. Gelman",Rejoinder to ``Analysis of variance--why it is more important than ever'' by A. Gelman [math.ST/0504499],2005-08-26,http://arxiv.org/abs/math/0508530v1,Andrew Gelman,arxiv.org,"math.ST, stat.TH"
"Discussion of ""EQUI-energy sampler"" by Kou, Zhou and Wong","Discussion of ``EQUI-energy sampler'' by Kou, Zhou and Wong [math.ST/0507080]",2006-11-08,http://arxiv.org/abs/math/0611219v1,"Ming-Hui Chen, Sungduk Kim",arxiv.org,"math.ST, stat.TH"
"Discussion of ""EQUI-energy sampler"" by Kou, Zhou and Wong","Discussion of ``EQUI-energy sampler'' by Kou, Zhou and Wong [math.ST/0507080]",2006-11-08,http://arxiv.org/abs/math/0611222v1,"Ying Nian Wu, Song-Chun Zhu",arxiv.org,"math.ST, stat.TH"
Comment: The Place of Death in the Quality of Life,Comment on The Place of Death in the Quality of Life [math.ST/0612783],2006-12-27,http://arxiv.org/abs/math/0612786v1,Paul R. Rosenbaum,arxiv.org,"math.ST, stat.TH"
"Comment: Complex Causal Questions Require Careful Model Formulation:   Discussion of Rubin on Experiments with ""Censoring"" Due to Death",Comment on Complex Causal Questions Require Careful Model Formulation: Discussion of Rubin on Experiments with ``Censoring'' Due to Death [math.ST/0612783],2006-12-27,http://arxiv.org/abs/math/0612788v1,Stephen E. Fienberg,arxiv.org,"math.ST, stat.TH"
"Rejoinder to Causal Inference Through Potential Outcomes and Principal   Stratification: Application to Studies with ""Censoring"" Due to Death",Rejoinder on Causal Inference Through Potential Outcomes and Principal Stratification: Application to Studies with ``Censoring'' Due to Death by D. B. Rubin [math.ST/0612783],2006-12-27,http://arxiv.org/abs/math/0612789v1,Donald B. Rubin,arxiv.org,"math.ST, stat.TH"
"Comment on ""Support Vector Machines with Applications""","Comment on ""Support Vector Machines with Applications"" [math.ST/0612817]",2006-12-28,http://arxiv.org/abs/math/0612821v1,"Peter L. Bartlett, Michael I. Jordan, Jon D. McAuliffe",arxiv.org,"math.ST, stat.TH"
"Rejoinder to ""Support Vector Machines with Applications""",Rejoinder to ``Support Vector Machines with Applications'' [math.ST/0612817],2006-12-28,http://arxiv.org/abs/math/0612825v1,"Javier M. Moguerza, Alberto Muñoz",arxiv.org,"math.ST, stat.TH"
Isaac Newton as a Probabilist,"In 1693, Isaac Newton answered a query from Samuel Pepys about a problem involving dice. Newton's analysis is discussed and attention is drawn to an error he made.",2007-01-03,http://arxiv.org/abs/math/0701089v1,Stephen M. Stigler,arxiv.org,"math.ST, stat.TH"
On the small-scale statistics of Lagrangian turbulence,We provide evidence that the small-scale statistics of the acceleration of a test particle in high-Reynolds number Lagrangian turbulence is correctly described by Tsallis statistics with entropic index q=3/2. We present theoretical arguments why Tsallis statistics can naturally arise in Lagrangian turbulence and why at the smallest scales q=3/2 is relevant. A generalized Heisenberg-Yaglom formula is derived from the nonextensive model.,2001-05-18,http://arxiv.org/abs/physics/0105058v1,Christian Beck,arxiv.org,"physics.flu-dyn, cond-mat.stat-mech"
On the existence of statistics intermediate between those of Fermi-Dirac   and Bose-Einstein,Once again the possibility of the existence of particle statistics intermediate between those of Fermi-Dirac and Bose-Einstein surfaces. Here attention is drawn to the fact that some fifteen years ago it was shown that such so-called 'intermediate' statistics correspond to no physical process and the stationary probability distributions of intermediate statistics are not compatible with any mechanism which allows a variation between Fermi-Dirac and Bose-Einstein statistics.,2004-07-15,http://arxiv.org/abs/physics/0407081v1,J. Dunning-Davies,arxiv.org,physics.gen-ph
Statistical variances in traffic data,We perform statistical analysis of the single-vehicle data measured on the Dutch freeway A9 and discussed in Ref. [2]. Using tools originating from the Random Matrix Theory we show that the significant changes in the statistics of the traffic data can be explained applying equilibrium statistical physics of interacting particles.,2006-11-06,http://arxiv.org/abs/physics/0611049v2,"Milan Krbalek, Petr Seba",arxiv.org,physics.data-an
"Discussion of ""2004 IMS Medallion Lecture: Local Rademacher complexities   and oracle inequalities in risk minimization"" by V. Koltchinskii","Discussion of ""2004 IMS Medallion Lecture: Local Rademacher complexities and oracle inequalities in risk minimization"" by V. Koltchinskii [arXiv:0708.0083]",2007-08-01,http://arxiv.org/abs/0708.0089v1,"Peter L. Bartlett, Shahar Mendelson",arxiv.org,"q-fin.RM, math.ST, stat.TH"
Discussion of ``2004 IMS Medallion Lecture: Local Rademacher   complexities and oracle inequalities in risk minimization'' by V.   Koltchinskii,Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and oracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083],2007-08-01,http://arxiv.org/abs/0708.0094v1,"Gilles Blanchard, Pascal Massart",arxiv.org,"math.ST, stat.TH"
Discussion of ``2004 IMS Medallion Lecture: Local Rademacher   complexities and oracle inequalities in risk minimization'' by V.   Koltchinskii,Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and oracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083],2007-08-01,http://arxiv.org/abs/0708.0098v1,"Stéphan Clémençon, Gábor Lugosi, Nicolas Vayatis",arxiv.org,"q-fin.RM, math.ST, stat.TH"
Discussion of ``2004 IMS Medallion Lecture: Local Rademacher   complexities and oracle inequalities in risk minimization'' by V.   Koltchinskii,Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and oracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083],2007-08-01,http://arxiv.org/abs/0708.0121v1,"Xiaotong Shen, Lifeng Wang",arxiv.org,"q-fin.RM, math.ST, stat.TH"
Discussion of ``2004 IMS Medallion Lecture: Local Rademacher   complexities and oracle inequalities in risk minimization'' by V.   Koltchinskii,Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and oracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083],2007-08-01,http://arxiv.org/abs/0708.0124v1,A. B. Tsybakov,arxiv.org,"q-fin.RM, math.ST, stat.TH"
Discussion of ``2004 IMS Medallion Lecture: Local Rademacher   complexities and oracle inequalities in risk minimization'' by V.   Koltchinskii,Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and oracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083],2007-08-01,http://arxiv.org/abs/0708.0132v1,Sara van de Geer,arxiv.org,"q-fin.RM, math.ST, stat.TH"
Rejoinder: 2004 IMS Medallion Lecture: Local Rademacher complexities and   oracle inequalities in risk minimization,Rejoinder: 2004 IMS Medallion Lecture: Local Rademacher complexities and oracle inequalities in risk minimization [arXiv:0708.0083],2007-08-01,http://arxiv.org/abs/0708.0135v1,Vladimir Koltchinskii,arxiv.org,"math.ST, stat.TH"
Applied statistics: A review,"The main phases of applied statistical work are discussed in general terms. The account starts with the clarification of objectives and proceeds through study design, measurement and analysis to interpretation. An attempt is made to extract some general notions.",2007-08-27,http://arxiv.org/abs/0708.3545v1,D. R. Cox,arxiv.org,stat.AP
Discussion of: Statistical analysis of an archeological find,Discussion of ``Statistical analysis of an archeological find'' by Andrey Feuerverger [arXiv:0804.0079],2008-04-01,http://arxiv.org/abs/0804.0080v1,Stephen M. Stigler,arxiv.org,stat.AP
Discussion of: Statistical analysis of an archaeological find,Discussion of ``Statistical analysis of an archeological find'' by Andrey Feuerverger [arXiv:0804.0079],2008-04-01,http://arxiv.org/abs/0804.0090v1,Donald L. Bentley,arxiv.org,stat.AP
Discussion of: Statistical analysis of an archeological find--skeptical   counting challenges to an archaeological find,Discussion of ``Statistical analysis of an archeological find'' by Andrey Feuerverger [arXiv:0804.0079],2008-04-01,http://arxiv.org/abs/0804.0093v1,Sheila M. Bird,arxiv.org,stat.AP
Discussion of: Statistical analysis of an archeological find,Discussion of ``Statistical analysis of an archeological find'' by Andrey Feuerverger [arXiv:0804.0079],2008-04-01,http://arxiv.org/abs/0804.0095v1,"Holger Höfling, Larry Wasserman",arxiv.org,stat.AP
Discussion of: Statistical analysis of an archeological find,Discussion of ``Statistical analysis of an archeological find'' by Andrey Feuerverger [arXiv:0804.0079],2008-04-01,http://arxiv.org/abs/0804.0099v1,"J. Mortera, P. Vicard",arxiv.org,stat.AP
Data-driven Sobolev tests of uniformity on compact Riemannian manifolds,Data-driven versions of Sobolev tests of uniformity on compact Riemannian manifolds are proposed. These tests are invariant under isometries and are consistent against all alternatives. The large-sample asymptotic null distributions are given.,2008-06-18,http://arxiv.org/abs/0806.2939v1,P. E. Jupp,arxiv.org,"math.ST, stat.TH, 62F03, 62F05, 62H11 (Primary)"
Revision of the fractional exclusion statistics,"I discuss the concept of fractional exclusion statistics (FES) and I show that in order to preserve the thermodynamic consistency of the formalism, the exclusion statistics parameters should change if the species of particles in the system are divided into subspecies. Using a simple and intuitive model I deduce the general equations that have to be obeyed by the exlcusion statistics parameters in any FES system.",2009-06-26,http://arxiv.org/abs/0906.4836v1,Dragoş-Victor Anghel,arxiv.org,"cond-mat.stat-mech, cond-mat.quant-gas"
Discussion of: Brownian distance covariance,"Discussion on ""Brownian distance covariance"" by G\'{a}bor J. Sz\'{e}kely and Maria L. Rizzo [arXiv:1010.0297]",2009-12-17,http://arxiv.org/abs/0912.3295v2,"Peter J. Bickel, Ying Xu",arxiv.org,"math.ST, stat.AP, stat.TH"
"Bayes, Jeffreys, Prior Distributions and the Philosophy of Statistics","Discussion of ""Harold Jeffreys's Theory of Probability revisited,"" by Christian Robert, Nicolas Chopin, and Judith Rousseau, for Statistical Science [arXiv:0804.3173]",2010-01-18,http://arxiv.org/abs/1001.2968v1,Andrew Gelman,arxiv.org,stat.ME
Some Proofs on Statistical Magnitudes for Continuous Phenomena,"In this work, the proofs concerning the continuity of the disequilibrium, Shannon information and statistical complexity in the space of distributions are presented. Also, some results on the existence of Shannon information for continuous systems are given.",2010-02-19,http://arxiv.org/abs/1002.3807v1,"Raquel G. Catalan, Jose Garay, Ricardo Lopez-Ruiz",arxiv.org,"nlin.AO, math.ST, physics.data-an, stat.TH"
"Discussion of ""Multivariate quantiles and multiple-output regression   quantiles: From $L_1$ optimization to halfspace depth""","Discussion of ""Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth"" by M. Hallin, D. Paindaveine and M. Siman [arXiv:1002.4486]",2010-02-24,http://arxiv.org/abs/1002.4494v1,Ying Wei,arxiv.org,"math.ST, stat.TH"
"Discussion of ""Multivariate quantiles and multiple-output regression   quantiles: From $L_1$ optimization to halfspace depth""","Discussion of ""Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth"" by M. Hallin, D. Paindaveine and M. Siman [arXiv:1002.4486]",2010-02-24,http://arxiv.org/abs/1002.4496v1,"Robert Serfling, Yijun Zuo",arxiv.org,"math.ST, stat.TH"
"Discussion of ""Multivariate quantiles and multiple-output regression   quantiles: From $L_1$ optimization to halfspace depth""","Discussion of ""Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth"" by M. Hallin, D. Paindaveine and M. Siman [arXiv:1002.4486]",2010-02-24,http://arxiv.org/abs/1002.4509v1,"Linglong Kong, Ivan Mizera",arxiv.org,"math.ST, stat.TH"
"Rejoinder to ""Multivariate quantiles and multiple-output regression   quantiles: From $L_1$ optimization to halfspace depth""","Rejoinder to ""Multivariate quantiles and multiple-output regression quantiles: From $L_1$ optimization to halfspace depth"" by M. Hallin, D. Paindaveine and M. Siman [arXiv:1002.4486]",2010-02-24,http://arxiv.org/abs/1002.4515v1,"Marc Hallin, Davy Paindaveine, Miroslav Šiman",arxiv.org,"math.ST, stat.TH"
Characterizations of Student's t-distribution via regressions of order   statistics,"Utilizing regression properties of order statistics, we characterize a family of distributions introduced by Akhundov, Balakrishnan, and Nevzorov (2004), that includes the t-distribution with two degrees of freedom as one of its members. Then we extend this characterization result to t-distribution with more than two degrees of freedom.",2010-10-21,http://arxiv.org/abs/1010.4355v1,"George P. Yanev, M. Ahsanullah",arxiv.org,"math.PR, math.ST, stat.TH, 62G30, 62E10"
Pathway Model and Nonextensive Statistical Mechanics,"The established technique of eliminating upper or lower parameters in a general hypergeometric series is profitably exploited to create pathways among confluent hypergeometric functions, binomial functions, Bessel functions, and exponential series. One such pathway, from the mathematical statistics point of view, results in distributions which naturally emerge within nonextensive statistical mechanics and Beck-Cohen superstatistics, as pursued in generalizations of Boltzmann-Gibbs statistics.",2010-10-21,http://arxiv.org/abs/1010.4597v1,"A. M. Mathai, H. J. Haubold, C. Tsallis",arxiv.org,"cond-mat.stat-mech, math-ph, math.MP"
Comment: How Should Indirect Evidence Be Used?,"Indirect evidence is crucial for successful statistical practice. Sometimes, however, it is better used informally. Future efforts should be directed toward understanding better the connection between statistical methods and scientific problems. [arXiv:1012.1161]",2010-12-07,http://arxiv.org/abs/1012.1479v1,Robert E. Kass,arxiv.org,stat.ME
Comment: Quantifying the Fraction of Missing Information for Hypothesis   Testing in Statistical and Genetic Studies,"Comment on ""Quantifying the Fraction of Missing Information for Hypothesis Testing in Statistical and Genetic Studies"" [arXiv:1102.2774]",2011-02-15,http://arxiv.org/abs/1102.2987v1,"I-Shou Chang, Chung-Hsing Chen, Li-Chu Chien, Chao A. Hsiung",arxiv.org,stat.ME
Comment: Quantifying the Fraction of Missing Information for Hypothesis   Testing in Statistical and Genetic Studies,"Comment on ""Quantifying the Fraction of Missing Information for Hypothesis Testing in Statistical and Genetic Studies"" [arXiv:1102.2774]",2011-02-15,http://arxiv.org/abs/1102.2993v1,"Tian Zheng, Shaw-Hwa Lo",arxiv.org,stat.ME
Rejoinder: Quantifying the Fraction of Missing Information for   Hypothesis Testing in Statistical and Genetic Studies,"Rejoinder to ""Quantifying the Fraction of Missing Information for Hypothesis Testing in Statistical and Genetic Studies"" [arXiv:1102.2774]",2011-02-15,http://arxiv.org/abs/1102.3005v1,"Dan L. Nicolae, Xiao-Li Meng, Augustine Kong",arxiv.org,stat.ME
Discussion of: A statistical analysis of multiple temperature proxies:   Are reconstructions of surface temperatures over the last 1000 years   reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",2011-04-21,http://arxiv.org/abs/1104.4171v1,L. Mark Berliner,arxiv.org,stat.AP
Discussion of: A statistical analysis of multiple temperature proxies:   Are reconstructions of surface temperatures over the last 1000 years   reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",2011-04-21,http://arxiv.org/abs/1104.4174v1,Alexey Kaplan,arxiv.org,stat.AP
Discussion of: A statistical analysis of multiple temperature proxies:   Are reconstructions of surface temperatures over the last 1000 years   reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",2011-04-21,http://arxiv.org/abs/1104.4185v1,Lasse Holmström,arxiv.org,stat.AP
Discussion of: A statistical analysis of multiple temperature proxies:   Are reconstructions of surface temperatures over the last 1000 years   reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",2011-04-21,http://arxiv.org/abs/1104.4188v1,Jason E. Smerdon,arxiv.org,stat.AP
Discussion of: A statistical analysis of multiple temperature proxies:   Are reconstructions of surface temperatures over the last 1000 years   reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",2011-04-21,http://arxiv.org/abs/1104.4193v1,"Peter Craigmile, Bala Rajaratnam",arxiv.org,stat.AP
Discussion of: A statistical analysis of multiple temperature proxies:   Are reconstructions of surface temperatures over the last 1000 years   reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",2011-05-03,http://arxiv.org/abs/1105.0519v1,"Doug Nychka, Bo Li",arxiv.org,stat.AP
Discussion of: A statistical analysis of multiple temperature proxies:   Are reconstructions of surface temperatures over the last 1000 years   reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",2011-05-03,http://arxiv.org/abs/1105.0522v1,Jonathan Rougier,arxiv.org,stat.AP
"Discussion of ""Statistical Inference: The Big Picture"" by R. E. Kass","Discussion of ""Statistical Inference: The Big Picture"" by R. E. Kass [arXiv:1106.2895]",2011-06-17,http://arxiv.org/abs/1106.3393v1,Steven N. Goodman,arxiv.org,stat.ME
"Discussion of ""Calibrated Bayes, for Statistics in General, and Missing   Data in Particular"" by R. J. A. Little","Discussion of ""Calibrated Bayes, for Statistics in General, and Missing Data in Particular"" by R. Little [arXiv:1108.1917]",2011-08-17,http://arxiv.org/abs/1108.3457v1,Nathaniel Schenker,arxiv.org,stat.ME
"Discussion of ""Calibrated Bayes, for Statistics in General, and Missing   Data in Particular"" by R. J. A. Little","Discussion of ""Calibrated Bayes, for Statistics in General, and Missing Data in Particular"" by R. Little [arXiv:1108.1917]",2011-08-17,http://arxiv.org/abs/1108.3461v1,Michael D. Larsen,arxiv.org,stat.ME
Broken Z2 symmetries and fluctuations in statistical mechanics,"An analogy is developed between the breaking of space-inversion symmetry in equilibrium statistical mechanics and the breaking of time-reversal symmetry in nonequilibrium statistical mechanics. In this way, similar relationships characterizing fluctuations are obtained in both contexts.",2012-06-07,http://arxiv.org/abs/1206.1547v1,Pierre Gaspard,arxiv.org,cond-mat.stat-mech
"After 50+ Years in Statistics, An Exchange",This is an exchange between Jerome Sacks and Donald Ylvisaker covering their career paths along with some related history and philosophy of Statistics.,2012-07-24,http://arxiv.org/abs/1207.5656v1,"Jerome Sacks, Donald Ylvisaker",arxiv.org,stat.ME
"Discussion of ""Statistical Modeling of Spatial Extremes"" by A. C.   Davison, S. A. Padoan and M. Ribatet","Discussion of ""Statistical Modeling of Spatial Extremes"" by A. C. Davison, S. A. Padoan and M. Ribatet [arXiv:1208.3378].",2012-08-17,http://arxiv.org/abs/1208.3559v1,"D. Cooley, S. R. Sain",arxiv.org,stat.ME
"Discussion of ""Statistical Modeling of Spatial Extremes"" by A. C.   Davison, S. A. Padoan and M. Ribatet","Discussion of ""Statistical Modeling of Spatial Extremes"" by A. C. Davison, S. A. Padoan and M. Ribatet [arXiv:1208.3378].",2012-08-17,http://arxiv.org/abs/1208.3574v1,"Benjamin Shaby, Brian J. Reich",arxiv.org,stat.ME
"Discussion of ""Statistical Modeling of Spatial Extremes"" by A. C.   Davison, S. A. Padoan and M. Ribatet","Discussion of ""Statistical Modeling of Spatial Extremes"" by A. C. Davison, S. A. Padoan and M. Ribatet [arXiv:1208.3378].",2012-08-17,http://arxiv.org/abs/1208.3575v1,"Darmesah Gabda, Ross Towe, Jennifer Wadsworth, Jonathan Tawn",arxiv.org,stat.ME
"Rejoinder to ""Statistical Modeling of Spatial Extremes""","Rejoinder to ""Statistical Modeling of Spatial Extremes"" by A. C. Davison, S. A. Padoan and M. Ribatet [arXiv:1208.3378].",2012-08-17,http://arxiv.org/abs/1208.3577v1,"A. C. Davison, S. A. Padoan, M. Ribatet",arxiv.org,stat.ME
Discussion: Latent variable graphical model selection via convex   optimization,"Discussion of ""Latent variable graphical model selection via convex optimization"" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky [arXiv:1008.1290].",2012-11-05,http://arxiv.org/abs/1211.0801v1,Ming Yuan,arxiv.org,"math.ST, cs.LG, stat.ML, stat.TH"
Discussion: Latent variable graphical model selection via convex   optimization,"Discussion of ""Latent variable graphical model selection via convex optimization"" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky [arXiv:1008.1290].",2012-11-05,http://arxiv.org/abs/1211.0806v1,"Steffen Lauritzen, Nicolai Meinshausen",arxiv.org,"math.ST, cs.LG, stat.ML, stat.TH"
Discussion: Latent variable graphical model selection via convex   optimization,"Discussion of ""Latent variable graphical model selection via convex optimization"" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky [arXiv:1008.1290].",2012-11-05,http://arxiv.org/abs/1211.0808v1,Martin J. Wainwright,arxiv.org,"math.ST, cs.LG, stat.ML, stat.TH"
Discussion: Latent variable graphical model selection via convex   optimization,"Discussion of ""Latent variable graphical model selection via convex optimization"" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky [arXiv:1008.1290].",2012-11-05,http://arxiv.org/abs/1211.0811v1,"Christophe Giraud, Alexandre Tsybakov",arxiv.org,"math.ST, stat.TH"
Discussion: Latent variable graphical model selection via convex   optimization,"Discussion of ""Latent variable graphical model selection via convex optimization"" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky [arXiv:1008.1290].",2012-11-05,http://arxiv.org/abs/1211.0813v1,"Zhao Ren, Harrison H. Zhou",arxiv.org,"math.ST, stat.TH"
Discussion: Latent variable graphical model selection via convex   optimization,"Discussion of ""Latent variable graphical model selection via convex optimization"" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky [arXiv:1008.1290].",2012-11-05,http://arxiv.org/abs/1211.0817v1,"Emmanuel J. Candés, Mahdi Soltanolkotabi",arxiv.org,"math.ST, cs.LG, stat.ML, stat.TH"
Statistical tests for group comparison of manifold-valued data,"Motivated by population studies of Diffusion Tensor Imaging, the paper investigates the use of mean-based and dispersion-based permutation tests to define and compute the significance of a statistical test for data taking values on nonlinear manifolds. The paper proposes statistical tests that are computationally tractable and geometrically sound for Diffusion Tensor Imaging.",2013-05-06,http://arxiv.org/abs/1305.1118v1,"Anne Collard, Christophe Phillips, Rodolphe Sepulchre",arxiv.org,"math.ST, stat.TH"
"Discussion: ""A significance test for the lasso""","Discussion of ""A significance test for the lasso"" by Richard Lockhart, Jonathan Taylor, Ryan J. Tibshirani, Robert Tibshirani [arXiv:1301.7161].",2014-05-27,http://arxiv.org/abs/1405.6792v1,"Peter Bühlmann, Lukas Meier, Sara van de Geer",arxiv.org,"math.ST, stat.TH"
"Discussion: ""A significance test for the lasso""","Discussion of ""A significance test for the lasso"" by Richard Lockhart, Jonathan Taylor, Ryan J. Tibshirani, Robert Tibshirani [arXiv:1301.7161].",2014-05-27,http://arxiv.org/abs/1405.6793v1,"T. Tony Cai, Ming Yuan",arxiv.org,"math.ST, stat.TH"
"Discussion: ""A significance test for the lasso""","Discussion of ""A significance test for the lasso"" by Richard Lockhart, Jonathan Taylor, Ryan J. Tibshirani, Robert Tibshirani [arXiv:1301.7161].",2014-05-27,http://arxiv.org/abs/1405.6796v1,"Jianqing Fan, Zheng Tracy Ke",arxiv.org,"math.ST, stat.TH"
"Discussion: ""A significance test for the lasso""","Discussion of ""A significance test for the lasso"" by Richard Lockhart, Jonathan Taylor, Ryan J. Tibshirani, Robert Tibshirani [arXiv:1301.7161].",2014-05-27,http://arxiv.org/abs/1405.6798v1,"Jinchi Lv, Zemin Zheng",arxiv.org,"math.ST, stat.ME, stat.TH"
"Discussion: ""A significance test for the lasso""","Discussion of ""A significance test for the lasso"" by Richard Lockhart, Jonathan Taylor, Ryan J. Tibshirani, Robert Tibshirani [arXiv:1301.7161].",2014-05-27,http://arxiv.org/abs/1405.6800v1,Larry Wasserman,arxiv.org,"math.ST, stat.ME, stat.TH"
"Discussion: ""A significance test for the lasso""","Discussion of ""A significance test for the lasso"" by Richard Lockhart, Jonathan Taylor, Ryan J. Tibshirani, Robert Tibshirani [arXiv:1301.7161].",2014-05-27,http://arxiv.org/abs/1405.6803v1,"A. Buja, L. Brown",arxiv.org,"math.ST, stat.ME, stat.TH"
"Discussion of ""Frequentist coverage of adaptive nonparametric Bayesian   credible sets""","Discussion of ""Frequentist coverage of adaptive nonparametric Bayesian credible sets"" by Szab\'o, van der Vaart and van Zanten [arXiv:1310.4489v5].",2014-10-28,http://arxiv.org/abs/1410.7600v2,Richard Nickl,arxiv.org,"math.ST, stat.TH"
"Discussion of ""On the Birnbaum Argument for the Strong Likelihood   Principle""","We discuss Birnbaum's result, its relevance to statistical reasoning, Mayo's objections and the result in [Electron. J. Statist. 7 (2013) 2645-2655] that the proof of this result doesn't establish what is commonly believed. [arXiv:1302.7021]",2014-11-04,http://arxiv.org/abs/1411.0808v1,Michael Evans,arxiv.org,stat.ME
On the State of Computing in Statistics Education: Tools for Learning   and for Doing,"This paper lays out the current landscape of tools used in statistics education. In particular, it considers graphing calculators, spreadsheets, applets and microworlds, standalone educational software, statistical programming tools, tools for reproducible research and bespoke tools. The strengths and weaknesses of the tools are considered, particularly in the context of McNamara (2016)'s list of attributes for a statistical computing tool. Best practices for computing in introductory statistics are suggested.",2016-10-01,http://arxiv.org/abs/1610.00984v1,Amelia McNamara,arxiv.org,"stat.CO, cs.CY"
Almost cosymplectic statistical manifolds,"This paper is a study of almost contact statistical manifolds. Especially this study is focused on almost cosymplectic statistical manifolds. We obtained basic properties of such manifolds. It is proved a characterization theorem and a corollary for the almost cosymplectic statistical manifold with Kaehler leaves. We also study curvature properties of an almost cosymplectic statistical manifold. Moreover, examples are constructed.",2018-01-30,http://arxiv.org/abs/1801.09890v1,"Aziz Yazla, İrem Küpeli Erken, Cengizhan Murathan",arxiv.org,"math.DG, 53B30, 53C15, 53C25 (Primary) 53D10 (Secondary)"
On Stochastic Comparisons of Order Statistics from Heterogeneous   Exponential Samples,"We show that the $k$th order statistic from a heterogeneous sample of $n\geq k$ exponential random variables is larger than that from a homogeneous exponential sample in the sense of star ordering, as conjectured by Xu and Balakrishnan (2012). As a consequence, we establish hazard rate ordering for order statistics between heterogeneous and homogeneous exponential samples, resolving an open problem of P\v{a}lt\v{a}nea (2008). Extensions to general spacings are also presented.",2016-07-22,http://arxiv.org/abs/1607.06564v1,Yaming Yu,arxiv.org,"math.ST, stat.TH"
Using data-compressors for statistical analysis of problems on   homogeneity testing and classification,"Nowadays data compressors are applied to many problems of text analysis, but many such applications are developed outside of the framework of mathematical statistics. In this paper we overcome this obstacle and show how several methods of classical mathematical statistics can be developed based on applications of the data compressors.",2017-01-15,http://arxiv.org/abs/1701.04028v1,"Boris Ryabko, Andrey Guskov, Irina Selivanova",arxiv.org,"cs.IT, math.IT, math.ST, stat.TH"
Characterizations of distributions via order statistics with random   exponential shifts,A class of probability distributions is characterized via equalities in law between two order statistics shifted by independent exponential variables. An explicit formula for the quintile function of the identified family of distributions is obtained. The results extend some known characterizations of exponential and logistic distributions.,2011-07-23,http://arxiv.org/abs/1107.4716v1,"M. Ahsanullah, V. B. Nevzorov, George P. Yanev",arxiv.org,"math.PR, math.ST, stat.TH, 62G30"
"Discussion of ""Frequentist coverage of adaptive nonparametric Bayesian   credible sets""","Discussion of ""Frequentist coverage of adaptive nonparametric Bayesian credible sets"" by Szab\'o, van der Vaart and van Zanten [arXiv:1310.4489v5].",2015-09-07,http://arxiv.org/abs/1509.01900v1,Ismaël Castillo,arxiv.org,"math.ST, stat.TH"
"Discussion of ""Frequentist coverage of adaptive nonparametric Bayesian   credible sets""","Discussion of ""Frequentist coverage of adaptive nonparametric Bayesian credible sets"" by Szab\'o, van der Vaart and van Zanten [arXiv:1310.4489v5].",2015-09-07,http://arxiv.org/abs/1509.01903v1,Judith Rousseau,arxiv.org,"math.ST, stat.TH"
"Discussion of ""Frequentist coverage of adaptive nonparametric Bayesian   credible sets""","Discussion of ""Frequentist coverage of adaptive nonparametric Bayesian credible sets"" by Szab\'o, van der Vaart and van Zanten [arXiv:1310.4489v5].",2015-09-07,http://arxiv.org/abs/1509.01904v1,"Mark G. Low, Zongming Ma",arxiv.org,"math.ST, stat.TH"
"Discussion of ""Frequentist coverage of adaptive nonparametric Bayesian   credible sets""","Discussion of ""Frequentist coverage of adaptive nonparametric Bayesian credible sets"" by Szab\'o, van der Vaart and van Zanten [arXiv:1310.4489v5].",2015-09-07,http://arxiv.org/abs/1509.01905v1,Subhashis Ghosal,arxiv.org,"math.ST, stat.TH"
Optimal moment inequalities for order statistics from nonnegative random   variables,"We obtain the best possible upper bounds for the moments of a single order statistic from independent, non-negative random variables, in terms of the population mean. The main result covers the independent identically distributed case. Furthermore, the case of the sample minimum for merely independent (not necessarily identically distributed) random variables is treated in detail.   Key-words and phrases: order statistics; optimal moment bounds; nonnegative random variables; sample minimum; reliability systems.",2018-06-13,http://arxiv.org/abs/1806.05095v1,Nickos Papadatos,arxiv.org,"math.ST, stat.TH"
Comment: Classifier Technology and the Illusion of Progress--Credit   Scoring,Comment on Classifier Technology and the Illusion of Progress--Credit Scoring [math.ST/0606441],2006-06-19,http://arxiv.org/abs/math/0606452v1,Ross W. Gayler,arxiv.org,"math.ST, stat.TH"
Elaboration on Two Points Raised in ``Classifier Technology and the   Illusion of Progress'',Comment: Elaboration on Two Points Raised in ``Classifier Technology and the Illusion of Progress'' [math.ST/0606441],2006-06-19,http://arxiv.org/abs/math/0606455v1,Robert C. Holte,arxiv.org,"math.ST, stat.TH"
Representative Ensembles in Statistical Mechanics,"The notion of representative statistical ensembles, correctly representing statistical systems, is strictly formulated. This notion allows for a proper description of statistical systems, avoiding inconsistencies in theory. As an illustration, a Bose-condensed system is considered. It is shown that a self-consistent treatment of the latter, using a representative ensemble, always yields a conserving and gapless theory.",2007-04-09,http://arxiv.org/abs/0704.1089v1,V. I. Yukalov,arxiv.org,cond-mat.stat-mech
On Statistical Significance of Signal,"A definition for the statistical significance of a signal in an experiment is proposed by establishing a correlation between the observed p-value and the normal distribution integral probability, which is suitable for both counting experiment and continuous test statistics. The explicit expressions to calculate the statistical significance for both cases are given.",2008-12-15,http://arxiv.org/abs/0812.2708v1,Yong-Sheng Zhu,arxiv.org,"physics.data-an, hep-ex"
Fluctuation theorems in nonextensive statistics,"Nonextensive statistics is a formalism of statistical mechanics that describes the ocurrence of power-law distributions in complex systems, particularly the so-called $q$ exponential family of distributions. In this work we present the use of fluctuation theorems for $q$-canonical ensembles as a powerful tool to readily obtain statistical properties. In particular, we have obtained strong conditions for the possible values of $q$ depending on the density of states of the system.",2019-09-27,http://arxiv.org/abs/1909.12816v1,"Haridas Umpierrez, Sergio Davis",arxiv.org,cond-mat.stat-mech
p-value peeking and estimating extrema,"A pervasive issue in statistical hypothesis testing is that the reported $p$-values are biased downward by data ""peeking"" -- the practice of reporting only progressively extreme values of the test statistic as more data samples are collected. We develop principled mechanisms to estimate such running extrema of test statistics, which directly address the effect of peeking in some general scenarios.",2020-11-02,http://arxiv.org/abs/2011.01343v1,Akshay Balsubramani,arxiv.org,"math.ST, stat.ME, stat.ML, stat.TH"
Dimension-Free Anticoncentration Bounds for Gaussian Order Statistics   with Discussion of Applications to Multiple Testing,"The following anticoncentration property is proved. The probability that the $k$-order statistic of an arbitrarily correlated jointly Gaussian random vector $X$ with unit variance components lies within an interval of length $\varepsilon$ is bounded above by $2{\varepsilon}k ({ 1+\mathrm{E}[\|X\|_\infty ]}) $. This bound has implications for generalized error rate control in statistical high-dimensional multiple hypothesis testing problems, which are discussed subsequently.",2021-07-22,http://arxiv.org/abs/2107.10766v1,Damian Kozbur,arxiv.org,"math.ST, stat.TH"
Discussion: One-step sparse estimates in nonconcave penalized likelihood   models,Discussion of ``One-step sparse estimates in nonconcave penalized likelihood models'' [arXiv:0808.1012],2008-08-07,http://arxiv.org/abs/0808.1013v1,"Peter Bühlmann, Lukas Meier",arxiv.org,"math.ST, stat.TH"
Discussion: One-step sparse estimates in nonconcave penalized likelihood   models: Who cares if it is a white cat or a black Cat?,Discussion of ``One-step sparse estimates in nonconcave penalized likelihood models'' [arXiv:0808.1012],2008-08-07,http://arxiv.org/abs/0808.1016v1,Xiao-Li Meng,arxiv.org,"math.ST, stat.TH, 62F99 (Primary) 62F15 (Secondary)"
Discussion: One-step sparse estimates in nonconcave penalized likelihood   models,Discussion of ``One-step sparse estimates in nonconcave penalized likelihood models'' [arXiv:0808.1012],2008-08-07,http://arxiv.org/abs/0808.1025v1,Cun-Hui Zhang,arxiv.org,"math.ST, stat.TH"
"United Statistical Algorithm, Small and Big Data: Future OF Statistician","This article provides the role of big idea statisticians in future of Big Data Science. We describe the `United Statistical Algorithms' framework for comprehensive unification of traditional and novel statistical methods for modeling Small Data and Big Data, especially mixed data (discrete, continuous).",2013-08-02,http://arxiv.org/abs/1308.0641v1,"Emanuel Parzen, Subhadeep Mukhopadhyay",arxiv.org,"math.ST, stat.ME, stat.ML, stat.TH"
Bayesian approach to extreme-value statistics based on conditional   maximum-entropy method,"Recently, the conditional maximum-entropy method (abbreviated as C-MaxEnt) has been proposed for selecting priors in Bayesian statistics in a very simple way. Here, it is examined for extreme-value statistics. For the Weibull type as an explicit example, it is shown how C-MaxEnt can give rise to a prior satisfying Jeffreys' rule.",2018-09-28,http://arxiv.org/abs/1810.00035v1,Sumiyoshi Abe,arxiv.org,"cond-mat.stat-mech, physics.data-an"
Statistical convergence of nets on locally solid Riesz spaces,"The statistical convergence is handled for sequences with the natural density, in general. In a recent paper, the statistical convergence for nets in Riesz spaces has been studied and investigated by developing topology-free techniques in Riesz spaces. In this paper, we introduce the statistically topological convergence for nets on locally solid Riesz spaces with solid topologies. Moreover, we introduce the statistical continuity on locally solid Riesz spaces.",2021-05-27,http://arxiv.org/abs/2105.13232v1,"Fatih Temizsu, Abdullah Aydın",arxiv.org,"math.FA, 40A35, 46A40, 40A05, 46B42, F.2.0"
Osband's Principle for Identification Functions,"Given a statistical functional of interest such as the mean or median, a (strict) identification function is zero in expectation at (and only at) the true functional value. Identification functions are key objects in forecast validation, statistical estimation and dynamic modelling. For a possibly vector-valued functional of interest, we fully characterise the class of (strict) identification functions subject to mild regularity conditions.",2022-08-16,http://arxiv.org/abs/2208.07685v1,"Timo Dimitriadis, Tobias Fissler, Johanna Ziegel",arxiv.org,"math.ST, stat.TH"
